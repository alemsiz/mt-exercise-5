2024-05-26 23:57:07,171 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                           cfg.name : word_level_model
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                     cfg.data.train : data/head100k.train.it-en
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.it-en
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.it-en
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-26 23:57:07,171 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -                 cfg.data.src.level : word
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 2000
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 2000
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : moses
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/word_level_model
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-26 23:57:07,172 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-26 23:57:07,173 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-26 23:57:07,175 - INFO - joeynmt.data - Building tokenizer...
2024-05-26 23:57:07,617 - INFO - joeynmt.tokenizers - it tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-26 23:57:07,618 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=moses)
2024-05-26 23:57:07,618 - INFO - joeynmt.data - Loading train set...
2024-05-26 23:57:23,787 - INFO - joeynmt.data - Building vocabulary...
2024-05-26 23:57:24,578 - INFO - joeynmt.data - Loading dev set...
2024-05-26 23:57:24,722 - INFO - joeynmt.data - Loading test set...
2024-05-26 23:57:24,962 - INFO - joeynmt.data - Data loaded.
2024-05-26 23:57:24,962 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-26 23:57:24,962 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-26 23:57:24,962 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-26 23:57:24,962 - INFO - joeynmt.data - First training example:
	[SRC] Al Gore : arrestiamo il riscaldamento globale
	[TRG] Al Gore : Averting the climate crisis
2024-05-26 23:57:24,962 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) di (7) che (8) e (9) è
2024-05-26 23:57:24,962 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2024-05-26 23:57:24,962 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2024-05-26 23:57:24,962 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2024-05-26 23:57:24,963 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-26 23:57:25,017 - INFO - joeynmt.model - Enc-dec model built.
2024-05-26 23:57:25,019 - INFO - joeynmt.model - Total params: 3925248
2024-05-26 23:57:25,020 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-26 23:57:25,020 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-26 23:57:25,020 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-26 23:57:25,020 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-26 23:57:25,020 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-26 23:57:25,020 - INFO - joeynmt.training - EPOCH 1
2024-05-26 23:57:38,442 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.052554, Batch Acc: 0.157220, Tokens per Sec:     5262, Lr: 0.000300
2024-05-26 23:57:52,378 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.957987, Batch Acc: 0.201981, Tokens per Sec:     4935, Lr: 0.000300
2024-05-26 23:58:06,734 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.715269, Batch Acc: 0.229592, Tokens per Sec:     4663, Lr: 0.000300
2024-05-26 23:58:21,976 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.639627, Batch Acc: 0.252868, Tokens per Sec:     4666, Lr: 0.000300
2024-05-26 23:58:36,837 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.641405, Batch Acc: 0.272462, Tokens per Sec:     4654, Lr: 0.000300
2024-05-26 23:58:36,838 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-26 23:58:36,838 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-26 23:59:00,234 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.62, ppl:  13.70, acc:   0.28, generation: 23.3768[sec], evaluation: 0.0000[sec]
2024-05-26 23:59:00,235 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-26 23:59:00,402 - INFO - joeynmt.training - Example #0
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '&apos;m', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']
2024-05-26 23:59:00,402 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-26 23:59:00,402 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-26 23:59:00,402 - INFO - joeynmt.training - 	Hypothesis: And I'm <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, and <unk>, <unk> <unk> <unk> <unk> <unk>, and <unk> <unk> <unk>, <unk>, <unk>, <unk>, <unk> <unk>.
2024-05-26 23:59:00,402 - INFO - joeynmt.training - Example #1
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-26 23:59:00,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-26 23:59:00,402 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-26 23:59:00,402 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-26 23:59:00,403 - INFO - joeynmt.training - Example #2
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'is', '<unk>', 'is', '<unk>', ',', 'the', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', ',', 'the', '<unk>', '.', '</s>']
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Hypothesis: <unk> is <unk> is <unk>, the <unk> <unk>, <unk>, <unk>, the <unk>.
2024-05-26 23:59:00,403 - INFO - joeynmt.training - Example #3
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> and <unk> and <unk>.
2024-05-26 23:59:00,403 - INFO - joeynmt.training - Example #4
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-26 23:59:00,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-26 23:59:00,403 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>.
2024-05-26 23:59:15,454 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.477255, Batch Acc: 0.292604, Tokens per Sec:     4460, Lr: 0.000300
2024-05-26 23:59:30,584 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.578466, Batch Acc: 0.305787, Tokens per Sec:     4574, Lr: 0.000300
2024-05-26 23:59:44,393 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.345104, Batch Acc: 0.317341, Tokens per Sec:     4968, Lr: 0.000300
2024-05-26 23:59:58,232 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.368403, Batch Acc: 0.331056, Tokens per Sec:     4961, Lr: 0.000300
2024-05-27 00:00:12,134 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.431252, Batch Acc: 0.342192, Tokens per Sec:     4948, Lr: 0.000300
2024-05-27 00:00:12,135 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:00:12,135 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:00:43,505 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.36, ppl:  10.56, acc:   0.34, generation: 31.3493[sec], evaluation: 0.0000[sec]
2024-05-27 00:00:43,507 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:00:43,674 - INFO - joeynmt.training - Example #0
2024-05-27 00:00:43,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:00:43,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:00:43,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', ',', 'I', '&apos;ve', 'been', '<unk>', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'in', 'the', '<unk>', '<unk>', '<unk>', 'years', '.', '</s>']
2024-05-27 00:00:43,674 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:00:43,674 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:00:43,674 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk>, I've been <unk> <unk> <unk> <unk> that <unk> <unk> <unk>, in the <unk> <unk> <unk> years.
2024-05-27 00:00:43,674 - INFO - joeynmt.training - Example #1
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', '<unk>', 'is', 'not', '<unk>', '.', '</s>']
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> the <unk> of the <unk> is not <unk>.
2024-05-27 00:00:43,675 - INFO - joeynmt.training - Example #2
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', ',', 'in', 'a', '<unk>', ',', 'in', 'the', '<unk>', ',', 'in', 'the', '<unk>', 'of', 'the', '<unk>', '.', '</s>']
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk>, in a <unk>, in the <unk>, in the <unk> of the <unk>.
2024-05-27 00:00:43,675 - INFO - joeynmt.training - Example #3
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:00:43,675 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> <unk>.
2024-05-27 00:00:43,675 - INFO - joeynmt.training - Example #4
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:00:43,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:00:43,676 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:00:43,676 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:00:43,676 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> <unk>, <unk> <unk>.
2024-05-27 00:00:57,526 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.301857, Batch Acc: 0.350238, Tokens per Sec:     5090, Lr: 0.000300
2024-05-27 00:01:11,676 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.325500, Batch Acc: 0.361872, Tokens per Sec:     4747, Lr: 0.000300
2024-05-27 00:01:26,657 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.097243, Batch Acc: 0.366423, Tokens per Sec:     4519, Lr: 0.000300
2024-05-27 00:01:41,583 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.221671, Batch Acc: 0.381939, Tokens per Sec:     4578, Lr: 0.000300
2024-05-27 00:01:55,786 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.018310, Batch Acc: 0.390772, Tokens per Sec:     4793, Lr: 0.000300
2024-05-27 00:01:55,786 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:01:55,786 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:02:22,539 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.75, acc:   0.38, generation: 26.7314[sec], evaluation: 0.0000[sec]
2024-05-27 00:02:22,540 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:02:22,712 - INFO - joeynmt.training - Example #0
2024-05-27 00:02:22,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:02:22,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:02:22,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'was', '<unk>', 'these', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', ',', '<unk>', 'years', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', ',', '<unk>', 'percent', 'of', 'the', '<unk>', 'percent', 'of', 'the', '<unk>', 'percent', 'of', 'the', '<unk>', '.', '</s>']
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Hypothesis: The year I was <unk> these <unk> <unk> <unk> <unk> <unk>, which <unk> <unk>, which <unk> <unk>, <unk> years, <unk>, <unk>, <unk>, <unk>, <unk> percent of the <unk> percent of the <unk> percent of the <unk>.
2024-05-27 00:02:22,713 - INFO - joeynmt.training - Example #1
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', ',', 'because', 'it', '&apos;s', 'not', '<unk>', ',', 'because', 'it', '&apos;s', 'not', '<unk>', '.', '</s>']
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk>, because it's not <unk>, because it's not <unk>.
2024-05-27 00:02:22,713 - INFO - joeynmt.training - Example #2
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'in', 'a', 'lot', 'of', '<unk>', ',', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', ',', 'the', '<unk>', 'of', 'the', '<unk>', ',', '</s>']
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> <unk>, in a lot of <unk>, the <unk> of the <unk> of the <unk>, the <unk> of the <unk>,
2024-05-27 00:02:22,713 - INFO - joeynmt.training - Example #3
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:02:22,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:02:22,713 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:02:22,714 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:02:22,714 - INFO - joeynmt.training - Example #4
2024-05-27 00:02:22,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:02:22,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:02:22,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', ',', '<unk>', '<unk>', ',', 'the', '<unk>', '<unk>', '<unk>', 'years', '.', '</s>']
2024-05-27 00:02:22,714 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:02:22,714 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:02:22,714 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk>, <unk> <unk>, the <unk> <unk> <unk> years.
2024-05-27 00:02:37,374 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.022530, Batch Acc: 0.397141, Tokens per Sec:     4636, Lr: 0.000300
2024-05-27 00:02:52,359 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.039086, Batch Acc: 0.409023, Tokens per Sec:     4687, Lr: 0.000300
2024-05-27 00:03:06,619 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     1.974414, Batch Acc: 0.412279, Tokens per Sec:     4841, Lr: 0.000300
2024-05-27 00:03:20,674 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     1.893510, Batch Acc: 0.424626, Tokens per Sec:     4973, Lr: 0.000300
2024-05-27 00:03:34,811 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.037668, Batch Acc: 0.428567, Tokens per Sec:     4803, Lr: 0.000300
2024-05-27 00:03:34,812 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:03:34,812 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:04:06,036 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.52, acc:   0.42, generation: 31.2036[sec], evaluation: 0.0000[sec]
2024-05-27 00:04:06,039 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:04:06,204 - INFO - joeynmt.training - Example #0
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', '&apos;ve', 'been', '<unk>', 'these', '<unk>', 'for', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', 'million', 'years', 'of', 'the', 'United', 'States', 'has', 'been', '<unk>', 'to', '<unk>', ',', '<unk>', 'percent', 'of', 'the', '<unk>', ',', '<unk>', 'percent', 'of', 'the', '<unk>', ',', '<unk>', 'percent', 'of', 'the', '<unk>', '.', '</s>']
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Hypothesis: The year I've been <unk> these <unk> for <unk> <unk>, which <unk> <unk> <unk>, which is <unk> million years of the United States has been <unk> to <unk>, <unk> percent of the <unk>, <unk> percent of the <unk>, <unk> percent of the <unk>.
2024-05-27 00:04:06,205 - INFO - joeynmt.training - Example #1
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'is', 'because', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', 'because', 'it', '&apos;s', 'not', '<unk>', '.', '</s>']
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:04:06,205 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> is because of the problem because it's not because it's not <unk>.
2024-05-27 00:04:06,205 - INFO - joeynmt.training - Example #2
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:04:06,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', '<unk>', 'system', '.', '</s>']
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a sense, the <unk> of the <unk> of the <unk> system.
2024-05-27 00:04:06,206 - INFO - joeynmt.training - Example #3
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:04:06,206 - INFO - joeynmt.training - Example #4
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:04:06,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:04:06,206 - INFO - joeynmt.training - 	Hypothesis: The next next will be a <unk> <unk> <unk>.
2024-05-27 00:04:20,912 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.933256, Batch Acc: 0.436343, Tokens per Sec:     4488, Lr: 0.000300
2024-05-27 00:04:35,963 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.882713, Batch Acc: 0.444937, Tokens per Sec:     4574, Lr: 0.000300
2024-05-27 00:04:50,293 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.003510, Batch Acc: 0.447085, Tokens per Sec:     4783, Lr: 0.000300
2024-05-27 00:05:04,921 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.666821, Batch Acc: 0.450561, Tokens per Sec:     4635, Lr: 0.000300
2024-05-27 00:05:19,293 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.159506, Batch Acc: 0.451931, Tokens per Sec:     4812, Lr: 0.000300
2024-05-27 00:05:19,294 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:05:19,294 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:05:48,808 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.72, acc:   0.44, generation: 29.4924[sec], evaluation: 0.0000[sec]
2024-05-27 00:05:48,809 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:05:48,976 - INFO - joeynmt.training - Example #0
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', '&apos;ve', 'been', '<unk>', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'who', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Hypothesis: The year I've been <unk> these <unk> to <unk> that <unk> <unk> <unk>, who <unk> <unk> <unk> <unk>, and <unk> <unk> <unk> <unk>, it's <unk> <unk>, it's <unk> <unk>, it's <unk> <unk> <unk>, it's <unk> <unk>.
2024-05-27 00:05:48,977 - INFO - joeynmt.training - Example #1
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', '<unk>', 'is', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> <unk> is because it's not <unk> the ice of the ice.
2024-05-27 00:05:48,977 - INFO - joeynmt.training - Example #2
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', '<unk>', 'of', 'the', '<unk>', 'system', '.', '</s>']
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:05:48,977 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is, in a sense, the <unk> of the <unk> system.
2024-05-27 00:05:48,977 - INFO - joeynmt.training - Example #3
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:05:48,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:05:48,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:05:48,978 - INFO - joeynmt.training - Example #4
2024-05-27 00:05:48,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:05:48,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:05:48,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', 'on', 'the', 'last', 'years', '.', '</s>']
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:05:48,978 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> on the last years.
2024-05-27 00:06:03,173 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.056168, Batch Acc: 0.460330, Tokens per Sec:     4818, Lr: 0.000300
2024-05-27 00:06:18,127 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     1.861200, Batch Acc: 0.457263, Tokens per Sec:     4653, Lr: 0.000300
2024-05-27 00:06:33,014 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     1.760426, Batch Acc: 0.467649, Tokens per Sec:     4618, Lr: 0.000300
2024-05-27 00:06:46,493 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     1.888344, Batch Acc: 0.472509, Tokens per Sec:     5096, Lr: 0.000300
2024-05-27 00:06:59,949 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     1.752420, Batch Acc: 0.474174, Tokens per Sec:     5151, Lr: 0.000300
2024-05-27 00:06:59,950 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:06:59,950 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:07:25,476 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.29, acc:   0.46, generation: 25.5054[sec], evaluation: 0.0000[sec]
2024-05-27 00:07:25,478 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:07:25,641 - INFO - joeynmt.helpers - delete models/word_level_model/500.ckpt
2024-05-27 00:07:25,642 - INFO - joeynmt.training - Example #0
2024-05-27 00:07:25,642 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:07:25,642 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:07:25,642 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', '&apos;ve', 'got', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'three', 'million', 'years', '.', '</s>']
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Hypothesis: The last year I've got these <unk> for <unk> that <unk> <unk> <unk>, which is for three million years.
2024-05-27 00:07:25,643 - INFO - joeynmt.training - Example #1
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', '.', '</s>']
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the <unk> of the problem because it's not <unk>.
2024-05-27 00:07:25,643 - INFO - joeynmt.training - Example #2
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'global', 'global', '<unk>', 'system', '.', '</s>']
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is, in a sense, the heart of the <unk> of global global <unk> system.
2024-05-27 00:07:25,643 - INFO - joeynmt.training - Example #3
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:07:25,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:07:25,643 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:07:25,644 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:07:25,644 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:07:25,644 - INFO - joeynmt.training - Example #4
2024-05-27 00:07:25,644 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:07:25,644 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:07:25,644 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'in', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:07:25,644 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:07:25,644 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:07:25,644 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> in the last 25 years.
2024-05-27 00:07:39,273 - INFO - joeynmt.training - Epoch   1: total training loss 6829.80
2024-05-27 00:07:39,274 - INFO - joeynmt.training - EPOCH 2
2024-05-27 00:07:40,341 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     1.593110, Batch Acc: 0.499799, Tokens per Sec:     4654, Lr: 0.000300
2024-05-27 00:07:54,298 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.530080, Batch Acc: 0.491324, Tokens per Sec:     5009, Lr: 0.000300
2024-05-27 00:08:09,452 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.501334, Batch Acc: 0.490532, Tokens per Sec:     4597, Lr: 0.000300
2024-05-27 00:08:24,605 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.773548, Batch Acc: 0.491925, Tokens per Sec:     4630, Lr: 0.000300
2024-05-27 00:08:38,739 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.614694, Batch Acc: 0.496941, Tokens per Sec:     4892, Lr: 0.000300
2024-05-27 00:08:38,740 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:08:38,740 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:09:03,100 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.91, acc:   0.48, generation: 24.3381[sec], evaluation: 0.0000[sec]
2024-05-27 00:09:03,102 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:09:03,264 - INFO - joeynmt.helpers - delete models/word_level_model/1000.ckpt
2024-05-27 00:09:03,265 - INFO - joeynmt.training - Example #0
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', '&apos;ve', 'got', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', 'the', '<unk>', '<unk>', ',', '<unk>', 'the', '<unk>', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'percent', '.', '</s>']
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Hypothesis: The last year I've got these <unk> for <unk> that the <unk> <unk> <unk> <unk>, which is <unk> the <unk> <unk>, <unk> the <unk> of the United States, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> percent.
2024-05-27 00:09:03,266 - INFO - joeynmt.training - Example #1
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> of the problem because it doesn't <unk> the <unk> of the ice.
2024-05-27 00:09:03,266 - INFO - joeynmt.training - Example #2
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:09:03,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', '<unk>', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:09:03,266 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a sense, the heart of the <unk> <unk> of the global <unk>.
2024-05-27 00:09:03,266 - INFO - joeynmt.training - Example #3
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk> <unk>.
2024-05-27 00:09:03,267 - INFO - joeynmt.training - Example #4
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:09:03,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:09:03,267 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last last 25 years.
2024-05-27 00:09:18,381 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.721653, Batch Acc: 0.493220, Tokens per Sec:     4532, Lr: 0.000300
2024-05-27 00:09:33,339 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.615261, Batch Acc: 0.495391, Tokens per Sec:     4678, Lr: 0.000300
2024-05-27 00:09:48,339 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.645624, Batch Acc: 0.499483, Tokens per Sec:     4579, Lr: 0.000300
2024-05-27 00:10:02,239 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.761269, Batch Acc: 0.500600, Tokens per Sec:     4920, Lr: 0.000300
2024-05-27 00:10:15,931 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.631146, Batch Acc: 0.501789, Tokens per Sec:     5063, Lr: 0.000300
2024-05-27 00:10:15,932 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:10:15,932 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:10:43,644 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.71, acc:   0.48, generation: 27.6897[sec], evaluation: 0.0000[sec]
2024-05-27 00:10:43,645 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:10:43,807 - INFO - joeynmt.helpers - delete models/word_level_model/1500.ckpt
2024-05-27 00:10:43,808 - INFO - joeynmt.training - Example #0
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', '<unk>', '<unk>', ',', 'for', 'three', 'million', 'years', 'of', '<unk>', ',', '<unk>', ',', '<unk>', 'is', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'is', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> <unk> that the <unk> <unk> <unk>, which is <unk> <unk> <unk>, for three million years of <unk>, <unk>, <unk> is <unk> <unk> <unk>, <unk> is <unk> <unk> <unk>.
2024-05-27 00:10:43,809 - INFO - joeynmt.training - Example #1
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'so', 'this', '<unk>', '<unk>', '<unk>', 'because', 'it', 'doesn', '&apos;t', '<unk>', 'the', 'ice', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Hypothesis: And so this <unk> <unk> <unk> because it doesn't <unk> the ice of the ice.
2024-05-27 00:10:43,809 - INFO - joeynmt.training - Example #2
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:10:43,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', '<unk>', 'system', '.', '</s>']
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:10:43,809 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a sense, the heart of the <unk> of the <unk> system.
2024-05-27 00:10:43,810 - INFO - joeynmt.training - Example #3
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk>.
2024-05-27 00:10:43,810 - INFO - joeynmt.training - Example #4
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:10:43,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:10:43,810 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:10:57,975 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.653279, Batch Acc: 0.502402, Tokens per Sec:     4823, Lr: 0.000300
2024-05-27 00:11:12,304 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.437174, Batch Acc: 0.503470, Tokens per Sec:     4777, Lr: 0.000300
2024-05-27 00:11:27,469 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.594400, Batch Acc: 0.506506, Tokens per Sec:     4627, Lr: 0.000300
2024-05-27 00:11:42,371 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.621979, Batch Acc: 0.509513, Tokens per Sec:     4694, Lr: 0.000300
2024-05-27 00:11:57,166 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.701131, Batch Acc: 0.507435, Tokens per Sec:     4482, Lr: 0.000300
2024-05-27 00:11:57,167 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:11:57,167 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:12:23,540 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.49, generation: 26.3528[sec], evaluation: 0.0000[sec]
2024-05-27 00:12:23,541 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:12:23,705 - INFO - joeynmt.helpers - delete models/word_level_model/2000.ckpt
2024-05-27 00:12:23,706 - INFO - joeynmt.training - Example #0
2024-05-27 00:12:23,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:12:23,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:12:23,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', '<unk>', ',', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:12:23,706 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk>, which is about three million years had the size of the <unk>, <unk>, <unk> <unk>, <unk> <unk>, it's <unk> <unk>, it's <unk> <unk>.
2024-05-27 00:12:23,707 - INFO - joeynmt.training - Example #1
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', ',', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Hypothesis: So, this <unk> <unk>, because it's not <unk> the <unk> of the ice.
2024-05-27 00:12:23,707 - INFO - joeynmt.training - Example #2
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of the global <unk>.
2024-05-27 00:12:23,707 - INFO - joeynmt.training - Example #3
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:12:23,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:12:23,707 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:12:23,708 - INFO - joeynmt.training - Example #4
2024-05-27 00:12:23,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:12:23,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:12:23,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:12:23,708 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:12:23,708 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:12:23,708 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:12:38,423 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.541746, Batch Acc: 0.510306, Tokens per Sec:     4708, Lr: 0.000300
2024-05-27 00:12:52,706 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.780945, Batch Acc: 0.515142, Tokens per Sec:     4853, Lr: 0.000300
2024-05-27 00:13:06,748 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.833507, Batch Acc: 0.513556, Tokens per Sec:     4812, Lr: 0.000300
2024-05-27 00:13:22,449 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.562763, Batch Acc: 0.515656, Tokens per Sec:     4473, Lr: 0.000300
2024-05-27 00:13:36,944 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.566893, Batch Acc: 0.517988, Tokens per Sec:     4801, Lr: 0.000300
2024-05-27 00:13:36,944 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:13:36,944 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:14:04,211 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.50, generation: 27.2441[sec], evaluation: 0.0000[sec]
2024-05-27 00:14:04,211 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:14:04,379 - INFO - joeynmt.helpers - delete models/word_level_model/2500.ckpt
2024-05-27 00:14:04,380 - INFO - joeynmt.training - Example #0
2024-05-27 00:14:04,380 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:14:04,380 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:14:04,380 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:14:04,380 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:14:04,380 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for <unk> that the <unk> <unk> <unk>, which for almost three million years has had the size of the United States <unk>, <unk> <unk> 40 percent.
2024-05-27 00:14:04,381 - INFO - joeynmt.training - Example #1
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Hypothesis: So, this <unk> <unk> the <unk> of the <unk> of the ice.
2024-05-27 00:14:04,381 - INFO - joeynmt.training - Example #2
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', 'global', '<unk>', 'system', '.', '</s>']
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is, in a sense, the heart of the global <unk> system.
2024-05-27 00:14:04,381 - INFO - joeynmt.training - Example #3
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:14:04,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:14:04,381 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk> and <unk>.
2024-05-27 00:14:04,381 - INFO - joeynmt.training - Example #4
2024-05-27 00:14:04,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:14:04,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:14:04,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:14:04,382 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:14:04,382 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:14:04,382 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:14:18,982 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.604054, Batch Acc: 0.522544, Tokens per Sec:     4692, Lr: 0.000300
2024-05-27 00:14:33,468 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.600754, Batch Acc: 0.513300, Tokens per Sec:     4807, Lr: 0.000300
2024-05-27 00:14:47,212 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.697147, Batch Acc: 0.513479, Tokens per Sec:     4958, Lr: 0.000300
2024-05-27 00:15:01,129 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.541271, Batch Acc: 0.519502, Tokens per Sec:     5057, Lr: 0.000300
2024-05-27 00:15:15,703 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.385615, Batch Acc: 0.521227, Tokens per Sec:     4763, Lr: 0.000300
2024-05-27 00:15:15,703 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:15:15,703 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:15:42,034 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.26, acc:   0.50, generation: 26.3099[sec], evaluation: 0.0000[sec]
2024-05-27 00:15:42,034 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:15:42,194 - INFO - joeynmt.helpers - delete models/word_level_model/3000.ckpt
2024-05-27 00:15:42,195 - INFO - joeynmt.training - Example #0
2024-05-27 00:15:42,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:15:42,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:15:42,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'and', '<unk>', '<unk>', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:15:42,195 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:15:42,195 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:15:42,195 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, and <unk> <unk> <unk> 40 percent.
2024-05-27 00:15:42,195 - INFO - joeynmt.training - Example #1
2024-05-27 00:15:42,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', '<unk>', 'the', 'ice', '.', '</s>']
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Hypothesis: So, this <unk> the <unk> of the problem because it doesn't <unk> the ice.
2024-05-27 00:15:42,196 - INFO - joeynmt.training - Example #2
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', '<unk>', '.', '</s>']
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart <unk> of global <unk>.
2024-05-27 00:15:42,196 - INFO - joeynmt.training - Example #3
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '&apos;re', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:15:42,196 - INFO - joeynmt.training - 	Hypothesis: You're <unk> and <unk> <unk>.
2024-05-27 00:15:42,196 - INFO - joeynmt.training - Example #4
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:15:42,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:15:42,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:15:42,197 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:15:42,197 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:15:42,197 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:15:56,832 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.692442, Batch Acc: 0.519910, Tokens per Sec:     4647, Lr: 0.000300
2024-05-27 00:16:11,127 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.694463, Batch Acc: 0.519856, Tokens per Sec:     4732, Lr: 0.000300
2024-05-27 00:16:25,671 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.674555, Batch Acc: 0.524233, Tokens per Sec:     4776, Lr: 0.000300
2024-05-27 00:16:40,276 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.537821, Batch Acc: 0.524896, Tokens per Sec:     4765, Lr: 0.000300
2024-05-27 00:16:54,258 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.533805, Batch Acc: 0.518546, Tokens per Sec:     4763, Lr: 0.000300
2024-05-27 00:16:54,258 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:16:54,258 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:17:21,380 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.51, generation: 27.1010[sec], evaluation: 0.0000[sec]
2024-05-27 00:17:21,381 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:17:21,545 - INFO - joeynmt.helpers - delete models/word_level_model/3500.ckpt
2024-05-27 00:17:21,545 - INFO - joeynmt.training - Example #0
2024-05-27 00:17:21,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:17:21,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:17:21,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', 'is', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>']
2024-05-27 00:17:21,545 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:17:21,545 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> to <unk> that the <unk> <unk> <unk>, which is about three million years of the United States, <unk> <unk>, <unk> is <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk>
2024-05-27 00:17:21,546 - INFO - joeynmt.training - Example #1
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '&apos;m', 'going', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '.', '</s>']
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Hypothesis: And I'm going to <unk> the <unk> of the problem because it's not <unk> the ice.
2024-05-27 00:17:21,546 - INFO - joeynmt.training - Example #2
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'climate', 'change', '.', '</s>']
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart of the <unk> of the climate change.
2024-05-27 00:17:21,546 - INFO - joeynmt.training - Example #3
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:17:21,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:17:21,546 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:17:21,546 - INFO - joeynmt.training - Example #4
2024-05-27 00:17:21,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:17:21,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:17:21,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:17:21,547 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:17:21,547 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:17:21,547 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 00:17:36,437 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.405911, Batch Acc: 0.522240, Tokens per Sec:     4621, Lr: 0.000300
2024-05-27 00:17:46,086 - INFO - joeynmt.training - Epoch   2: total training loss 5039.52
2024-05-27 00:17:46,086 - INFO - joeynmt.training - EPOCH 3
2024-05-27 00:17:50,006 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     1.500089, Batch Acc: 0.536850, Tokens per Sec:     4860, Lr: 0.000300
2024-05-27 00:18:04,798 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.465780, Batch Acc: 0.541602, Tokens per Sec:     4610, Lr: 0.000300
2024-05-27 00:18:20,083 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.448879, Batch Acc: 0.542797, Tokens per Sec:     4565, Lr: 0.000300
2024-05-27 00:18:34,884 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.576738, Batch Acc: 0.537970, Tokens per Sec:     4767, Lr: 0.000300
2024-05-27 00:18:34,886 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:18:34,886 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:19:03,026 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.12, acc:   0.51, generation: 28.1182[sec], evaluation: 0.0000[sec]
2024-05-27 00:19:03,026 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:19:03,188 - INFO - joeynmt.helpers - delete models/word_level_model/4000.ckpt
2024-05-27 00:19:03,188 - INFO - joeynmt.training - Example #0
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'almost', 'three', 'million', 'years', ',', 'the', 'United', 'States', 'has', 'had', 'the', 'size', 'of', '<unk>', '<unk>', ',', '<unk>', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> <unk> that the <unk> <unk> <unk>, which is for almost three million years, the United States has had the size of <unk> <unk>, <unk> is <unk> 40 percent.
2024-05-27 00:19:03,189 - INFO - joeynmt.training - Example #1
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> of the problem because it's not <unk> the ice <unk>.
2024-05-27 00:19:03,189 - INFO - joeynmt.training - Example #2
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:19:03,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:19:03,189 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart of the climate system.
2024-05-27 00:19:03,189 - INFO - joeynmt.training - Example #3
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:19:03,190 - INFO - joeynmt.training - Example #4
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:19:03,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '25', 'years', '.', '</s>']
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:19:03,190 - INFO - joeynmt.training - 	Hypothesis: The next next 25 years.
2024-05-27 00:19:17,311 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.640502, Batch Acc: 0.535677, Tokens per Sec:     4730, Lr: 0.000300
2024-05-27 00:19:31,167 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.593088, Batch Acc: 0.540485, Tokens per Sec:     4939, Lr: 0.000300
2024-05-27 00:19:45,803 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.428406, Batch Acc: 0.543866, Tokens per Sec:     4790, Lr: 0.000300
2024-05-27 00:20:00,242 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.639725, Batch Acc: 0.542442, Tokens per Sec:     4769, Lr: 0.000300
2024-05-27 00:20:14,624 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.439758, Batch Acc: 0.541252, Tokens per Sec:     4833, Lr: 0.000300
2024-05-27 00:20:14,624 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:20:14,624 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:20:45,091 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.07, acc:   0.51, generation: 30.4457[sec], evaluation: 0.0000[sec]
2024-05-27 00:20:45,093 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:20:45,256 - INFO - joeynmt.helpers - delete models/word_level_model/4500.ckpt
2024-05-27 00:20:45,257 - INFO - joeynmt.training - Example #0
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'is', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> <unk>, which <unk> <unk> <unk>, which is for almost three million years had the size of the United States <unk>, <unk> is <unk> <unk>, <unk> <unk>, <unk> is <unk> 40 percent.
2024-05-27 00:20:45,257 - INFO - joeynmt.training - Example #1
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', 'of', 'ice', '.', '</s>']
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:20:45,257 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> of the problem because it's not <unk> the ice of ice.
2024-05-27 00:20:45,257 - INFO - joeynmt.training - Example #2
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:20:45,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of the global <unk>.
2024-05-27 00:20:45,258 - INFO - joeynmt.training - Example #3
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk>.
2024-05-27 00:20:45,258 - INFO - joeynmt.training - Example #4
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:20:45,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:20:45,258 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:21:00,882 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.446679, Batch Acc: 0.538578, Tokens per Sec:     4394, Lr: 0.000300
2024-05-27 00:21:15,254 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.626612, Batch Acc: 0.541467, Tokens per Sec:     4841, Lr: 0.000300
2024-05-27 00:21:29,617 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.546058, Batch Acc: 0.539655, Tokens per Sec:     4811, Lr: 0.000300
2024-05-27 00:21:44,542 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.613621, Batch Acc: 0.539412, Tokens per Sec:     4614, Lr: 0.000300
2024-05-27 00:21:58,116 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.454000, Batch Acc: 0.542612, Tokens per Sec:     5079, Lr: 0.000300
2024-05-27 00:21:58,117 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:21:58,117 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:22:28,126 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.00, acc:   0.51, generation: 29.9872[sec], evaluation: 0.0000[sec]
2024-05-27 00:22:28,127 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:22:28,292 - INFO - joeynmt.helpers - delete models/word_level_model/5000.ckpt
2024-05-27 00:22:28,293 - INFO - joeynmt.training - Example #0
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> for <unk> that <unk> <unk> <unk>, which for almost three million years, had the size of the United States <unk>, <unk> is <unk> 40 percent.
2024-05-27 00:22:28,293 - INFO - joeynmt.training - Example #1
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'of', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'it', '.', '</s>']
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> of the <unk> of the problem because it's not <unk> it.
2024-05-27 00:22:28,293 - INFO - joeynmt.training - Example #2
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:22:28,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'system', '.', '</s>']
2024-05-27 00:22:28,293 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a certain sense, the heart <unk> of global system.
2024-05-27 00:22:28,294 - INFO - joeynmt.training - Example #3
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:22:28,294 - INFO - joeynmt.training - Example #4
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:22:28,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:22:28,294 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:22:42,218 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.577754, Batch Acc: 0.542631, Tokens per Sec:     5017, Lr: 0.000300
2024-05-27 00:22:56,609 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.200622, Batch Acc: 0.540987, Tokens per Sec:     4762, Lr: 0.000300
2024-05-27 00:23:10,295 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.492962, Batch Acc: 0.548696, Tokens per Sec:     5149, Lr: 0.000300
2024-05-27 00:23:24,583 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.343227, Batch Acc: 0.538104, Tokens per Sec:     4889, Lr: 0.000300
2024-05-27 00:23:39,519 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.440744, Batch Acc: 0.544396, Tokens per Sec:     4570, Lr: 0.000300
2024-05-27 00:23:39,519 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:23:39,519 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:24:08,096 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.52, generation: 28.5550[sec], evaluation: 0.0000[sec]
2024-05-27 00:24:08,097 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:24:08,262 - INFO - joeynmt.helpers - delete models/word_level_model/5500.ckpt
2024-05-27 00:24:08,264 - INFO - joeynmt.training - Example #0
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'for', 'almost', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'of', 'the', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'is']
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these <unk> <unk> <unk> <unk>, which <unk> <unk> <unk> <unk>, for almost three million years, had the size of the United States, <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> of the <unk> <unk>, <unk> <unk> <unk> <unk>, which is
2024-05-27 00:24:08,264 - INFO - joeynmt.training - Example #1
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:24:08,264 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', 'shows', 'the', 'ice', '.', '</s>']
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:24:08,264 - INFO - joeynmt.training - 	Hypothesis: And this <unk> the <unk> of the problem because it's not shows the ice.
2024-05-27 00:24:08,264 - INFO - joeynmt.training - Example #2
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'global', 'global', 'global', 'system', '.', '</s>']
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart of global global global system.
2024-05-27 00:24:08,265 - INFO - joeynmt.training - Example #3
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:24:08,265 - INFO - joeynmt.training - Example #4
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:24:08,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:24:08,265 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:24:22,162 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.402132, Batch Acc: 0.541963, Tokens per Sec:     4872, Lr: 0.000300
2024-05-27 00:24:35,986 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.536852, Batch Acc: 0.541985, Tokens per Sec:     4920, Lr: 0.000300
2024-05-27 00:24:50,818 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.611639, Batch Acc: 0.538508, Tokens per Sec:     4616, Lr: 0.000300
2024-05-27 00:25:04,840 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.494320, Batch Acc: 0.541540, Tokens per Sec:     4889, Lr: 0.000300
2024-05-27 00:25:19,439 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.384689, Batch Acc: 0.543728, Tokens per Sec:     4667, Lr: 0.000300
2024-05-27 00:25:19,439 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:25:19,439 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:25:48,221 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.90, acc:   0.52, generation: 28.7605[sec], evaluation: 0.0000[sec]
2024-05-27 00:25:48,221 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:25:48,388 - INFO - joeynmt.helpers - delete models/word_level_model/6000.ckpt
2024-05-27 00:25:48,389 - INFO - joeynmt.training - Example #0
2024-05-27 00:25:48,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:25:48,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:25:48,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'almost', 'three', 'million', 'years', 'years', 'old', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'is', '<unk>', '<unk>', ',', 'and', 'it', '&apos;s', '<unk>', 'of', 'the', '<unk>', '<unk>', ',', 'which', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that <unk> <unk> <unk>, which is for almost three million years years old, had the size of the United States <unk>, is <unk> <unk>, and it's <unk> of the <unk> <unk>, which is <unk> 40 percent.
2024-05-27 00:25:48,390 - INFO - joeynmt.training - Example #1
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', 'of', 'ice', '.', '</s>']
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Hypothesis: So, this <unk> <unk> the <unk> of the problem because it doesn't show the ice of ice.
2024-05-27 00:25:48,390 - INFO - joeynmt.training - Example #2
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', '<unk>', '.', '</s>']
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:25:48,390 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of global <unk>.
2024-05-27 00:25:48,390 - INFO - joeynmt.training - Example #3
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:25:48,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> <unk>.
2024-05-27 00:25:48,391 - INFO - joeynmt.training - Example #4
2024-05-27 00:25:48,391 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:25:48,391 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:25:48,391 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:25:48,391 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:26:03,385 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.534573, Batch Acc: 0.545241, Tokens per Sec:     4556, Lr: 0.000300
2024-05-27 00:26:17,350 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.494039, Batch Acc: 0.551457, Tokens per Sec:     4892, Lr: 0.000300
2024-05-27 00:26:32,053 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.599308, Batch Acc: 0.544339, Tokens per Sec:     4606, Lr: 0.000300
2024-05-27 00:26:47,124 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.336859, Batch Acc: 0.543249, Tokens per Sec:     4548, Lr: 0.000300
2024-05-27 00:27:02,108 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.355982, Batch Acc: 0.548888, Tokens per Sec:     4504, Lr: 0.000300
2024-05-27 00:27:02,108 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:27:02,108 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:27:32,373 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.52, generation: 30.2427[sec], evaluation: 0.0000[sec]
2024-05-27 00:27:32,374 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:27:32,532 - INFO - joeynmt.helpers - delete models/word_level_model/6500.ckpt
2024-05-27 00:27:32,533 - INFO - joeynmt.training - Example #0
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> is <unk> 40 percent.
2024-05-27 00:27:32,533 - INFO - joeynmt.training - Example #1
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:27:32,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '&apos;m', 'going', 'to', 'do', 'this', '<unk>', 'of', 'the', '<unk>', 'of', 'ice', '.', '</s>']
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:27:32,533 - INFO - joeynmt.training - 	Hypothesis: And I'm going to do this <unk> of the <unk> of ice.
2024-05-27 00:27:32,533 - INFO - joeynmt.training - Example #2
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart <unk> of the <unk> of climate system.
2024-05-27 00:27:32,534 - INFO - joeynmt.training - Example #3
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '&apos;re', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Hypothesis: You're <unk> <unk> and <unk> <unk>.
2024-05-27 00:27:32,534 - INFO - joeynmt.training - Example #4
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:27:32,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:27:32,534 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:27:47,834 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.466377, Batch Acc: 0.542975, Tokens per Sec:     4503, Lr: 0.000300
2024-05-27 00:28:02,083 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.596429, Batch Acc: 0.546994, Tokens per Sec:     4903, Lr: 0.000300
2024-05-27 00:28:10,698 - INFO - joeynmt.training - Epoch   3: total training loss 4640.73
2024-05-27 00:28:10,698 - INFO - joeynmt.training - EPOCH 4
2024-05-27 00:28:16,507 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     1.408024, Batch Acc: 0.558791, Tokens per Sec:     4836, Lr: 0.000300
2024-05-27 00:28:31,073 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.440284, Batch Acc: 0.559021, Tokens per Sec:     4685, Lr: 0.000300
2024-05-27 00:28:46,525 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.373634, Batch Acc: 0.564885, Tokens per Sec:     4468, Lr: 0.000300
2024-05-27 00:28:46,526 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:28:46,526 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:29:12,891 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.83, acc:   0.52, generation: 26.3438[sec], evaluation: 0.0000[sec]
2024-05-27 00:29:12,892 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:29:13,057 - INFO - joeynmt.helpers - delete models/word_level_model/7000.ckpt
2024-05-27 00:29:13,058 - INFO - joeynmt.training - Example #0
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'old', 'had', 'the', 'size', 'of', 'the', '<unk>', '<unk>', ',', 'it', '&apos;s', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk>, which is about three million years old had the size of the <unk> <unk>, it's <unk> 40 percent.
2024-05-27 00:29:13,058 - INFO - joeynmt.training - Example #1
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '&apos;m', 'going', 'to', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '.', '</s>']
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:29:13,058 - INFO - joeynmt.training - 	Hypothesis: And I'm going to <unk> the problem because it doesn't show the ice.
2024-05-27 00:29:13,058 - INFO - joeynmt.training - Example #2
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:29:13,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart heart <unk> of climate system.
2024-05-27 00:29:13,059 - INFO - joeynmt.training - Example #3
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk>.
2024-05-27 00:29:13,059 - INFO - joeynmt.training - Example #4
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:29:13,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:29:13,059 - INFO - joeynmt.training - 	Hypothesis: The next next <unk> is going to be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 00:29:27,577 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.338094, Batch Acc: 0.559485, Tokens per Sec:     4735, Lr: 0.000300
2024-05-27 00:29:42,851 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.357115, Batch Acc: 0.565492, Tokens per Sec:     4563, Lr: 0.000300
2024-05-27 00:29:57,399 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.480202, Batch Acc: 0.562145, Tokens per Sec:     4606, Lr: 0.000300
2024-05-27 00:30:12,529 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.451649, Batch Acc: 0.561432, Tokens per Sec:     4525, Lr: 0.000300
2024-05-27 00:30:27,327 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.417552, Batch Acc: 0.557153, Tokens per Sec:     4710, Lr: 0.000300
2024-05-27 00:30:27,327 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:30:27,327 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:30:53,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.52, generation: 26.6151[sec], evaluation: 0.0000[sec]
2024-05-27 00:30:53,964 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:30:54,130 - INFO - joeynmt.helpers - delete models/word_level_model/7500.ckpt
2024-05-27 00:30:54,131 - INFO - joeynmt.training - Example #0
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'it', '&apos;s', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> <unk> <unk>, which <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, it's <unk> 40 percent.
2024-05-27 00:30:54,131 - INFO - joeynmt.training - Example #1
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', ',', 'this', '<unk>', 'the', '<unk>', 'of', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Hypothesis: So, this <unk> the <unk> of the <unk> of the ice.
2024-05-27 00:30:54,131 - INFO - joeynmt.training - Example #2
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:30:54,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'global', 'warming', '.', '</s>']
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:30:54,131 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of global warming.
2024-05-27 00:30:54,132 - INFO - joeynmt.training - Example #3
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk>.
2024-05-27 00:30:54,132 - INFO - joeynmt.training - Example #4
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:30:54,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:30:54,132 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:31:08,641 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.292702, Batch Acc: 0.560624, Tokens per Sec:     4808, Lr: 0.000300
2024-05-27 00:31:23,393 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.568638, Batch Acc: 0.557716, Tokens per Sec:     4683, Lr: 0.000300
2024-05-27 00:31:37,589 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.267811, Batch Acc: 0.558320, Tokens per Sec:     4862, Lr: 0.000300
2024-05-27 00:31:51,873 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.410406, Batch Acc: 0.557072, Tokens per Sec:     4765, Lr: 0.000300
2024-05-27 00:32:05,926 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.337625, Batch Acc: 0.559196, Tokens per Sec:     4991, Lr: 0.000300
2024-05-27 00:32:05,926 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:32:05,926 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:32:34,426 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.79, acc:   0.53, generation: 28.4795[sec], evaluation: 0.0000[sec]
2024-05-27 00:32:34,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:32:34,591 - INFO - joeynmt.helpers - delete models/word_level_model/8000.ckpt
2024-05-27 00:32:34,593 - INFO - joeynmt.training - Example #0
2024-05-27 00:32:34,593 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:32:34,593 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:32:34,593 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:32:34,593 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that <unk> <unk> <unk>, which is almost three million years had the size of the United States <unk> <unk>, <unk>, <unk> 40 percent.
2024-05-27 00:32:34,594 - INFO - joeynmt.training - Example #1
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:32:34,594 - INFO - joeynmt.training - Example #2
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a certain sense, the heart of the <unk> of climate system.
2024-05-27 00:32:34,594 - INFO - joeynmt.training - Example #3
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:32:34,594 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:32:34,594 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk>.
2024-05-27 00:32:34,595 - INFO - joeynmt.training - Example #4
2024-05-27 00:32:34,595 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:32:34,595 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:32:34,595 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:32:34,595 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:32:34,595 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:32:34,595 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:32:49,102 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     1.441377, Batch Acc: 0.561584, Tokens per Sec:     4676, Lr: 0.000300
2024-05-27 00:33:03,539 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     1.514533, Batch Acc: 0.564426, Tokens per Sec:     4930, Lr: 0.000300
2024-05-27 00:33:18,626 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.485341, Batch Acc: 0.560896, Tokens per Sec:     4531, Lr: 0.000300
2024-05-27 00:33:33,630 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.287058, Batch Acc: 0.556837, Tokens per Sec:     4639, Lr: 0.000300
2024-05-27 00:33:48,195 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.410113, Batch Acc: 0.558679, Tokens per Sec:     4790, Lr: 0.000300
2024-05-27 00:33:48,195 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:33:48,195 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:34:18,982 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.71, acc:   0.53, generation: 30.7653[sec], evaluation: 0.0000[sec]
2024-05-27 00:34:18,984 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:34:19,150 - INFO - joeynmt.helpers - delete models/word_level_model/8500.ckpt
2024-05-27 00:34:19,151 - INFO - joeynmt.training - Example #0
2024-05-27 00:34:19,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:34:19,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:34:19,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '40', 'percent', '.', '</s>']
2024-05-27 00:34:19,151 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:34:19,151 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:34:19,151 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> to <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the <unk> <unk> <unk>, which is 40 percent.
2024-05-27 00:34:19,152 - INFO - joeynmt.training - Example #1
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'so', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Hypothesis: And so this <unk> the <unk> of the problem because it's not <unk> the ice <unk>.
2024-05-27 00:34:19,152 - INFO - joeynmt.training - Example #2
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'climate', 'system', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart <unk> of the climate system of climate system.
2024-05-27 00:34:19,152 - INFO - joeynmt.training - Example #3
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:34:19,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:34:19,152 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> <unk>.
2024-05-27 00:34:19,152 - INFO - joeynmt.training - Example #4
2024-05-27 00:34:19,153 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:34:19,153 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:34:19,153 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:34:19,153 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:34:19,153 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:34:19,153 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:34:34,490 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.374008, Batch Acc: 0.559510, Tokens per Sec:     4572, Lr: 0.000300
2024-05-27 00:34:49,043 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.326526, Batch Acc: 0.564127, Tokens per Sec:     4786, Lr: 0.000300
2024-05-27 00:35:03,749 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.535629, Batch Acc: 0.559561, Tokens per Sec:     4639, Lr: 0.000300
2024-05-27 00:35:17,925 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.340980, Batch Acc: 0.560949, Tokens per Sec:     4951, Lr: 0.000300
2024-05-27 00:35:32,485 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.489313, Batch Acc: 0.559056, Tokens per Sec:     4731, Lr: 0.000300
2024-05-27 00:35:32,486 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:35:32,486 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:35:59,914 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.71, acc:   0.53, generation: 27.4066[sec], evaluation: 0.0000[sec]
2024-05-27 00:35:59,915 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:36:00,076 - INFO - joeynmt.helpers - delete models/word_level_model/9000.ckpt
2024-05-27 00:36:00,077 - INFO - joeynmt.training - Example #0
2024-05-27 00:36:00,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:36:00,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:36:00,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> 40 percent.
2024-05-27 00:36:00,078 - INFO - joeynmt.training - Example #1
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', 'of', 'the', '<unk>', '.', '</s>']
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the ice of the <unk>.
2024-05-27 00:36:00,078 - INFO - joeynmt.training - Example #2
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:36:00,078 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart <unk> of the global <unk>.
2024-05-27 00:36:00,078 - INFO - joeynmt.training - Example #3
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:36:00,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Hypothesis: You <unk> <unk> and <unk> <unk>.
2024-05-27 00:36:00,079 - INFO - joeynmt.training - Example #4
2024-05-27 00:36:00,079 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:36:00,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:36:00,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:36:00,079 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:36:15,858 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.320243, Batch Acc: 0.556357, Tokens per Sec:     4285, Lr: 0.000300
2024-05-27 00:36:31,110 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.352582, Batch Acc: 0.563193, Tokens per Sec:     4465, Lr: 0.000300
2024-05-27 00:36:45,847 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.483997, Batch Acc: 0.560180, Tokens per Sec:     4583, Lr: 0.000300
2024-05-27 00:37:00,588 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.380895, Batch Acc: 0.557595, Tokens per Sec:     4689, Lr: 0.000300
2024-05-27 00:37:15,100 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.389601, Batch Acc: 0.556700, Tokens per Sec:     4863, Lr: 0.000300
2024-05-27 00:37:15,100 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:37:15,100 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:37:43,500 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.53, generation: 28.3789[sec], evaluation: 0.0000[sec]
2024-05-27 00:37:43,500 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:37:43,667 - INFO - joeynmt.helpers - delete models/word_level_model/9500.ckpt
2024-05-27 00:37:43,668 - INFO - joeynmt.training - Example #0
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'it', '&apos;s', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, it's <unk> 40 percent.
2024-05-27 00:37:43,668 - INFO - joeynmt.training - Example #1
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'we', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:37:43,668 - INFO - joeynmt.training - 	Hypothesis: And we <unk> this <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:37:43,668 - INFO - joeynmt.training - Example #2
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:37:43,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'change', '.', '</s>']
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a certain sense, the heart of the climate change.
2024-05-27 00:37:43,669 - INFO - joeynmt.training - Example #3
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in the summer.
2024-05-27 00:37:43,669 - INFO - joeynmt.training - Example #4
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:37:43,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:37:43,669 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:37:58,565 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.441793, Batch Acc: 0.561722, Tokens per Sec:     4698, Lr: 0.000300
2024-05-27 00:38:13,362 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.397196, Batch Acc: 0.564056, Tokens per Sec:     4600, Lr: 0.000300
2024-05-27 00:38:28,357 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.436657, Batch Acc: 0.558659, Tokens per Sec:     4537, Lr: 0.000300
2024-05-27 00:38:33,610 - INFO - joeynmt.training - Epoch   4: total training loss 4403.00
2024-05-27 00:38:33,611 - INFO - joeynmt.training - EPOCH 5
2024-05-27 00:38:42,435 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     1.431486, Batch Acc: 0.582911, Tokens per Sec:     4780, Lr: 0.000300
2024-05-27 00:38:57,073 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     1.334096, Batch Acc: 0.580536, Tokens per Sec:     4643, Lr: 0.000300
2024-05-27 00:38:57,074 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:38:57,074 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:39:25,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.67, acc:   0.53, generation: 28.8682[sec], evaluation: 0.0000[sec]
2024-05-27 00:39:25,966 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:39:26,131 - INFO - joeynmt.helpers - delete models/word_level_model/10000.ckpt
2024-05-27 00:39:26,132 - INFO - joeynmt.training - Example #0
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', '40', 'percent', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:39:26,132 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:39:26,132 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:39:26,132 - INFO - joeynmt.training - 	Hypothesis: I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years of the <unk> <unk> <unk>, which is 40 percent of the United States, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> 40 percent.
2024-05-27 00:39:26,132 - INFO - joeynmt.training - Example #1
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:39:26,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:39:26,132 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Hypothesis: And this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:39:26,133 - INFO - joeynmt.training - Example #2
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart of the global <unk>.
2024-05-27 00:39:26,133 - INFO - joeynmt.training - Example #3
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:39:26,133 - INFO - joeynmt.training - Example #4
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:39:26,133 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:39:26,133 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:39:42,117 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.335759, Batch Acc: 0.580774, Tokens per Sec:     4277, Lr: 0.000300
2024-05-27 00:39:56,614 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.273767, Batch Acc: 0.579751, Tokens per Sec:     4818, Lr: 0.000300
2024-05-27 00:40:11,982 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.247234, Batch Acc: 0.577834, Tokens per Sec:     4417, Lr: 0.000300
2024-05-27 00:40:26,099 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.289451, Batch Acc: 0.578178, Tokens per Sec:     4826, Lr: 0.000300
2024-05-27 00:40:40,451 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.367182, Batch Acc: 0.575557, Tokens per Sec:     4795, Lr: 0.000300
2024-05-27 00:40:40,452 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:40:40,452 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:41:09,597 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.68, acc:   0.53, generation: 29.1240[sec], evaluation: 0.0000[sec]
2024-05-27 00:41:09,763 - INFO - joeynmt.helpers - delete models/word_level_model/10500.ckpt
2024-05-27 00:41:09,765 - INFO - joeynmt.training - Example #0
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'the', '<unk>', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:41:09,765 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:41:09,765 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:41:09,765 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, the <unk> of the United States <unk>, <unk> 40 percent.
2024-05-27 00:41:09,765 - INFO - joeynmt.training - Example #1
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:41:09,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:41:09,765 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:41:09,765 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:41:09,766 - INFO - joeynmt.training - Example #2
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart of <unk> of climate change.
2024-05-27 00:41:09,766 - INFO - joeynmt.training - Example #3
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', 'it', '<unk>', '.', '</s>']
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and it <unk>.
2024-05-27 00:41:09,766 - INFO - joeynmt.training - Example #4
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:41:09,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:41:09,766 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:41:24,195 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.383103, Batch Acc: 0.571486, Tokens per Sec:     4602, Lr: 0.000300
2024-05-27 00:41:38,990 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.428664, Batch Acc: 0.575936, Tokens per Sec:     4708, Lr: 0.000300
2024-05-27 00:41:53,298 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     1.378784, Batch Acc: 0.576669, Tokens per Sec:     4953, Lr: 0.000300
2024-05-27 00:42:08,997 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     1.366743, Batch Acc: 0.572294, Tokens per Sec:     4446, Lr: 0.000300
2024-05-27 00:42:23,889 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     1.345808, Batch Acc: 0.577745, Tokens per Sec:     4740, Lr: 0.000300
2024-05-27 00:42:23,889 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:42:23,889 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:42:52,300 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.53, generation: 28.3897[sec], evaluation: 0.0000[sec]
2024-05-27 00:42:52,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:42:52,467 - INFO - joeynmt.helpers - delete models/word_level_model/11000.ckpt
2024-05-27 00:42:52,468 - INFO - joeynmt.training - Example #0
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', 'it', '&apos;s', '<unk>', '<unk>', ',', 'it', 'was', '40', 'percent', '.', '</s>']
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years, had the size of the United States, it's <unk> <unk>, it was 40 percent.
2024-05-27 00:42:52,468 - INFO - joeynmt.training - Example #1
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '.', '</s>']
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:42:52,468 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it's not <unk> the ice.
2024-05-27 00:42:52,468 - INFO - joeynmt.training - Example #2
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:42:52,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart <unk> of the global system.
2024-05-27 00:42:52,469 - INFO - joeynmt.training - Example #3
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:42:52,469 - INFO - joeynmt.training - Example #4
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:42:52,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:42:52,469 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:43:07,130 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     1.421481, Batch Acc: 0.568011, Tokens per Sec:     4699, Lr: 0.000300
2024-05-27 00:43:22,605 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     1.532053, Batch Acc: 0.574916, Tokens per Sec:     4591, Lr: 0.000300
2024-05-27 00:43:37,973 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     1.288878, Batch Acc: 0.572325, Tokens per Sec:     4417, Lr: 0.000300
2024-05-27 00:43:52,559 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     1.273781, Batch Acc: 0.570918, Tokens per Sec:     4797, Lr: 0.000300
2024-05-27 00:44:07,591 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     1.379697, Batch Acc: 0.568257, Tokens per Sec:     4594, Lr: 0.000300
2024-05-27 00:44:07,592 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:44:07,592 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:44:35,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.66, acc:   0.53, generation: 28.3837[sec], evaluation: 0.0000[sec]
2024-05-27 00:44:36,165 - INFO - joeynmt.helpers - delete models/word_level_model/11500.ckpt
2024-05-27 00:44:36,165 - INFO - joeynmt.training - Example #0
2024-05-27 00:44:36,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:44:36,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:44:36,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'has', 'been', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> for <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, has been <unk> 40 percent.
2024-05-27 00:44:36,166 - INFO - joeynmt.training - Example #1
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '<unk>', 'this', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Hypothesis: And I <unk> this <unk> the problem because it doesn't show the ice <unk>.
2024-05-27 00:44:36,166 - INFO - joeynmt.training - Example #2
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:44:36,166 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a sense, the heart <unk> of the climate system.
2024-05-27 00:44:36,166 - INFO - joeynmt.training - Example #3
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:44:36,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk>.
2024-05-27 00:44:36,167 - INFO - joeynmt.training - Example #4
2024-05-27 00:44:36,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:44:36,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:44:36,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'over', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:44:36,167 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> over the last 25 years.
2024-05-27 00:44:51,211 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     1.584875, Batch Acc: 0.571935, Tokens per Sec:     4433, Lr: 0.000300
2024-05-27 00:45:06,662 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     1.284744, Batch Acc: 0.575507, Tokens per Sec:     4507, Lr: 0.000300
2024-05-27 00:45:21,009 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     1.407589, Batch Acc: 0.569977, Tokens per Sec:     4817, Lr: 0.000300
2024-05-27 00:45:36,003 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.384558, Batch Acc: 0.568969, Tokens per Sec:     4556, Lr: 0.000300
2024-05-27 00:45:51,172 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.449395, Batch Acc: 0.569304, Tokens per Sec:     4472, Lr: 0.000300
2024-05-27 00:45:51,172 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:45:51,172 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:46:18,733 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.63, acc:   0.53, generation: 27.5399[sec], evaluation: 0.0000[sec]
2024-05-27 00:46:18,734 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:46:18,902 - INFO - joeynmt.helpers - delete models/word_level_model/12000.ckpt
2024-05-27 00:46:18,903 - INFO - joeynmt.training - Example #0
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk>.
2024-05-27 00:46:18,903 - INFO - joeynmt.training - Example #1
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:46:18,903 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:46:18,903 - INFO - joeynmt.training - Example #2
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:46:18,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', '<unk>', '.', '</s>']
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart <unk> of the global <unk>.
2024-05-27 00:46:18,904 - INFO - joeynmt.training - Example #3
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:46:18,904 - INFO - joeynmt.training - Example #4
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:46:18,904 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:46:18,904 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 00:46:33,315 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.473663, Batch Acc: 0.570090, Tokens per Sec:     4780, Lr: 0.000300
2024-05-27 00:46:47,575 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.291095, Batch Acc: 0.572286, Tokens per Sec:     4917, Lr: 0.000300
2024-05-27 00:47:03,369 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     1.352187, Batch Acc: 0.565805, Tokens per Sec:     4233, Lr: 0.000300
2024-05-27 00:47:17,529 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     1.420979, Batch Acc: 0.565460, Tokens per Sec:     4934, Lr: 0.000300
2024-05-27 00:47:32,482 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     1.309506, Batch Acc: 0.572987, Tokens per Sec:     4659, Lr: 0.000300
2024-05-27 00:47:32,482 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:47:32,482 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:48:00,825 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.54, generation: 28.3213[sec], evaluation: 0.0000[sec]
2024-05-27 00:48:00,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:48:00,989 - INFO - joeynmt.helpers - delete models/word_level_model/13000.ckpt
2024-05-27 00:48:00,990 - INFO - joeynmt.training - Example #0
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> <unk> <unk>, which for about three million years had the size of the United States <unk>, <unk> <unk> <unk>, <unk> <unk> <unk>, <unk> of 40 percent.
2024-05-27 00:48:00,990 - INFO - joeynmt.training - Example #1
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:48:00,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:48:00,990 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:48:00,991 - INFO - joeynmt.training - Example #2
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', '<unk>', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the <unk> <unk> of climate change.
2024-05-27 00:48:00,991 - INFO - joeynmt.training - Example #3
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:48:00,991 - INFO - joeynmt.training - Example #4
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:48:00,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:48:00,991 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:48:16,390 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     1.339540, Batch Acc: 0.569840, Tokens per Sec:     4380, Lr: 0.000300
2024-05-27 00:48:32,525 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.328171, Batch Acc: 0.567723, Tokens per Sec:     4299, Lr: 0.000300
2024-05-27 00:48:47,000 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.541914, Batch Acc: 0.571264, Tokens per Sec:     4732, Lr: 0.000300
2024-05-27 00:49:01,429 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     1.417263, Batch Acc: 0.567362, Tokens per Sec:     4704, Lr: 0.000300
2024-05-27 00:49:04,573 - INFO - joeynmt.training - Epoch   5: total training loss 4256.07
2024-05-27 00:49:04,573 - INFO - joeynmt.training - EPOCH 6
2024-05-27 00:49:16,361 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     1.241125, Batch Acc: 0.590840, Tokens per Sec:     4459, Lr: 0.000300
2024-05-27 00:49:16,361 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:49:16,361 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:49:43,356 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.58, acc:   0.54, generation: 26.9739[sec], evaluation: 0.0000[sec]
2024-05-27 00:49:43,358 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:49:43,527 - INFO - joeynmt.helpers - delete models/word_level_model/12500.ckpt
2024-05-27 00:49:43,528 - INFO - joeynmt.training - Example #0
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', ',', '</s>']
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk>,
2024-05-27 00:49:43,528 - INFO - joeynmt.training - Example #1
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', 'do', 'this', '<unk>', '<unk>', ',', 'because', 'it', 'doesn', '&apos;t', 'show', 'you', 'in', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:49:43,528 - INFO - joeynmt.training - 	Hypothesis: I'm going to do this <unk> <unk>, because it doesn't show you in the ice <unk>.
2024-05-27 00:49:43,528 - INFO - joeynmt.training - Example #2
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:49:43,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the <unk> of climate system.
2024-05-27 00:49:43,529 - INFO - joeynmt.training - Example #3
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 00:49:43,529 - INFO - joeynmt.training - Example #4
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:49:43,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:49:43,529 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 00:49:57,786 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     1.244468, Batch Acc: 0.592492, Tokens per Sec:     4819, Lr: 0.000300
2024-05-27 00:50:12,380 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.480313, Batch Acc: 0.589187, Tokens per Sec:     4792, Lr: 0.000300
2024-05-27 00:50:27,371 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.311849, Batch Acc: 0.592194, Tokens per Sec:     4758, Lr: 0.000300
2024-05-27 00:50:41,221 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     1.430381, Batch Acc: 0.587037, Tokens per Sec:     4815, Lr: 0.000300
2024-05-27 00:50:55,427 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     1.281362, Batch Acc: 0.585589, Tokens per Sec:     4739, Lr: 0.000300
2024-05-27 00:50:55,427 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:50:55,427 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:51:21,575 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.60, acc:   0.53, generation: 26.1274[sec], evaluation: 0.0000[sec]
2024-05-27 00:51:21,741 - INFO - joeynmt.helpers - delete models/word_level_model/14000.ckpt
2024-05-27 00:51:21,743 - INFO - joeynmt.training - Example #0
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', 'has', 'the', 'size', 'of', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Hypothesis: I showed these <unk> <unk> <unk> <unk>, which for almost three million years, has the size of the <unk> <unk> <unk>, which for almost three million years, <unk> <unk> <unk>.
2024-05-27 00:51:21,743 - INFO - joeynmt.training - Example #1
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:51:21,743 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:51:21,743 - INFO - joeynmt.training - Example #2
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:51:21,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a certain sense, the heart of the <unk> of the climate system.
2024-05-27 00:51:21,744 - INFO - joeynmt.training - Example #3
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk>.
2024-05-27 00:51:21,744 - INFO - joeynmt.training - Example #4
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:51:21,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:51:21,744 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 00:51:36,356 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     1.434822, Batch Acc: 0.585002, Tokens per Sec:     4701, Lr: 0.000300
2024-05-27 00:51:51,215 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     1.402290, Batch Acc: 0.583094, Tokens per Sec:     4734, Lr: 0.000300
2024-05-27 00:52:06,366 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     1.215215, Batch Acc: 0.587783, Tokens per Sec:     4477, Lr: 0.000300
2024-05-27 00:52:21,158 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     1.339257, Batch Acc: 0.585813, Tokens per Sec:     4744, Lr: 0.000300
2024-05-27 00:52:36,219 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     1.152518, Batch Acc: 0.584489, Tokens per Sec:     4579, Lr: 0.000300
2024-05-27 00:52:36,220 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:52:36,220 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:53:02,167 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.59, acc:   0.54, generation: 25.9261[sec], evaluation: 0.0000[sec]
2024-05-27 00:53:02,331 - INFO - joeynmt.helpers - delete models/word_level_model/13500.ckpt
2024-05-27 00:53:02,332 - INFO - joeynmt.training - Example #0
2024-05-27 00:53:02,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:53:02,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:53:02,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 00:53:02,332 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States, <unk> <unk>, <unk> <unk> of 40 percent.
2024-05-27 00:53:02,333 - INFO - joeynmt.training - Example #1
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it's not <unk> the ice <unk>.
2024-05-27 00:53:02,333 - INFO - joeynmt.training - Example #2
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart of the global system.
2024-05-27 00:53:02,333 - INFO - joeynmt.training - Example #3
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:53:02,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:53:02,333 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in the summer.
2024-05-27 00:53:02,334 - INFO - joeynmt.training - Example #4
2024-05-27 00:53:02,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:53:02,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:53:02,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:53:02,334 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:53:02,334 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:53:02,334 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:53:17,040 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     1.230260, Batch Acc: 0.581480, Tokens per Sec:     4578, Lr: 0.000300
2024-05-27 00:53:31,275 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     1.350987, Batch Acc: 0.580074, Tokens per Sec:     4936, Lr: 0.000300
2024-05-27 00:53:46,094 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     1.312598, Batch Acc: 0.584233, Tokens per Sec:     4594, Lr: 0.000300
2024-05-27 00:54:00,516 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     1.359669, Batch Acc: 0.584178, Tokens per Sec:     4818, Lr: 0.000300
2024-05-27 00:54:14,865 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     1.316355, Batch Acc: 0.574560, Tokens per Sec:     4760, Lr: 0.000300
2024-05-27 00:54:14,865 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:54:14,865 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:54:43,187 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.59, acc:   0.54, generation: 28.3003[sec], evaluation: 0.0000[sec]
2024-05-27 00:54:43,350 - INFO - joeynmt.helpers - delete models/word_level_model/14500.ckpt
2024-05-27 00:54:43,351 - INFO - joeynmt.training - Example #0
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk>, <unk> <unk> of 40 percent.
2024-05-27 00:54:43,351 - INFO - joeynmt.training - Example #1
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:54:43,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:54:43,351 - INFO - joeynmt.training - 	Hypothesis: We <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:54:43,352 - INFO - joeynmt.training - Example #2
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of climate system.
2024-05-27 00:54:43,352 - INFO - joeynmt.training - Example #3
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk> in summer.
2024-05-27 00:54:43,352 - INFO - joeynmt.training - Example #4
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:54:43,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:54:43,352 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:54:57,875 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     1.403183, Batch Acc: 0.585771, Tokens per Sec:     4727, Lr: 0.000300
2024-05-27 00:55:12,824 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     1.364072, Batch Acc: 0.578522, Tokens per Sec:     4775, Lr: 0.000300
2024-05-27 00:55:28,131 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     1.398372, Batch Acc: 0.577049, Tokens per Sec:     4313, Lr: 0.000300
2024-05-27 00:55:43,337 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     1.293750, Batch Acc: 0.580915, Tokens per Sec:     4484, Lr: 0.000300
2024-05-27 00:55:57,163 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     1.247275, Batch Acc: 0.578159, Tokens per Sec:     4900, Lr: 0.000300
2024-05-27 00:55:57,164 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:55:57,164 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:56:26,815 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.58, acc:   0.53, generation: 29.6302[sec], evaluation: 0.0000[sec]
2024-05-27 00:56:26,980 - INFO - joeynmt.helpers - delete models/word_level_model/15000.ckpt
2024-05-27 00:56:26,981 - INFO - joeynmt.training - Example #0
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'is', 'for', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', 'is', '<unk>', '40', 'percent', '.', '</s>']
2024-05-27 00:56:26,981 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:56:26,981 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:56:26,981 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk> <unk>, which is for about three million years had the size of the United States, is <unk> 40 percent.
2024-05-27 00:56:26,981 - INFO - joeynmt.training - Example #1
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:56:26,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'shows', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:56:26,981 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't shows the ice <unk>.
2024-05-27 00:56:26,982 - INFO - joeynmt.training - Example #2
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'global', 'system', '.', '</s>']
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> <unk> is, in a way, the heart <unk> of the global global system.
2024-05-27 00:56:26,982 - INFO - joeynmt.training - Example #3
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 00:56:26,982 - INFO - joeynmt.training - Example #4
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:56:26,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:56:26,982 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 00:56:41,574 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     1.305751, Batch Acc: 0.580001, Tokens per Sec:     4646, Lr: 0.000300
2024-05-27 00:56:56,336 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     1.283030, Batch Acc: 0.580065, Tokens per Sec:     4606, Lr: 0.000300
2024-05-27 00:57:10,728 - INFO - joeynmt.training - Epoch   6, Step:    17800, Batch Loss:     1.521643, Batch Acc: 0.583128, Tokens per Sec:     4804, Lr: 0.000300
2024-05-27 00:57:25,822 - INFO - joeynmt.training - Epoch   6, Step:    17900, Batch Loss:     1.311966, Batch Acc: 0.582952, Tokens per Sec:     4570, Lr: 0.000300
2024-05-27 00:57:40,393 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     1.292039, Batch Acc: 0.583578, Tokens per Sec:     4816, Lr: 0.000300
2024-05-27 00:57:40,393 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:57:40,393 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:58:08,580 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.51, acc:   0.54, generation: 28.1661[sec], evaluation: 0.0000[sec]
2024-05-27 00:58:08,582 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 00:58:08,746 - INFO - joeynmt.helpers - delete models/word_level_model/16000.ckpt
2024-05-27 00:58:08,748 - INFO - joeynmt.training - Example #0
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '</s>']
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>,
2024-05-27 00:58:08,748 - INFO - joeynmt.training - Example #1
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:58:08,748 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 00:58:08,748 - INFO - joeynmt.training - Example #2
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:58:08,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart of the <unk> of the global system.
2024-05-27 00:58:08,749 - INFO - joeynmt.training - Example #3
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk>.
2024-05-27 00:58:08,749 - INFO - joeynmt.training - Example #4
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:58:08,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:58:08,749 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 00:58:23,047 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     1.371908, Batch Acc: 0.581304, Tokens per Sec:     4671, Lr: 0.000300
2024-05-27 00:58:37,237 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     1.354769, Batch Acc: 0.576273, Tokens per Sec:     4984, Lr: 0.000300
2024-05-27 00:58:51,144 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     1.384450, Batch Acc: 0.587604, Tokens per Sec:     5080, Lr: 0.000300
2024-05-27 00:59:06,051 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     1.404221, Batch Acc: 0.580270, Tokens per Sec:     4533, Lr: 0.000300
2024-05-27 00:59:20,622 - INFO - joeynmt.training - Epoch   6, Step:    18500, Batch Loss:     1.423498, Batch Acc: 0.574845, Tokens per Sec:     4624, Lr: 0.000300
2024-05-27 00:59:20,622 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 00:59:20,623 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 00:59:47,087 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.53, generation: 26.4424[sec], evaluation: 0.0000[sec]
2024-05-27 00:59:47,257 - INFO - joeynmt.helpers - delete models/word_level_model/16500.ckpt
2024-05-27 00:59:47,258 - INFO - joeynmt.training - Example #0
2024-05-27 00:59:47,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 00:59:47,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', '.', '</s>']
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that <unk> <unk> <unk> <unk>, which for almost three million years had the size of the <unk> <unk> <unk>, which for almost three million years.
2024-05-27 00:59:47,259 - INFO - joeynmt.training - Example #1
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '.', '</s>']
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this problem because it's not <unk> the ice.
2024-05-27 00:59:47,259 - INFO - joeynmt.training - Example #2
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', '<unk>', 'of', 'climate', 'system', '.', '</s>']
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 00:59:47,259 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a way, the heart <unk> of the <unk> of climate system.
2024-05-27 00:59:47,259 - INFO - joeynmt.training - Example #3
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 00:59:47,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 00:59:47,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk>.
2024-05-27 00:59:47,260 - INFO - joeynmt.training - Example #4
2024-05-27 00:59:47,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 00:59:47,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 00:59:47,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 00:59:47,260 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 00:59:48,890 - INFO - joeynmt.training - Epoch   6: total training loss 4135.74
2024-05-27 00:59:48,890 - INFO - joeynmt.training - EPOCH 7
2024-05-27 01:00:03,100 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     1.303044, Batch Acc: 0.602128, Tokens per Sec:     4371, Lr: 0.000300
2024-05-27 01:00:18,235 - INFO - joeynmt.training - Epoch   7, Step:    18700, Batch Loss:     1.269306, Batch Acc: 0.602253, Tokens per Sec:     4605, Lr: 0.000300
2024-05-27 01:00:33,002 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     1.272627, Batch Acc: 0.601297, Tokens per Sec:     4649, Lr: 0.000300
2024-05-27 01:00:47,745 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     1.159412, Batch Acc: 0.596429, Tokens per Sec:     4570, Lr: 0.000300
2024-05-27 01:01:02,576 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.458517, Batch Acc: 0.600825, Tokens per Sec:     4546, Lr: 0.000300
2024-05-27 01:01:02,578 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:01:02,578 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:01:30,770 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 28.1710[sec], evaluation: 0.0000[sec]
2024-05-27 01:01:30,939 - INFO - joeynmt.helpers - delete models/word_level_model/17000.ckpt
2024-05-27 01:01:30,941 - INFO - joeynmt.training - Example #0
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'showed', 'these', '<unk>', 'for', 'almost', 'three', 'million', 'years', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these <unk> for almost three million years, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk>.
2024-05-27 01:01:30,941 - INFO - joeynmt.training - Example #1
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', 'in', 'the', '<unk>', '.', '</s>']
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:01:30,941 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> the problem because it doesn't show the ice in the <unk>.
2024-05-27 01:01:30,941 - INFO - joeynmt.training - Example #2
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:01:30,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the global system.
2024-05-27 01:01:30,942 - INFO - joeynmt.training - Example #3
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in the summer.
2024-05-27 01:01:30,942 - INFO - joeynmt.training - Example #4
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:01:30,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:01:30,942 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 01:01:45,443 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     1.385377, Batch Acc: 0.596482, Tokens per Sec:     4736, Lr: 0.000300
2024-05-27 01:02:00,812 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     1.399787, Batch Acc: 0.594570, Tokens per Sec:     4588, Lr: 0.000300
2024-05-27 01:02:15,505 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     1.220719, Batch Acc: 0.593869, Tokens per Sec:     4722, Lr: 0.000300
2024-05-27 01:02:30,112 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.183062, Batch Acc: 0.592308, Tokens per Sec:     4680, Lr: 0.000300
2024-05-27 01:02:45,471 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     1.410887, Batch Acc: 0.597475, Tokens per Sec:     4363, Lr: 0.000300
2024-05-27 01:02:45,471 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:02:45,471 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:03:12,872 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.55, acc:   0.54, generation: 27.3793[sec], evaluation: 0.0000[sec]
2024-05-27 01:03:13,038 - INFO - joeynmt.helpers - delete models/word_level_model/17500.ckpt
2024-05-27 01:03:13,040 - INFO - joeynmt.training - Example #0
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> of 40 percent.
2024-05-27 01:03:13,040 - INFO - joeynmt.training - Example #1
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:03:13,040 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this problem because it doesn't show <unk> of the ice.
2024-05-27 01:03:13,040 - INFO - joeynmt.training - Example #2
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:03:13,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'global', 'warming', 'system', '.', '</s>']
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart of the <unk> of the global warming system.
2024-05-27 01:03:13,041 - INFO - joeynmt.training - Example #3
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> <unk>.
2024-05-27 01:03:13,041 - INFO - joeynmt.training - Example #4
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:03:13,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:03:13,041 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 01:03:28,151 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     1.150321, Batch Acc: 0.600558, Tokens per Sec:     4482, Lr: 0.000300
2024-05-27 01:03:43,003 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     1.294147, Batch Acc: 0.595994, Tokens per Sec:     4649, Lr: 0.000300
2024-05-27 01:03:57,372 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     1.422540, Batch Acc: 0.589184, Tokens per Sec:     4867, Lr: 0.000300
2024-05-27 01:04:11,497 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     1.332148, Batch Acc: 0.589756, Tokens per Sec:     4855, Lr: 0.000300
2024-05-27 01:04:26,062 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     1.283277, Batch Acc: 0.587877, Tokens per Sec:     4700, Lr: 0.000300
2024-05-27 01:04:26,062 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:04:26,062 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:04:52,440 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 26.3570[sec], evaluation: 0.0000[sec]
2024-05-27 01:04:52,604 - INFO - joeynmt.helpers - delete models/word_level_model/15500.ckpt
2024-05-27 01:04:52,606 - INFO - joeynmt.training - Example #0
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'for', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> for <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> 40 percent <unk>.
2024-05-27 01:04:52,606 - INFO - joeynmt.training - Example #1
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'you', 'the', 'ice', 'in', 'the', '<unk>', '.', '</s>']
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:04:52,606 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show you the ice in the <unk>.
2024-05-27 01:04:52,606 - INFO - joeynmt.training - Example #2
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:04:52,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'change', 'in', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the climate change in the global system.
2024-05-27 01:04:52,607 - INFO - joeynmt.training - Example #3
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> in the summer.
2024-05-27 01:04:52,607 - INFO - joeynmt.training - Example #4
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:04:52,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'for', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:04:52,607 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> for the last 25 years.
2024-05-27 01:05:07,937 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     1.371957, Batch Acc: 0.591967, Tokens per Sec:     4332, Lr: 0.000300
2024-05-27 01:05:22,863 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     1.301650, Batch Acc: 0.590054, Tokens per Sec:     4564, Lr: 0.000300
2024-05-27 01:05:37,586 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     1.334817, Batch Acc: 0.590768, Tokens per Sec:     4785, Lr: 0.000300
2024-05-27 01:05:52,174 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     1.211951, Batch Acc: 0.590505, Tokens per Sec:     4744, Lr: 0.000300
2024-05-27 01:06:06,112 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     1.416056, Batch Acc: 0.588510, Tokens per Sec:     4987, Lr: 0.000300
2024-05-27 01:06:06,112 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:06:06,112 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:06:31,522 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.49, acc:   0.54, generation: 25.3893[sec], evaluation: 0.0000[sec]
2024-05-27 01:06:31,523 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 01:06:31,690 - INFO - joeynmt.helpers - delete models/word_level_model/19500.ckpt
2024-05-27 01:06:31,692 - INFO - joeynmt.training - Example #0
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', ',', 'has', 'been', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk>, has been <unk> <unk> of 40 percent.
2024-05-27 01:06:31,692 - INFO - joeynmt.training - Example #1
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', '&apos;re', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', 'of', 'the', '<unk>', '.', '</s>']
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Hypothesis: We're going to <unk> this <unk> of the problem because it doesn't show the ice of the <unk>.
2024-05-27 01:06:31,692 - INFO - joeynmt.training - Example #2
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:06:31,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'global', 'warming', '.', '</s>']
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:06:31,692 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a way, the heart of the <unk> of the global warming.
2024-05-27 01:06:31,693 - INFO - joeynmt.training - Example #3
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 01:06:31,693 - INFO - joeynmt.training - Example #4
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:06:31,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:06:31,693 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 01:06:45,951 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     1.378097, Batch Acc: 0.590228, Tokens per Sec:     4854, Lr: 0.000300
2024-05-27 01:07:00,491 - INFO - joeynmt.training - Epoch   7, Step:    20700, Batch Loss:     1.375316, Batch Acc: 0.583337, Tokens per Sec:     4732, Lr: 0.000300
2024-05-27 01:07:15,648 - INFO - joeynmt.training - Epoch   7, Step:    20800, Batch Loss:     1.373732, Batch Acc: 0.587046, Tokens per Sec:     4666, Lr: 0.000300
2024-05-27 01:07:30,423 - INFO - joeynmt.training - Epoch   7, Step:    20900, Batch Loss:     1.346441, Batch Acc: 0.590538, Tokens per Sec:     4613, Lr: 0.000300
2024-05-27 01:07:44,670 - INFO - joeynmt.training - Epoch   7, Step:    21000, Batch Loss:     1.278249, Batch Acc: 0.588902, Tokens per Sec:     4842, Lr: 0.000300
2024-05-27 01:07:44,671 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:07:44,671 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:08:09,700 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.52, acc:   0.54, generation: 25.0079[sec], evaluation: 0.0000[sec]
2024-05-27 01:08:09,864 - INFO - joeynmt.helpers - delete models/word_level_model/19000.ckpt
2024-05-27 01:08:09,865 - INFO - joeynmt.training - Example #0
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:08:09,865 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:08:09,865 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:08:09,865 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk> <unk>, <unk> <unk> <unk>.
2024-05-27 01:08:09,865 - INFO - joeynmt.training - Example #1
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:08:09,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', 'ice', '.', '</s>']
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it's not <unk> the ice.
2024-05-27 01:08:09,866 - INFO - joeynmt.training - Example #2
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', 'global', 'warming', '.', '</s>']
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a way, the heart of the global warming.
2024-05-27 01:08:09,866 - INFO - joeynmt.training - Example #3
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:08:09,866 - INFO - joeynmt.training - 	Hypothesis: They <unk> and <unk> <unk>.
2024-05-27 01:08:09,866 - INFO - joeynmt.training - Example #4
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:08:09,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:08:09,867 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:08:09,867 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:08:09,867 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 01:08:24,897 - INFO - joeynmt.training - Epoch   7, Step:    21100, Batch Loss:     1.381644, Batch Acc: 0.583621, Tokens per Sec:     4523, Lr: 0.000300
2024-05-27 01:08:39,042 - INFO - joeynmt.training - Epoch   7, Step:    21200, Batch Loss:     1.579642, Batch Acc: 0.588387, Tokens per Sec:     4931, Lr: 0.000300
2024-05-27 01:08:53,396 - INFO - joeynmt.training - Epoch   7, Step:    21300, Batch Loss:     1.302757, Batch Acc: 0.587999, Tokens per Sec:     4779, Lr: 0.000300
2024-05-27 01:09:07,794 - INFO - joeynmt.training - Epoch   7, Step:    21400, Batch Loss:     1.469050, Batch Acc: 0.585303, Tokens per Sec:     4693, Lr: 0.000300
2024-05-27 01:09:22,529 - INFO - joeynmt.training - Epoch   7, Step:    21500, Batch Loss:     1.327323, Batch Acc: 0.587746, Tokens per Sec:     4712, Lr: 0.000300
2024-05-27 01:09:22,530 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:09:22,530 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:09:49,078 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.47, acc:   0.54, generation: 26.5277[sec], evaluation: 0.0000[sec]
2024-05-27 01:09:49,079 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 01:09:49,243 - INFO - joeynmt.helpers - delete models/word_level_model/18500.ckpt
2024-05-27 01:09:49,245 - INFO - joeynmt.training - Example #0
2024-05-27 01:09:49,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:09:49,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:09:49,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'it', 'was', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> <unk> <unk>, which for about three million years had the size of the United States <unk>, it was <unk> of 40 percent.
2024-05-27 01:09:49,246 - INFO - joeynmt.training - Example #1
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 01:09:49,246 - INFO - joeynmt.training - Example #2
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart <unk> of the global system.
2024-05-27 01:09:49,246 - INFO - joeynmt.training - Example #3
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:09:49,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:09:49,246 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:09:49,247 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 01:09:49,247 - INFO - joeynmt.training - Example #4
2024-05-27 01:09:49,247 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:09:49,247 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:09:49,247 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:09:49,247 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:09:49,247 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:09:49,247 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 01:10:02,982 - INFO - joeynmt.training - Epoch   7: total training loss 4031.34
2024-05-27 01:10:02,982 - INFO - joeynmt.training - EPOCH 8
2024-05-27 01:10:03,613 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     1.279224, Batch Acc: 0.607790, Tokens per Sec:     4641, Lr: 0.000300
2024-05-27 01:10:17,565 - INFO - joeynmt.training - Epoch   8, Step:    21700, Batch Loss:     1.170701, Batch Acc: 0.604576, Tokens per Sec:     4938, Lr: 0.000300
2024-05-27 01:10:32,945 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     1.275689, Batch Acc: 0.605449, Tokens per Sec:     4389, Lr: 0.000300
2024-05-27 01:10:47,415 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     1.303396, Batch Acc: 0.606370, Tokens per Sec:     4952, Lr: 0.000300
2024-05-27 01:11:02,200 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     1.244750, Batch Acc: 0.608540, Tokens per Sec:     4655, Lr: 0.000300
2024-05-27 01:11:02,200 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:11:02,200 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:11:30,453 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 28.2313[sec], evaluation: 0.0000[sec]
2024-05-27 01:11:30,618 - INFO - joeynmt.helpers - delete models/word_level_model/20000.ckpt
2024-05-27 01:11:30,620 - INFO - joeynmt.training - Example #0
2024-05-27 01:11:30,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:11:30,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:11:30,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', 'is', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, is <unk> <unk> of 40 percent.
2024-05-27 01:11:30,621 - INFO - joeynmt.training - Example #1
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this problem because it doesn't show the ice <unk>.
2024-05-27 01:11:30,621 - INFO - joeynmt.training - Example #2
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:11:30,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:11:30,621 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the <unk> of the global system.
2024-05-27 01:11:30,621 - INFO - joeynmt.training - Example #3
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 01:11:30,622 - INFO - joeynmt.training - Example #4
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:11:30,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:11:30,622 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 01:11:45,207 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     1.243311, Batch Acc: 0.606451, Tokens per Sec:     4684, Lr: 0.000300
2024-05-27 01:12:00,035 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.306294, Batch Acc: 0.603455, Tokens per Sec:     4752, Lr: 0.000300
2024-05-27 01:12:14,158 - INFO - joeynmt.training - Epoch   8, Step:    22300, Batch Loss:     1.290809, Batch Acc: 0.605329, Tokens per Sec:     4935, Lr: 0.000300
2024-05-27 01:12:28,047 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     1.311218, Batch Acc: 0.603260, Tokens per Sec:     5014, Lr: 0.000300
2024-05-27 01:12:41,706 - INFO - joeynmt.training - Epoch   8, Step:    22500, Batch Loss:     1.216255, Batch Acc: 0.598110, Tokens per Sec:     5176, Lr: 0.000300
2024-05-27 01:12:41,706 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:12:41,706 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:13:04,923 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.52, acc:   0.54, generation: 23.1949[sec], evaluation: 0.0000[sec]
2024-05-27 01:13:05,086 - INFO - joeynmt.helpers - delete models/word_level_model/22000.ckpt
2024-05-27 01:13:05,087 - INFO - joeynmt.helpers - delete /Users/siz/Documents/mt-exercise-5/models/word_level_model/22000.ckpt
2024-05-27 01:13:05,087 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/siz/Documents/mt-exercise-5/models/word_level_model/22000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/siz/Documents/mt-exercise-5/models/word_level_model/22000.ckpt')
2024-05-27 01:13:05,087 - INFO - joeynmt.training - Example #0
2024-05-27 01:13:05,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:13:05,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the <unk> <unk> <unk>, <unk> <unk>, <unk> <unk> <unk>.
2024-05-27 01:13:05,088 - INFO - joeynmt.training - Example #1
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 01:13:05,088 - INFO - joeynmt.training - Example #2
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:13:05,088 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a certain sense, the heart of the climate system.
2024-05-27 01:13:05,088 - INFO - joeynmt.training - Example #3
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:13:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', 'they', '<unk>', '.', 'They', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and they <unk>. They <unk> in the summer.
2024-05-27 01:13:05,089 - INFO - joeynmt.training - Example #4
2024-05-27 01:13:05,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:13:05,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:13:05,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'for', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:13:05,089 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> for the last 25 years.
2024-05-27 01:13:18,946 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     1.462285, Batch Acc: 0.600326, Tokens per Sec:     4992, Lr: 0.000300
2024-05-27 01:13:32,233 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     1.220137, Batch Acc: 0.602513, Tokens per Sec:     5115, Lr: 0.000300
2024-05-27 01:13:46,134 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     1.261774, Batch Acc: 0.604054, Tokens per Sec:     5000, Lr: 0.000300
2024-05-27 01:14:00,422 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     1.243302, Batch Acc: 0.598276, Tokens per Sec:     4783, Lr: 0.000300
2024-05-27 01:14:15,209 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     1.227518, Batch Acc: 0.595669, Tokens per Sec:     4756, Lr: 0.000300
2024-05-27 01:14:15,210 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:14:15,210 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:14:42,321 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.55, acc:   0.54, generation: 27.0908[sec], evaluation: 0.0000[sec]
2024-05-27 01:14:42,324 - INFO - joeynmt.training - Example #0
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', 'in', 'the', 'United', 'States', ',', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Hypothesis: I showed these <unk> <unk> that the <unk> <unk> <unk>, which for about three million years had the size of the United States <unk>, <unk> <unk> <unk> in the United States, <unk> <unk> <unk>.
2024-05-27 01:14:42,324 - INFO - joeynmt.training - Example #1
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', '<unk>', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:14:42,324 - INFO - joeynmt.training - 	Hypothesis: And this <unk> <unk> the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:14:42,324 - INFO - joeynmt.training - Example #2
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:14:42,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a sense, the heart of the <unk> of the global system.
2024-05-27 01:14:42,325 - INFO - joeynmt.training - Example #3
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:14:42,325 - INFO - joeynmt.training - Example #4
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:14:42,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'for', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:14:42,325 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> for the last 25 years.
2024-05-27 01:14:57,666 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     1.309936, Batch Acc: 0.602072, Tokens per Sec:     4554, Lr: 0.000300
2024-05-27 01:15:12,409 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     1.482460, Batch Acc: 0.603116, Tokens per Sec:     4623, Lr: 0.000300
2024-05-27 01:15:26,542 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.380135, Batch Acc: 0.600168, Tokens per Sec:     4894, Lr: 0.000300
2024-05-27 01:15:40,948 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     1.323066, Batch Acc: 0.599326, Tokens per Sec:     4781, Lr: 0.000300
2024-05-27 01:15:55,733 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     1.223873, Batch Acc: 0.595976, Tokens per Sec:     4640, Lr: 0.000300
2024-05-27 01:15:55,734 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:15:55,734 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:16:21,690 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 25.9359[sec], evaluation: 0.0000[sec]
2024-05-27 01:16:21,691 - INFO - joeynmt.training - Example #0
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:16:21,691 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:16:21,691 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:16:21,691 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk> <unk>.
2024-05-27 01:16:21,691 - INFO - joeynmt.training - Example #1
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:16:21,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'ice', '.', '</s>']
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the <unk> of ice.
2024-05-27 01:16:21,692 - INFO - joeynmt.training - Example #2
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', '<unk>', '<unk>', '<unk>', '&apos;s', 'heart', 'system', '.', '</s>']
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a way, the <unk> <unk> <unk>'s heart system.
2024-05-27 01:16:21,692 - INFO - joeynmt.training - Example #3
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '.', '</s>']
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk>.
2024-05-27 01:16:21,692 - INFO - joeynmt.training - Example #4
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:16:21,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:16:21,692 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 01:16:36,631 - INFO - joeynmt.training - Epoch   8, Step:    23600, Batch Loss:     1.211315, Batch Acc: 0.597189, Tokens per Sec:     4529, Lr: 0.000300
2024-05-27 01:16:51,551 - INFO - joeynmt.training - Epoch   8, Step:    23700, Batch Loss:     1.241976, Batch Acc: 0.599399, Tokens per Sec:     4482, Lr: 0.000300
2024-05-27 01:17:06,423 - INFO - joeynmt.training - Epoch   8, Step:    23800, Batch Loss:     1.246914, Batch Acc: 0.598963, Tokens per Sec:     4693, Lr: 0.000300
2024-05-27 01:17:20,914 - INFO - joeynmt.training - Epoch   8, Step:    23900, Batch Loss:     1.266469, Batch Acc: 0.600169, Tokens per Sec:     4750, Lr: 0.000300
2024-05-27 01:17:35,544 - INFO - joeynmt.training - Epoch   8, Step:    24000, Batch Loss:     1.282007, Batch Acc: 0.595167, Tokens per Sec:     4622, Lr: 0.000300
2024-05-27 01:17:35,544 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:17:35,544 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:18:01,444 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.54, acc:   0.54, generation: 25.8795[sec], evaluation: 0.0000[sec]
2024-05-27 01:18:01,445 - INFO - joeynmt.training - Example #0
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that <unk> <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk> <unk>, <unk> of 40 percent.
2024-05-27 01:18:01,446 - INFO - joeynmt.training - Example #1
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the <unk> of the ice <unk>.
2024-05-27 01:18:01,446 - INFO - joeynmt.training - Example #2
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', 'of', 'the', '<unk>', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:18:01,446 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart of the <unk> of the climate system.
2024-05-27 01:18:01,446 - INFO - joeynmt.training - Example #3
2024-05-27 01:18:01,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:18:01,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:18:01,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:18:01,447 - INFO - joeynmt.training - Example #4
2024-05-27 01:18:01,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:18:01,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:18:01,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:18:01,447 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 01:18:15,668 - INFO - joeynmt.training - Epoch   8, Step:    24100, Batch Loss:     1.186121, Batch Acc: 0.596060, Tokens per Sec:     4850, Lr: 0.000300
2024-05-27 01:18:30,293 - INFO - joeynmt.training - Epoch   8, Step:    24200, Batch Loss:     1.249482, Batch Acc: 0.598889, Tokens per Sec:     4739, Lr: 0.000300
2024-05-27 01:18:44,463 - INFO - joeynmt.training - Epoch   8, Step:    24300, Batch Loss:     1.329037, Batch Acc: 0.589980, Tokens per Sec:     4999, Lr: 0.000300
2024-05-27 01:18:59,986 - INFO - joeynmt.training - Epoch   8, Step:    24400, Batch Loss:     1.329348, Batch Acc: 0.595527, Tokens per Sec:     4358, Lr: 0.000300
2024-05-27 01:19:14,006 - INFO - joeynmt.training - Epoch   8, Step:    24500, Batch Loss:     1.554574, Batch Acc: 0.597210, Tokens per Sec:     4831, Lr: 0.000300
2024-05-27 01:19:14,007 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:19:14,007 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:19:39,695 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.52, acc:   0.54, generation: 25.6676[sec], evaluation: 0.0000[sec]
2024-05-27 01:19:39,695 - INFO - joeynmt.training - Example #0
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', '<unk>', '<unk>', '<unk>', 'in', 'the', 'United', 'States', ',', '<unk>', ',', '<unk>', 'of', 'the', 'United', 'States', '.', '</s>']
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the <unk> <unk> <unk> in the United States, <unk>, <unk> of the United States.
2024-05-27 01:19:39,696 - INFO - joeynmt.training - Example #1
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:19:39,696 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:19:39,696 - INFO - joeynmt.training - Example #2
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:19:39,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a sense, the heart <unk> of the global climate system.
2024-05-27 01:19:39,697 - INFO - joeynmt.training - Example #3
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> in summer.
2024-05-27 01:19:39,697 - INFO - joeynmt.training - Example #4
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:19:39,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:19:39,697 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 01:19:55,017 - INFO - joeynmt.training - Epoch   8, Step:    24600, Batch Loss:     1.291107, Batch Acc: 0.588728, Tokens per Sec:     4529, Lr: 0.000300
2024-05-27 01:20:06,235 - INFO - joeynmt.training - Epoch   8: total training loss 3933.96
2024-05-27 01:20:06,235 - INFO - joeynmt.training - EPOCH 9
2024-05-27 01:20:09,499 - INFO - joeynmt.training - Epoch   9, Step:    24700, Batch Loss:     1.294997, Batch Acc: 0.622827, Tokens per Sec:     5077, Lr: 0.000300
2024-05-27 01:20:24,495 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     1.336717, Batch Acc: 0.621923, Tokens per Sec:     4541, Lr: 0.000300
2024-05-27 01:20:38,847 - INFO - joeynmt.training - Epoch   9, Step:    24900, Batch Loss:     1.224676, Batch Acc: 0.615593, Tokens per Sec:     4738, Lr: 0.000300
2024-05-27 01:20:53,304 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     1.320943, Batch Acc: 0.614739, Tokens per Sec:     4711, Lr: 0.000300
2024-05-27 01:20:53,305 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:20:53,305 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:21:19,907 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.50, acc:   0.54, generation: 26.5811[sec], evaluation: 0.0000[sec]
2024-05-27 01:21:20,076 - INFO - joeynmt.helpers - delete models/word_level_model/22500.ckpt
2024-05-27 01:21:20,076 - INFO - joeynmt.helpers - delete /Users/siz/Documents/mt-exercise-5/models/word_level_model/22500.ckpt
2024-05-27 01:21:20,076 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/siz/Documents/mt-exercise-5/models/word_level_model/22500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/siz/Documents/mt-exercise-5/models/word_level_model/22500.ckpt')
2024-05-27 01:21:20,077 - INFO - joeynmt.training - Example #0
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:21:20,077 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:21:20,077 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:21:20,077 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk> <unk> <unk>, <unk> of 40 percent.
2024-05-27 01:21:20,077 - INFO - joeynmt.training - Example #1
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:21:20,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it's not <unk> the <unk> of the ice.
2024-05-27 01:21:20,078 - INFO - joeynmt.training - Example #2
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'some', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'global', 'warming', 'system', '.', '</s>']
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in some sense, the heart <unk> of the global global warming system.
2024-05-27 01:21:20,078 - INFO - joeynmt.training - Example #3
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:21:20,078 - INFO - joeynmt.training - 	Hypothesis: It's <unk> <unk> and <unk> in summer.
2024-05-27 01:21:20,078 - INFO - joeynmt.training - Example #4
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:21:20,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'in', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:21:20,079 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:21:20,079 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:21:20,079 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> in the last 25 years.
2024-05-27 01:21:34,866 - INFO - joeynmt.training - Epoch   9, Step:    25100, Batch Loss:     1.167592, Batch Acc: 0.617258, Tokens per Sec:     4663, Lr: 0.000300
2024-05-27 01:21:49,176 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     1.212259, Batch Acc: 0.612191, Tokens per Sec:     4871, Lr: 0.000300
2024-05-27 01:22:03,616 - INFO - joeynmt.training - Epoch   9, Step:    25300, Batch Loss:     1.173343, Batch Acc: 0.612009, Tokens per Sec:     4761, Lr: 0.000300
2024-05-27 01:22:17,760 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     1.230258, Batch Acc: 0.607938, Tokens per Sec:     4723, Lr: 0.000300
2024-05-27 01:22:31,523 - INFO - joeynmt.training - Epoch   9, Step:    25500, Batch Loss:     1.241993, Batch Acc: 0.608709, Tokens per Sec:     4946, Lr: 0.000300
2024-05-27 01:22:31,523 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:22:31,523 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:22:56,727 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.53, acc:   0.54, generation: 25.1830[sec], evaluation: 0.0000[sec]
2024-05-27 01:22:56,727 - INFO - joeynmt.training - Example #0
2024-05-27 01:22:56,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:22:56,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:22:56,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', '<unk>', ',', 'is', '<unk>', 'by', '40', 'percent', '.', '</s>']
2024-05-27 01:22:56,727 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States, <unk> <unk> <unk>, is <unk> by 40 percent.
2024-05-27 01:22:56,728 - INFO - joeynmt.training - Example #1
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', '&apos;s', 'not', '<unk>', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Hypothesis: And I'm going to <unk> this problem because it's not <unk> the <unk> of the ice.
2024-05-27 01:22:56,728 - INFO - joeynmt.training - Example #2
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'warming', '.', '</s>']
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart <unk> of the global warming.
2024-05-27 01:22:56,728 - INFO - joeynmt.training - Example #3
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:22:56,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:22:56,728 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:22:56,729 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:22:56,729 - INFO - joeynmt.training - Example #4
2024-05-27 01:22:56,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:22:56,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:22:56,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:22:56,729 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:22:56,729 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:22:56,729 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 01:23:11,590 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     1.260897, Batch Acc: 0.610573, Tokens per Sec:     4677, Lr: 0.000300
2024-05-27 01:23:26,154 - INFO - joeynmt.training - Epoch   9, Step:    25700, Batch Loss:     1.208172, Batch Acc: 0.611496, Tokens per Sec:     4790, Lr: 0.000300
2024-05-27 01:23:40,288 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.194713, Batch Acc: 0.607840, Tokens per Sec:     4882, Lr: 0.000300
2024-05-27 01:23:54,050 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.319725, Batch Acc: 0.602547, Tokens per Sec:     4884, Lr: 0.000300
2024-05-27 01:24:08,811 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     1.279005, Batch Acc: 0.610343, Tokens per Sec:     4649, Lr: 0.000300
2024-05-27 01:24:08,811 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:24:08,811 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:24:36,167 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.50, acc:   0.54, generation: 27.3351[sec], evaluation: 0.0000[sec]
2024-05-27 01:24:36,333 - INFO - joeynmt.helpers - delete models/word_level_model/21000.ckpt
2024-05-27 01:24:36,333 - INFO - joeynmt.training - Example #0
2024-05-27 01:24:36,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:24:36,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:24:36,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:24:36,333 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> <unk>.
2024-05-27 01:24:36,334 - INFO - joeynmt.training - Example #1
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:24:36,334 - INFO - joeynmt.training - Example #2
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', '<unk>', '<unk>', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a way, the <unk> <unk> of the global climate system.
2024-05-27 01:24:36,334 - INFO - joeynmt.training - Example #3
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:24:36,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:24:36,334 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:24:36,335 - INFO - joeynmt.training - Example #4
2024-05-27 01:24:36,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:24:36,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:24:36,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'in', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:24:36,335 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:24:36,335 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:24:36,335 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> in the last 25 years.
2024-05-27 01:24:50,142 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.231127, Batch Acc: 0.610353, Tokens per Sec:     4794, Lr: 0.000210
2024-05-27 01:25:05,194 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     1.337373, Batch Acc: 0.614414, Tokens per Sec:     4682, Lr: 0.000210
2024-05-27 01:25:19,591 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     1.113993, Batch Acc: 0.614798, Tokens per Sec:     4891, Lr: 0.000210
2024-05-27 01:25:34,290 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     1.163825, Batch Acc: 0.614797, Tokens per Sec:     4741, Lr: 0.000210
2024-05-27 01:25:47,686 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.241780, Batch Acc: 0.611393, Tokens per Sec:     5161, Lr: 0.000210
2024-05-27 01:25:47,686 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:25:47,686 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:26:12,197 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.42, acc:   0.55, generation: 24.4891[sec], evaluation: 0.0000[sec]
2024-05-27 01:26:12,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 01:26:12,365 - INFO - joeynmt.helpers - delete models/word_level_model/18000.ckpt
2024-05-27 01:26:12,366 - INFO - joeynmt.training - Example #0
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', ',', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:26:12,366 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:26:12,366 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:26:12,366 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States, <unk> <unk>, <unk> <unk> <unk>, <unk> <unk> <unk> <unk>.
2024-05-27 01:26:12,366 - INFO - joeynmt.training - Example #1
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:26:12,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:26:12,366 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:26:12,367 - INFO - joeynmt.training - Example #2
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'climate', 'change', 'in', 'the', 'global', 'system', '.', '</s>']
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Hypothesis: The <unk> <unk> is, in a way, the heart <unk> of the climate change in the global system.
2024-05-27 01:26:12,367 - INFO - joeynmt.training - Example #3
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:26:12,367 - INFO - joeynmt.training - Example #4
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:26:12,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:26:12,367 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:26:12,368 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 01:26:26,523 - INFO - joeynmt.training - Epoch   9, Step:    26600, Batch Loss:     1.173195, Batch Acc: 0.606788, Tokens per Sec:     4702, Lr: 0.000210
2024-05-27 01:26:40,737 - INFO - joeynmt.training - Epoch   9, Step:    26700, Batch Loss:     1.193893, Batch Acc: 0.612944, Tokens per Sec:     4912, Lr: 0.000210
2024-05-27 01:26:55,373 - INFO - joeynmt.training - Epoch   9, Step:    26800, Batch Loss:     1.336774, Batch Acc: 0.610260, Tokens per Sec:     4779, Lr: 0.000210
2024-05-27 01:27:09,769 - INFO - joeynmt.training - Epoch   9, Step:    26900, Batch Loss:     1.275550, Batch Acc: 0.611924, Tokens per Sec:     4812, Lr: 0.000210
2024-05-27 01:27:23,953 - INFO - joeynmt.training - Epoch   9, Step:    27000, Batch Loss:     1.222162, Batch Acc: 0.614674, Tokens per Sec:     4860, Lr: 0.000210
2024-05-27 01:27:23,953 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:27:23,953 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:27:49,786 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.42, acc:   0.55, generation: 25.8126[sec], evaluation: 0.0000[sec]
2024-05-27 01:27:49,951 - INFO - joeynmt.helpers - delete models/word_level_model/25000.ckpt
2024-05-27 01:27:49,952 - INFO - joeynmt.training - Example #0
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', 'had', '<unk>', 'dimensions', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States had <unk> dimensions of the United States <unk>, <unk> <unk>.
2024-05-27 01:27:49,952 - INFO - joeynmt.training - Example #1
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:27:49,952 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 01:27:49,952 - INFO - joeynmt.training - Example #2
2024-05-27 01:27:49,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'system', '.', '</s>']
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the climate system.
2024-05-27 01:27:49,953 - INFO - joeynmt.training - Example #3
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> in summer.
2024-05-27 01:27:49,953 - INFO - joeynmt.training - Example #4
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:27:49,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:27:49,953 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 01:28:04,473 - INFO - joeynmt.training - Epoch   9, Step:    27100, Batch Loss:     1.152095, Batch Acc: 0.615765, Tokens per Sec:     4672, Lr: 0.000210
2024-05-27 01:28:19,771 - INFO - joeynmt.training - Epoch   9, Step:    27200, Batch Loss:     1.123053, Batch Acc: 0.610219, Tokens per Sec:     4525, Lr: 0.000210
2024-05-27 01:28:34,204 - INFO - joeynmt.training - Epoch   9, Step:    27300, Batch Loss:     1.150128, Batch Acc: 0.615490, Tokens per Sec:     4744, Lr: 0.000210
2024-05-27 01:28:49,041 - INFO - joeynmt.training - Epoch   9, Step:    27400, Batch Loss:     1.194731, Batch Acc: 0.613088, Tokens per Sec:     4610, Lr: 0.000210
2024-05-27 01:29:03,046 - INFO - joeynmt.training - Epoch   9, Step:    27500, Batch Loss:     1.098689, Batch Acc: 0.611281, Tokens per Sec:     4855, Lr: 0.000210
2024-05-27 01:29:03,046 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:29:03,046 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:29:28,934 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.40, acc:   0.55, generation: 25.8660[sec], evaluation: 0.0000[sec]
2024-05-27 01:29:28,934 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 01:29:29,100 - INFO - joeynmt.helpers - delete models/word_level_model/26000.ckpt
2024-05-27 01:29:29,101 - INFO - joeynmt.training - Example #0
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:29:29,101 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:29:29,101 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:29:29,101 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> 40 percent <unk>.
2024-05-27 01:29:29,101 - INFO - joeynmt.training - Example #1
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:29:29,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:29:29,102 - INFO - joeynmt.training - Example #2
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'climate', 'change', '.', '</s>']
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart <unk> of climate change.
2024-05-27 01:29:29,102 - INFO - joeynmt.training - Example #3
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>']
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:29:29,102 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> <unk>.
2024-05-27 01:29:29,102 - INFO - joeynmt.training - Example #4
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:29:29,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:29:29,103 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:29:29,103 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:29:29,103 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the last 25 years.
2024-05-27 01:29:44,230 - INFO - joeynmt.training - Epoch   9, Step:    27600, Batch Loss:     1.327716, Batch Acc: 0.611797, Tokens per Sec:     4589, Lr: 0.000210
2024-05-27 01:29:59,531 - INFO - joeynmt.training - Epoch   9, Step:    27700, Batch Loss:     1.299651, Batch Acc: 0.613562, Tokens per Sec:     4637, Lr: 0.000210
2024-05-27 01:30:08,870 - INFO - joeynmt.training - Epoch   9: total training loss 3812.03
2024-05-27 01:30:08,870 - INFO - joeynmt.training - EPOCH 10
2024-05-27 01:30:14,682 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     1.167418, Batch Acc: 0.635503, Tokens per Sec:     4219, Lr: 0.000210
2024-05-27 01:30:30,049 - INFO - joeynmt.training - Epoch  10, Step:    27900, Batch Loss:     1.192773, Batch Acc: 0.637791, Tokens per Sec:     4537, Lr: 0.000210
2024-05-27 01:30:45,393 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     1.192831, Batch Acc: 0.638272, Tokens per Sec:     4405, Lr: 0.000210
2024-05-27 01:30:45,393 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:30:45,393 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:31:09,815 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.46, acc:   0.55, generation: 24.4002[sec], evaluation: 0.0000[sec]
2024-05-27 01:31:09,980 - INFO - joeynmt.helpers - delete models/word_level_model/20500.ckpt
2024-05-27 01:31:09,981 - INFO - joeynmt.training - Example #0
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', 'to', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> to <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> 40 percent <unk>.
2024-05-27 01:31:09,981 - INFO - joeynmt.training - Example #1
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', '&apos;s', 'not', 'shows', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:31:09,981 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this problem because it's not shows the <unk> of the ice.
2024-05-27 01:31:09,981 - INFO - joeynmt.training - Example #2
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:31:09,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'global', 'warming', '.', '</s>']
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart <unk> of the global global warming.
2024-05-27 01:31:09,982 - INFO - joeynmt.training - Example #3
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '&apos;re', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Hypothesis: They're <unk> <unk> and <unk> in summer.
2024-05-27 01:31:09,982 - INFO - joeynmt.training - Example #4
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:31:09,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'for', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:31:09,982 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> for the last 25 years.
2024-05-27 01:31:24,380 - INFO - joeynmt.training - Epoch  10, Step:    28100, Batch Loss:     1.002518, Batch Acc: 0.633003, Tokens per Sec:     4718, Lr: 0.000210
2024-05-27 01:31:39,128 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     1.066193, Batch Acc: 0.637636, Tokens per Sec:     4753, Lr: 0.000210
2024-05-27 01:31:53,455 - INFO - joeynmt.training - Epoch  10, Step:    28300, Batch Loss:     1.417862, Batch Acc: 0.631899, Tokens per Sec:     4839, Lr: 0.000210
2024-05-27 01:32:07,869 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     1.134446, Batch Acc: 0.626989, Tokens per Sec:     4913, Lr: 0.000210
2024-05-27 01:32:21,820 - INFO - joeynmt.training - Epoch  10, Step:    28500, Batch Loss:     1.269222, Batch Acc: 0.635580, Tokens per Sec:     4865, Lr: 0.000210
2024-05-27 01:32:21,820 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:32:21,820 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:32:47,768 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.47, acc:   0.54, generation: 25.9261[sec], evaluation: 0.0000[sec]
2024-05-27 01:32:47,934 - INFO - joeynmt.helpers - delete models/word_level_model/21500.ckpt
2024-05-27 01:32:47,935 - INFO - joeynmt.training - Example #0
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'showed', 'these', '<unk>', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:32:47,935 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:32:47,935 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:32:47,935 - INFO - joeynmt.training - 	Hypothesis: I showed these <unk> <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk>, <unk> 40 percent <unk>.
2024-05-27 01:32:47,935 - INFO - joeynmt.training - Example #1
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:32:47,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this problem because it doesn't show the <unk> of the ice.
2024-05-27 01:32:47,936 - INFO - joeynmt.training - Example #2
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'global', 'warming', '.', '</s>']
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> is, in a certain sense, the heart <unk> of the global global warming.
2024-05-27 01:32:47,936 - INFO - joeynmt.training - Example #3
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:32:47,936 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in the summer.
2024-05-27 01:32:47,936 - INFO - joeynmt.training - Example #4
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:32:47,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:32:47,937 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:32:47,937 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:32:47,937 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> <unk> on the last 25 years.
2024-05-27 01:33:02,797 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     1.242498, Batch Acc: 0.628428, Tokens per Sec:     4550, Lr: 0.000210
2024-05-27 01:33:17,434 - INFO - joeynmt.training - Epoch  10, Step:    28700, Batch Loss:     1.186012, Batch Acc: 0.626105, Tokens per Sec:     4674, Lr: 0.000210
2024-05-27 01:33:32,258 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.109840, Batch Acc: 0.633269, Tokens per Sec:     4658, Lr: 0.000210
2024-05-27 01:33:47,214 - INFO - joeynmt.training - Epoch  10, Step:    28900, Batch Loss:     1.121956, Batch Acc: 0.632154, Tokens per Sec:     4687, Lr: 0.000210
2024-05-27 01:34:01,527 - INFO - joeynmt.training - Epoch  10, Step:    29000, Batch Loss:     1.080150, Batch Acc: 0.629682, Tokens per Sec:     4988, Lr: 0.000210
2024-05-27 01:34:01,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:34:01,527 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:34:30,095 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.55, generation: 28.5467[sec], evaluation: 0.0000[sec]
2024-05-27 01:34:30,256 - INFO - joeynmt.helpers - delete models/word_level_model/28500.ckpt
2024-05-27 01:34:30,257 - INFO - joeynmt.helpers - delete /Users/siz/Documents/mt-exercise-5/models/word_level_model/28500.ckpt
2024-05-27 01:34:30,257 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/siz/Documents/mt-exercise-5/models/word_level_model/28500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/siz/Documents/mt-exercise-5/models/word_level_model/28500.ckpt')
2024-05-27 01:34:30,257 - INFO - joeynmt.training - Example #0
2024-05-27 01:34:30,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:34:30,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:34:30,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that the <unk> <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk> <unk>, <unk> 40 percent <unk>.
2024-05-27 01:34:30,258 - INFO - joeynmt.training - Example #1
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Hypothesis: I <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 01:34:30,258 - INFO - joeynmt.training - Example #2
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:34:30,258 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart <unk> of the global climate system.
2024-05-27 01:34:30,258 - INFO - joeynmt.training - Example #3
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:34:30,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:34:30,259 - INFO - joeynmt.training - Example #4
2024-05-27 01:34:30,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:34:30,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:34:30,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'on', 'the', '<unk>', 'of', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:34:30,259 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> on the <unk> of the last 25 years.
2024-05-27 01:34:44,651 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     1.222198, Batch Acc: 0.630784, Tokens per Sec:     4769, Lr: 0.000210
2024-05-27 01:34:59,549 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     1.152548, Batch Acc: 0.626679, Tokens per Sec:     4588, Lr: 0.000210
2024-05-27 01:35:14,115 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     1.350557, Batch Acc: 0.627511, Tokens per Sec:     4700, Lr: 0.000210
2024-05-27 01:35:28,933 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     1.100769, Batch Acc: 0.627452, Tokens per Sec:     4629, Lr: 0.000210
2024-05-27 01:35:44,330 - INFO - joeynmt.training - Epoch  10, Step:    29500, Batch Loss:     1.205356, Batch Acc: 0.628537, Tokens per Sec:     4543, Lr: 0.000210
2024-05-27 01:35:44,330 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:35:44,330 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:36:11,892 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.46, acc:   0.55, generation: 27.5412[sec], evaluation: 0.0000[sec]
2024-05-27 01:36:11,894 - INFO - joeynmt.training - Example #0
2024-05-27 01:36:11,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:36:11,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:36:11,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'showed', 'these', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', ',', '<unk>', '<unk>', '<unk>', ',', 'has', '<unk>', '40', 'percent', '<unk>', '.', '</s>']
2024-05-27 01:36:11,894 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:36:11,894 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:36:11,894 - INFO - joeynmt.training - 	Hypothesis: I showed these <unk> <unk> <unk>, which for almost three million years has had the size of the United States <unk>, <unk> <unk> <unk>, has <unk> 40 percent <unk>.
2024-05-27 01:36:11,894 - INFO - joeynmt.training - Example #1
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the ice <unk>.
2024-05-27 01:36:11,895 - INFO - joeynmt.training - Example #2
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', 'heart', '<unk>', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the heart <unk> of the global climate system.
2024-05-27 01:36:11,895 - INFO - joeynmt.training - Example #3
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', '<unk>', '<unk>', 'and', '<unk>', 'in', 'the', 'summer', '.', '</s>']
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:36:11,895 - INFO - joeynmt.training - 	Hypothesis: They <unk> <unk> and <unk> in the summer.
2024-05-27 01:36:11,895 - INFO - joeynmt.training - Example #4
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:36:11,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'for', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:36:11,896 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:36:11,896 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:36:11,896 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> for the last 25 years.
2024-05-27 01:36:27,119 - INFO - joeynmt.training - Epoch  10, Step:    29600, Batch Loss:     1.252551, Batch Acc: 0.626508, Tokens per Sec:     4497, Lr: 0.000210
2024-05-27 01:36:42,258 - INFO - joeynmt.training - Epoch  10, Step:    29700, Batch Loss:     1.308436, Batch Acc: 0.622618, Tokens per Sec:     4440, Lr: 0.000210
2024-05-27 01:36:57,724 - INFO - joeynmt.training - Epoch  10, Step:    29800, Batch Loss:     1.300086, Batch Acc: 0.626549, Tokens per Sec:     4440, Lr: 0.000210
2024-05-27 01:37:12,248 - INFO - joeynmt.training - Epoch  10, Step:    29900, Batch Loss:     1.116731, Batch Acc: 0.622584, Tokens per Sec:     4863, Lr: 0.000210
2024-05-27 01:37:27,089 - INFO - joeynmt.training - Epoch  10, Step:    30000, Batch Loss:     1.216632, Batch Acc: 0.626436, Tokens per Sec:     4644, Lr: 0.000210
2024-05-27 01:37:27,090 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:37:27,090 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:37:54,023 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.45, acc:   0.54, generation: 26.9125[sec], evaluation: 0.0000[sec]
2024-05-27 01:37:54,187 - INFO - joeynmt.helpers - delete models/word_level_model/28000.ckpt
2024-05-27 01:37:54,188 - INFO - joeynmt.training - Example #0
2024-05-27 01:37:54,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:37:54,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:37:54,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:37:54,188 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:37:54,188 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:37:54,188 - INFO - joeynmt.training - 	Hypothesis: <unk> year I showed these <unk> <unk> that <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk> <unk>, <unk> of the United States <unk> <unk>, <unk> of 40 percent.
2024-05-27 01:37:54,189 - INFO - joeynmt.training - Example #1
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', '&apos;m', 'going', 'to', '<unk>', 'this', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '<unk>', '.', '</s>']
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Hypothesis: I'm going to <unk> this <unk> of the problem because it doesn't show the <unk> of the ice <unk>.
2024-05-27 01:37:54,189 - INFO - joeynmt.training - Example #2
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'certain', 'sense', ',', 'the', 'heart', 'of', 'the', 'climate', 'change', '.', '</s>']
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a certain sense, the heart of the climate change.
2024-05-27 01:37:54,189 - INFO - joeynmt.training - Example #3
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '&apos;s', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:37:54,189 - INFO - joeynmt.training - 	Hypothesis: It's <unk> and <unk> in summer.
2024-05-27 01:37:54,189 - INFO - joeynmt.training - Example #4
2024-05-27 01:37:54,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:37:54,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:37:54,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'will', 'be', 'a', '<unk>', '<unk>', 'in', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:37:54,190 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:37:54,190 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:37:54,190 - INFO - joeynmt.training - 	Hypothesis: The next <unk> will be a <unk> <unk> in the last 25 years.
2024-05-27 01:38:09,327 - INFO - joeynmt.training - Epoch  10, Step:    30100, Batch Loss:     1.118949, Batch Acc: 0.623653, Tokens per Sec:     4516, Lr: 0.000210
2024-05-27 01:38:23,327 - INFO - joeynmt.training - Epoch  10, Step:    30200, Batch Loss:     1.259566, Batch Acc: 0.624445, Tokens per Sec:     4919, Lr: 0.000210
2024-05-27 01:38:37,237 - INFO - joeynmt.training - Epoch  10, Step:    30300, Batch Loss:     1.145081, Batch Acc: 0.619638, Tokens per Sec:     4724, Lr: 0.000210
2024-05-27 01:38:51,351 - INFO - joeynmt.training - Epoch  10, Step:    30400, Batch Loss:     1.224499, Batch Acc: 0.620871, Tokens per Sec:     5028, Lr: 0.000210
2024-05-27 01:39:05,497 - INFO - joeynmt.training - Epoch  10, Step:    30500, Batch Loss:     1.117907, Batch Acc: 0.623601, Tokens per Sec:     4826, Lr: 0.000210
2024-05-27 01:39:05,497 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:39:05,497 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:39:35,025 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.46, acc:   0.55, generation: 29.5060[sec], evaluation: 0.0000[sec]
2024-05-27 01:39:35,027 - INFO - joeynmt.training - Example #0
2024-05-27 01:39:35,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['L&apos;', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'diapositive', 'per', 'dimostrare', 'che', 'la', 'calotta', 'glaciale', 'artica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '48', 'Stati', 'Uniti', 'continentali', ',', 'si', 'è', 'ristretta', 'del', '40', '%', '.']
2024-05-27 01:39:35,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states', ',', 'has', 'shrunk', 'by', '40', 'percent', '.']
2024-05-27 01:39:35,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'year', ',', 'I', 'showed', 'these', '<unk>', '<unk>', 'that', 'the', '<unk>', '<unk>', '<unk>', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', 'the', 'United', 'States', '<unk>', '<unk>', ',', '<unk>', '<unk>', 'of', '40', 'percent', '.', '</s>']
2024-05-27 01:39:35,027 - INFO - joeynmt.training - 	Source:     L&apos; anno scorso ho mostrato queste diapositive per dimostrare che la calotta glaciale artica , che per quasi tre milioni di anni ha avuto le dimensioni dei 48 Stati Uniti continentali , si è ristretta del 40 % .
2024-05-27 01:39:35,027 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2024-05-27 01:39:35,027 - INFO - joeynmt.training - 	Hypothesis: <unk> year, I showed these <unk> <unk> that the <unk> <unk> <unk>, which for almost three million years had the size of the United States <unk> <unk>, <unk> <unk> of 40 percent.
2024-05-27 01:39:35,028 - INFO - joeynmt.training - Example #1
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tuttavia', 'questo', 'sottovaluta', 'la', 'gravità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'spessore', 'del', 'ghiaccio', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'thickness', 'of', 'the', 'ice', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'this', '<unk>', 'the', '<unk>', 'of', 'the', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', '<unk>', 'of', 'the', 'ice', '.', '</s>']
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema perché non mostra lo spessore del ghiaccio .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Hypothesis: <unk> this <unk> the <unk> of the problem because it doesn't show the <unk> of the ice.
2024-05-27 01:39:35,028 - INFO - joeynmt.training - Example #2
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'calotta', 'glaciale', 'artica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pulsante', 'del', 'sistema', 'climatico', 'globale', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'arctic', 'ice', 'cap', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', '<unk>', 'is', ',', 'in', 'a', 'way', ',', 'the', '<unk>', 'heart', 'of', 'the', 'global', 'climate', 'system', '.', '</s>']
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è , in un certo senso , il cuore pulsante del sistema climatico globale .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is , in a sense , the beating heart of the global climate system .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> <unk> is, in a way, the <unk> heart of the global climate system.
2024-05-27 01:39:35,028 - INFO - joeynmt.training - Example #3
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'espande', 'd&apos;', 'inverno', 'e', 'si', 'ritira', 'd&apos;', 'estate', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer', '.']
2024-05-27 01:39:35,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', '<unk>', '<unk>', 'and', '<unk>', 'in', 'summer', '.', '</s>']
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Source:     Si espande d&apos; inverno e si ritira d&apos; estate .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer .
2024-05-27 01:39:35,028 - INFO - joeynmt.training - 	Hypothesis: It <unk> <unk> and <unk> in summer.
2024-05-27 01:39:35,029 - INFO - joeynmt.training - Example #4
2024-05-27 01:39:35,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossima', 'diapositiva', 'sarà', 'una', 'rapida', 'carrellata', 'sugli', 'avvenimenti', 'degli', 'ultimi', '25', 'anni', '.']
2024-05-27 01:39:35,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapid', 'fast-forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2024-05-27 01:39:35,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '<unk>', 'is', 'going', 'to', 'be', 'a', '<unk>', '<unk>', 'on', 'the', 'last', '25', 'years', '.', '</s>']
2024-05-27 01:39:35,029 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida carrellata sugli avvenimenti degli ultimi 25 anni .
2024-05-27 01:39:35,029 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2024-05-27 01:39:35,029 - INFO - joeynmt.training - 	Hypothesis: The next <unk> is going to be a <unk> <unk> on the last 25 years.
2024-05-27 01:39:50,094 - INFO - joeynmt.training - Epoch  10, Step:    30600, Batch Loss:     1.214247, Batch Acc: 0.622373, Tokens per Sec:     4525, Lr: 0.000210
2024-05-27 01:40:04,386 - INFO - joeynmt.training - Epoch  10, Step:    30700, Batch Loss:     1.416265, Batch Acc: 0.620920, Tokens per Sec:     4779, Lr: 0.000210
2024-05-27 01:40:19,759 - INFO - joeynmt.training - Epoch  10, Step:    30800, Batch Loss:     1.113675, Batch Acc: 0.622459, Tokens per Sec:     4467, Lr: 0.000210
2024-05-27 01:40:27,524 - INFO - joeynmt.training - Epoch  10: total training loss 3656.58
2024-05-27 01:40:27,524 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-27 01:40:27,524 - INFO - joeynmt.training - Best validation result (greedy) at step    27500:   4.40 ppl.
2024-05-27 01:40:27,534 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-27 01:40:27,576 - INFO - joeynmt.model - Enc-dec model built.
2024-05-27 01:40:27,608 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/word_level_model/27500.ckpt.
2024-05-27 01:40:27,610 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=None)
2024-05-27 01:40:27,611 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-27 01:40:27,611 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:40:27,611 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:40:47,578 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 19.9462[sec], evaluation: 0.0000[sec]
2024-05-27 01:40:47,580 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/word_level_model/00027500.hyps.dev.
2024-05-27 01:40:47,580 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-27 01:40:47,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 01:40:47,580 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 01:41:19,920 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 32.3073[sec], evaluation: 0.0000[sec]
2024-05-27 01:41:19,925 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/word_level_model/00027500.hyps.test.
