2024-05-27 17:37:58,648 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                           cfg.name : bpe_level_model_2000
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                     cfg.data.train : data/head100k.train.it-en
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.it-en
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.it-en
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : bpe_data/2000/bpe_vocab.it-en.joint.cleaned
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2024-05-27 17:37:58,648 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : bpe_data/2000/codes.BPE
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : bpe_data/2000/bpe_vocab.it-en.joint.cleaned
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : bpe_data/2000/codes.BPE
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_level_model_2000
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-27 17:37:58,649 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-27 17:37:58,650 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-27 17:37:58,652 - INFO - joeynmt.data - Building tokenizer...
2024-05-27 17:37:58,654 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-27 17:37:58,654 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-27 17:37:58,654 - INFO - joeynmt.data - Loading train set...
2024-05-27 17:37:58,738 - INFO - joeynmt.data - Building vocabulary...
2024-05-27 17:37:58,765 - INFO - joeynmt.data - Loading dev set...
2024-05-27 17:37:58,766 - INFO - joeynmt.data - Loading test set...
2024-05-27 17:37:58,768 - INFO - joeynmt.data - Data loaded.
2024-05-27 17:37:58,768 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 17:37:58,768 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 17:37:58,768 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 17:37:58,768 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ or@@ e: ar@@ rest@@ iamo il ri@@ sc@@ al@@ d@@ amento glob@@ ale
	[TRG] A@@ l G@@ or@@ e: A@@ ver@@ ting the c@@ lim@@ ate c@@ ri@@ si@@ s
2024-05-27 17:37:58,768 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) the (6) e (7) to (8) di (9) in
2024-05-27 17:37:58,768 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) the (6) e (7) to (8) di (9) in
2024-05-27 17:37:58,768 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 1991
2024-05-27 17:37:58,768 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 1991
2024-05-27 17:37:58,769 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-27 17:37:58,818 - INFO - joeynmt.model - Enc-dec model built.
2024-05-27 17:37:58,820 - INFO - joeynmt.model - Total params: 3408896
2024-05-27 17:37:58,820 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2024-05-27 17:37:58,820 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1991),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1991),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-27 17:37:58,820 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-27 17:37:58,821 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-27 17:37:58,821 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-27 17:37:58,821 - INFO - joeynmt.training - EPOCH 1
2024-05-27 17:38:13,301 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.023787, Batch Acc: 0.038802, Tokens per Sec:     4980, Lr: 0.000300
2024-05-27 17:38:28,506 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.843979, Batch Acc: 0.060224, Tokens per Sec:     4678, Lr: 0.000300
2024-05-27 17:38:43,789 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.706351, Batch Acc: 0.075433, Tokens per Sec:     4589, Lr: 0.000300
2024-05-27 17:38:58,994 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.666115, Batch Acc: 0.085762, Tokens per Sec:     4637, Lr: 0.000300
2024-05-27 17:39:13,422 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.562494, Batch Acc: 0.093670, Tokens per Sec:     4982, Lr: 0.000300
2024-05-27 17:39:13,422 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:39:13,422 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:40:43,882 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.54, ppl:  34.45, acc:   0.10, generation: 90.4407[sec], evaluation: 0.0000[sec]
2024-05-27 17:40:43,884 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:40:44,039 - INFO - joeynmt.training - Example #0
2024-05-27 17:40:44,039 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:40:44,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:40:44,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 's', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 's', 'to', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 't', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 's', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@']
2024-05-27 17:40:44,039 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Hypothesis: So the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the wes of the ws of the wes of the wes of the wes of the wes of the ws to the wes of the wes of the wt of the wes of the wes of the ws of the wes of the wes of the w
2024-05-27 17:40:44,040 - INFO - joeynmt.training - Example #1
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'you', 'have', 'to', 'have', 'to', 'the', 'f@@', 'p@@', 'es', 'to', 'have', 'to', 'the', 'f@@', 'p@@', 'es', 'to', 'the', 'f@@', 'p@@', 'es', 'to', 'have', 'to', 'have', 'to', 'have', 'to', 'have', 'to', 'have', 'to', 'have', 'to', 'have', 'the', 'w@@', 'e.', '</s>']
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Hypothesis: So you have to have to the fpes to have to the fpes to the fpes to have to have to have to have to have to have to have the we.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - Example #2
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'the', 'f@@', 'c@@', 't', 'of', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'w@@', 's', 'of', 'the', 'w@@', 'ed', 'the', 'w@@', 's', 'to', 'have', 'the', 'w@@', 's', 'to', 'have', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@', 'p@@', 'es', 'of', 'the', 'f@@', 'd@@', 'es', 'of', 'the', 'f@@', 'p@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'w@@', 'es', 'of', 'the', 'f@@', 'p@@']
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Hypothesis: So the fct of the wes of the fpes of the wes of the fpes of the fpes of the wes of the wes of the wes of the wes of the wes of the fpes of the ws of the wed the ws to have the ws to have the wes of the fpes of the wes of the fppes of the fdes of the fpes of the wes of the wes of the fp
2024-05-27 17:40:44,040 - INFO - joeynmt.training - Example #3
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:40:44,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@', 'f@@']
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:40:44,040 - INFO - joeynmt.training - 	Hypothesis: And the fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
2024-05-27 17:40:44,041 - INFO - joeynmt.training - Example #4
2024-05-27 17:40:44,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:40:44,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:40:44,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'you', 'have', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']
2024-05-27 17:40:44,041 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:40:44,041 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:40:44,041 - INFO - joeynmt.training - 	Hypothesis: So you have a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
2024-05-27 17:40:59,801 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.501112, Batch Acc: 0.101046, Tokens per Sec:     4566, Lr: 0.000300
2024-05-27 17:41:14,915 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.453938, Batch Acc: 0.104324, Tokens per Sec:     4825, Lr: 0.000300
2024-05-27 17:41:29,438 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.417567, Batch Acc: 0.115695, Tokens per Sec:     4879, Lr: 0.000300
2024-05-27 17:41:44,441 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.320972, Batch Acc: 0.120456, Tokens per Sec:     4705, Lr: 0.000300
2024-05-27 17:42:00,567 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.257594, Batch Acc: 0.129622, Tokens per Sec:     4446, Lr: 0.000300
2024-05-27 17:42:00,568 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:42:00,568 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:43:34,231 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.31, ppl:  27.40, acc:   0.13, generation: 93.6508[sec], evaluation: 0.0000[sec]
2024-05-27 17:43:34,232 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:43:34,386 - INFO - joeynmt.training - Example #0
2024-05-27 17:43:34,386 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:43:34,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:43:34,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'have', 'a', 'little', 'little', 'little', 'little', 'little', 'ex@@', 'c@@', 'es', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'f@@', 'f', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'b@@', 'ed', 'to', 'the', 'f@@', 's', 'of', 'the', 'world', 'of', 'the', 'f@@', 's', 'of', 'the', 'f@@', 's', 'of', 'the', 'f@@', 'f@@', 'f@@', 's', 'of', 'the', 'world', 'of', 'the', 'f@@', 'f', 'of', 'the', 'f@@', 'f@@', 'f@@', 'th@@', '.', '</s>']
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Hypothesis: I have a little little little little little exces of the world of the world of the world of the world of the world of the world of the world of the ff of the world of the world of the world of the world of the world of the bed to the fs of the world of the fs of the fs of the fffs of the world of the ff of the fffth.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - Example #1
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't", "don't"]
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Hypothesis: I don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't don't
2024-05-27 17:43:34,387 - INFO - joeynmt.training - Example #2
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:43:34,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'f@@', 'f', 'of', 'the', 'b@@', 'ing', 'of', 'the', 'b@@', 'ing', 'of', 'the', 'b@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 's.', '</s>']
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:43:34,387 - INFO - joeynmt.training - 	Hypothesis: And the ff of the bing of the bing of the byyyyyyyyyys.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - Example #3
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'can', 'have', 'a', 'f@@', 's', 'and', 'the', 'w@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e.', '</s>']
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Hypothesis: And you can have a fs and the weeeeeeeeeeeeeeeeeeeee.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - Example #4
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:43:34,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'a', 'little', 'little', 'little', 'little', 'little', 'c@@', 'e', 'to', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'ex@@', 'c@@', 'e.', '</s>']
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:43:34,388 - INFO - joeynmt.training - 	Hypothesis: And I have a little little little little little ce to a little little little little little little little little little little little little little little little little exce.
2024-05-27 17:43:52,547 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.245015, Batch Acc: 0.139435, Tokens per Sec:     3934, Lr: 0.000300
2024-05-27 17:44:08,384 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.370767, Batch Acc: 0.153407, Tokens per Sec:     4472, Lr: 0.000300
2024-05-27 17:44:24,761 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.142280, Batch Acc: 0.166348, Tokens per Sec:     4441, Lr: 0.000300
2024-05-27 17:44:40,583 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.083848, Batch Acc: 0.177908, Tokens per Sec:     4445, Lr: 0.000300
2024-05-27 17:44:56,826 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.958633, Batch Acc: 0.189289, Tokens per Sec:     4533, Lr: 0.000300
2024-05-27 17:44:56,826 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:44:56,826 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:46:35,797 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.01, ppl:  20.37, acc:   0.19, generation: 98.9611[sec], evaluation: 0.0000[sec]
2024-05-27 17:46:35,799 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:46:35,948 - INFO - joeynmt.training - Example #0
2024-05-27 17:46:35,948 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:46:35,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:46:35,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'these', 'things', 'that', 'that', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'world', 'of', 'the', 're@@', 's', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 's@@', 'and@@', 's', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of']
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Hypothesis: And I think I think these things that that the first of the first of the world of the res of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the sands of the world of the world of the world of the world of
2024-05-27 17:46:35,949 - INFO - joeynmt.training - Example #1
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'this', 'is', 'this', 'is', 'the', 'first', 'first', 'is', 'the', 'first', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'first', 'of', 'the', 'same', 'time.', '</s>']
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Hypothesis: And this is this is this is the first first is the first of the world of the world of the world of the first of the first of the world of the world of the world of the world of the world of the world of the world of the world of the first of the same time.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - Example #2
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'b@@', 'est', 'of', 'the', 'b@@', 'est', 'of', 'the', 'b@@', 'est', 'of', 'the', 'b@@', 'est', 'of', 'the', 'b@@', 'est', 'of', 'the', 's@@', 'our@@', 's', 'of', 'the', 's@@', 'our@@', 's', 'of', 'the', 'b@@', 'est', 'of', 'the', 's@@', 'our@@', '.', '</s>']
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - 	Hypothesis: The best of the best of the best of the best of the best of the sours of the sours of the best of the sour.
2024-05-27 17:46:35,949 - INFO - joeynmt.training - Example #3
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:46:35,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'be', 'a', 'ch@@', 'ar@@', 't', 'and', 'they', 'have', 'a', 'little', 'bit', 'and', 'they', 'have', 'a', 'little', 'bit', 'and', 'and', 'they', 'have', 'a', 'little', 'bit', 'and', 'they', 'have', 'a', 'little', 'bit', 'of', 'the', 'w@@', 'w@@', 'w@@', 'ell@@', '.', '</s>']
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Hypothesis: You can be a chart and they have a little bit and they have a little bit and and they have a little bit and they have a little bit of the wwwell.
2024-05-27 17:46:35,950 - INFO - joeynmt.training - Example #4
2024-05-27 17:46:35,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:46:35,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:46:35,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'lot', 'of', 'the', 'first', 'of', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 'first', 'h@@', 'un@@', 'd', 'of', 'the', 'first', 'h@@', 'and@@', 's', 'of', 'the', 'first', 'h@@', 'un@@', 'd', 'of', 'the', 'first', 'of', 'the', 'first', 'bit', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'first', 'h@@', 'un@@', 'd', 'of', 'the', 'same', 'thing@@', 's.', '</s>']
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:46:35,950 - INFO - joeynmt.training - 	Hypothesis: The lot of the first of a little bit of a little bit of the first hund of the first hands of the first hund of the first of the first bit of the world of the world of the world of the world of the first hund of the same things.
2024-05-27 17:46:51,315 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.962182, Batch Acc: 0.196328, Tokens per Sec:     4682, Lr: 0.000300
2024-05-27 17:47:06,937 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.935637, Batch Acc: 0.203288, Tokens per Sec:     4470, Lr: 0.000300
2024-05-27 17:47:22,655 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.795213, Batch Acc: 0.212789, Tokens per Sec:     4549, Lr: 0.000300
2024-05-27 17:47:37,748 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.760532, Batch Acc: 0.221801, Tokens per Sec:     4714, Lr: 0.000300
2024-05-27 17:47:52,064 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.748398, Batch Acc: 0.229895, Tokens per Sec:     4989, Lr: 0.000300
2024-05-27 17:47:52,064 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:47:52,064 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:49:28,841 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.82, ppl:  16.71, acc:   0.22, generation: 96.7681[sec], evaluation: 0.0000[sec]
2024-05-27 17:49:28,842 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:49:28,995 - INFO - joeynmt.training - Example #0
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'ese', 'are', 'the', 'first', 'of', 'these', 'people', 'to', 'the', 'first', 'of', 'the', 'b@@', 'a@@', 'way', 'to', 'the', 'b@@', 'ro@@', 'om@@', 's', 'that', 'was', 'the', 'b@@', 'ro@@', 'om@@', 's', 'of', 'the', 'b@@', 'est', 'of', 'the', '1@@', '0@@', ',000', 'years', 'ag@@', 'ag@@', 'ag@@', 'es', 'of', 'the', '1@@', '0@@', ',000', 'years', 'ag@@', 'ag@@', 'ag@@', 'ag@@', 'es', 'of', 'the', '1@@', '0@@', ',000', 'years', 'ag@@', 'o,', '</s>']
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Hypothesis: These are the first of these people to the first of the baway to the brooms that was the brooms of the best of the 10,000 years agagages of the 10,000 years agagagages of the 10,000 years ago,
2024-05-27 17:49:28,995 - INFO - joeynmt.training - Example #1
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'is', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'b@@', 'a@@', 'in', 'the', 'same', 'of', 'the', 'b@@', 'ut@@', 'e.', '</s>']
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:49:28,995 - INFO - joeynmt.training - 	Hypothesis: The first thing that is the same of the same of the same of the same of the same of the bain the same of the bute.
2024-05-27 17:49:28,995 - INFO - joeynmt.training - Example #2
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:49:28,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'b@@', 'ro@@', 'c@@', 'k@@', 'et@@', 's', 'of', 'the', 'b@@', 'ut@@', ',', 'the', 'b@@', 'ut@@', 't@@', 'y@@', 'p@@', 't', 'of', 'the', 'b@@', 'a@@', 'th@@', 'y', 'of', 'the', 'b@@', 'ro@@', 'c@@', 't', 'of', 'the', 'b@@', 'ro@@', 'c@@', 'k@@', 'e.', '</s>']
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Hypothesis: The brockets of the but, the buttypt of the bathy of the broct of the brocke.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - Example #3
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'ey@@', "'re", 'h@@', 'im@@', 'es', 'and', 'the', 'b@@', 'ut@@', 'ut@@', 'ut@@', 'ion@@', '.', '</s>']
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Hypothesis: They're himes and the bututution.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - Example #4
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:49:28,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'the', '1@@', '0@@', ',000', 'dollar@@', 's', 'of', 'the', '1@@', '0@@', '-@@', 'f@@', 'our', 'year@@', '-@@', 'f@@', 'ound', 'of', 'the', '1@@', '0@@', '.', '</s>']
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:49:28,996 - INFO - joeynmt.training - 	Hypothesis: The first thing is the 10,000 dollars of the 10-four year-found of the 10.
2024-05-27 17:49:45,114 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.800352, Batch Acc: 0.234119, Tokens per Sec:     4352, Lr: 0.000300
2024-05-27 17:50:01,256 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.785864, Batch Acc: 0.239340, Tokens per Sec:     4468, Lr: 0.000300
2024-05-27 17:50:16,493 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.732293, Batch Acc: 0.242465, Tokens per Sec:     4697, Lr: 0.000300
2024-05-27 17:50:31,353 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.572870, Batch Acc: 0.249893, Tokens per Sec:     4897, Lr: 0.000300
2024-05-27 17:50:46,931 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.727192, Batch Acc: 0.261262, Tokens per Sec:     4549, Lr: 0.000300
2024-05-27 17:50:46,931 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:50:46,931 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:52:27,713 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.68, ppl:  14.56, acc:   0.25, generation: 100.7731[sec], evaluation: 0.0000[sec]
2024-05-27 17:52:27,715 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:52:27,863 - INFO - joeynmt.training - Example #0
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'first', 'first', 'to', 'the', 'first', 'time', 'these', 's@@', 'li@@', 've', 'to', 'the', 'way', 'to', 'the', 's@@', 'li@@', 'ght', 'of', 'the', 's@@', 'li@@', 'ght', 'of', 'the', 'si@@', 'de', 'of', 'the', 'last', 'years', 'ag@@', 'o,', 'in', 'the', '19@@', '8@@', '0@@', ',000', 'years', 'ag@@', 'o,', 'in', 'the', '19@@', '8@@', '0@@', 's,', 'the', '19@@', '8@@', '0', 'percent', 'of', 'the', '19@@', '6@@', '0', 'percent', 'of', 'the', '19@@', '6@@', '0', 'percent', 'of', 'the', 'last', 'year@@', 's.', '</s>']
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Hypothesis: The first first first to the first time these slive to the way to the slight of the slight of the side of the last years ago, in the 1980,000 years ago, in the 1980s, the 1980 percent of the 1960 percent of the 1960 percent of the last years.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - Example #1
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:52:27,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 're@@', 'ad', 'of', 'this', 'is', 'the', 're@@', 'c@@', 'lu@@', 'ding', 'the', 'same', 'way', 'because', 'the', 'same', 'way', 'the', 're@@', 'cent@@', 'ly', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 's@@', 'qu@@', 'qu@@', 'ite', 'of', 'the', 'c@@', 'le@@', 'ar', 'of', 'the', 'same', 'si@@', 'de.', '</s>']
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - 	Hypothesis: The read of this is the recluding the same way because the same way the recently of the result of the sququite of the clear of the same side.
2024-05-27 17:52:27,863 - INFO - joeynmt.training - Example #2
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 're@@', 'al@@', 'al@@', 'al@@', 'ong', 'the', 'c@@', 'le@@', 'ar', 'of', 'the', 're@@', 'al@@', 'ity', 'of', 'the', 're@@', 'c@@', 'lu@@', 'm@@', 's', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'c@@', 'le@@', 'ar', 'of', 'the', 're@@', 'al@@', 'th@@', '.', '</s>']
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Hypothesis: The realalalong the clear of the reality of the reclums of the world of the world of the world of the world of the world of the world of the clear of the realth.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - Example #3
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', "ere's", 'a', 'h@@', 'y@@', 'p@@', 'e', 'and', 'the', 's@@', 'li@@', 'st@@', 'a@@', 'y', 'and', 'the', 'b@@', 'a@@', 'th@@', 'th@@', '.', '</s>']
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Hypothesis: There's a hype and the slistay and the bathth.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - Example #4
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:52:27,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 're@@', 'ad', 'of', 'the', 're@@', 'c@@', 'ra@@', 'w@@', 'ing', 'a', 'lot', 'of', 'the', 're@@', 'ali@@', 'z@@', 'ed', 'to', 'the', 'last', 'year@@', 's,', 'the', 'last', 'year@@', 's.', '</s>']
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:52:27,864 - INFO - joeynmt.training - 	Hypothesis: The read of the recrawing a lot of the realized to the last years, the last years.
2024-05-27 17:52:43,058 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.585883, Batch Acc: 0.269463, Tokens per Sec:     4755, Lr: 0.000300
2024-05-27 17:52:57,894 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.562237, Batch Acc: 0.275967, Tokens per Sec:     4827, Lr: 0.000300
2024-05-27 17:53:13,681 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.542873, Batch Acc: 0.276477, Tokens per Sec:     4492, Lr: 0.000300
2024-05-27 17:53:29,828 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.343213, Batch Acc: 0.284255, Tokens per Sec:     4268, Lr: 0.000300
2024-05-27 17:53:44,877 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.626918, Batch Acc: 0.289862, Tokens per Sec:     4800, Lr: 0.000300
2024-05-27 17:53:44,879 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:53:44,879 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:55:07,981 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.56, ppl:  12.90, acc:   0.28, generation: 83.0946[sec], evaluation: 0.0000[sec]
2024-05-27 17:55:07,982 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:55:08,134 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/500.ckpt
2024-05-27 17:55:08,135 - INFO - joeynmt.training - Example #0
2024-05-27 17:55:08,135 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', 'have', 'to', 'show', 'you', 'that', 'this', 'is', 'going', 'to', 'show', 'you', 'that', 'the', 'f@@', 'our', 'h@@', 'ot@@', 'ot@@', 'ally', 're@@', 'co@@', 'up@@', 'le', 'of', 'the', 'h@@', 'un@@', 'd', 'of', 'two', 'million', 'years', 'of', 'years', 'ag@@', 'o,', 'that', 'was', 'a', '4@@', '0', 'percent', 'of', 'the', 'U@@', '.@@', 'S@@', '.', '</s>']
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Hypothesis: They have to show you that this is going to show you that the four hototally recouple of the hund of two million years of years ago, that was a 40 percent of the U.S.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - Example #1
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 's', 'this', 'is', 'that', 'this', 'is', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'same', 'thing', 'to', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'same', 'thing', 'to', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'same', 'thing.', '</s>']
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Hypothesis: As this is that this is the problem of the problem of the problem of the problem of the same thing to the problem of the same thing to the problem of the problem of the problem of the problem of the problem of the problem of the problem of the problem of the problem of the problem of the same thing.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - Example #2
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:55:08,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'b@@', 'ro@@', 'w@@', 's', 'of', 'the', 'ro@@', 'ro@@', 'c@@', 'k@@', 'et@@', ',', 'in', 'a', 'sen@@', 'se', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'sy@@', 'ste@@', 'm', 'of', 'the', 'in@@', 'th@@', '.', '</s>']
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - 	Hypothesis: The brows of the rorocket, in a sense of the system of the system of the system of the system of the system of the system of the inth.
2024-05-27 17:55:08,136 - INFO - joeynmt.training - Example #3
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'he', 'h@@', 'y@@', 'p@@', 'e', 'and', 'h@@', 'y@@', 'd@@', 'ro@@', 'w@@', 's', 'to', 'the', 'h@@', 'y@@', 'd@@', 'ro@@', 'p@@', '.', '</s>']
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Hypothesis: She hype and hydrows to the hydrop.
2024-05-27 17:55:08,137 - INFO - joeynmt.training - Example #4
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:55:08,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 're@@', 'ad', 'of', 'the', 'f@@', 'lo@@', 've', 'is', 'a', 'very', 'ch@@', 'o@@', 'ice', 'in', 'the', 'last', 'year@@', 's,', 'the', 'last', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'h@@', 'and@@', '.', '</s>']
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:55:08,137 - INFO - joeynmt.training - 	Hypothesis: The read of the flove is a very choice in the last years, the last year-year-year-year-year-year-year-year-year-year-hand.
2024-05-27 17:55:22,730 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.443799, Batch Acc: 0.301137, Tokens per Sec:     4904, Lr: 0.000300
2024-05-27 17:55:38,931 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.342097, Batch Acc: 0.300809, Tokens per Sec:     4342, Lr: 0.000300
2024-05-27 17:55:53,493 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.458800, Batch Acc: 0.308633, Tokens per Sec:     4896, Lr: 0.000300
2024-05-27 17:56:08,214 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.533126, Batch Acc: 0.313787, Tokens per Sec:     4892, Lr: 0.000300
2024-05-27 17:56:22,766 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.358231, Batch Acc: 0.323842, Tokens per Sec:     5001, Lr: 0.000300
2024-05-27 17:56:22,766 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:56:22,766 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:57:35,272 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.44, ppl:  11.47, acc:   0.31, generation: 72.4985[sec], evaluation: 0.0000[sec]
2024-05-27 17:57:35,274 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 17:57:35,426 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/1000.ckpt
2024-05-27 17:57:35,426 - INFO - joeynmt.training - Example #0
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'got', 'this', 'one', 'of', 'these', 'two', 'bi@@', 'll@@', 'ion', 'to', 'show', 'the', 'se@@', 'x@@', 'i@@', 'es', 'that', 'the', 'se@@', 'g@@', 'l@@', 'and', 'that', 'is', 'about', 'three', 'million', 'years', 'ag@@', 'o', 'for', 'three', 'million', 'years', 'ag@@', 'o,', 'which', 'is', 'the', '2@@', '5', 'million', 'years', 'ag@@', 'o,', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'U@@', '.@@', '.@@', '.@@', '.@@', '.@@', ',', 'which', 'is', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Hypothesis: And I got this one of these two billion to show the sexies that the segland that is about three million years ago for three million years ago, which is the 25 million years ago, the United States of the U....., which is 40 percent.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - Example #1
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'most', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'h@@', 'and@@', '.', '</s>']
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - 	Hypothesis: And this is the most of the problem of the problem of the problem of the hand.
2024-05-27 17:57:35,427 - INFO - joeynmt.training - Example #2
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 17:57:35,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'y@@', 'g@@', 'l@@', 'and', 'the', 'g@@', 'l@@', 'ev@@', 'el@@', 's', 'is', 'a', 'sen@@', 'se', 'of', 'the', 'c@@', 'li@@', 'st@@', 'ic@@', 's', 'of', 'the', 'c@@', 'li@@', 'li@@', 'st@@', 'ic@@', '.', '</s>']
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Hypothesis: The gygland the glevels is a sense of the clistics of the clilistic.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - Example #3
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'w@@', 'o@@', 'pe@@', 'n', 'and', 'they', 'get', 'the', 'w@@', 'rit@@', 'i@@', 'es', 'of', 'the', 'w@@', 'om@@', 'en@@', '.', '</s>']
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Hypothesis: And the wopen and they get the writies of the women.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - Example #4
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 17:57:35,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'next', 'is', 'going', 'to', 'be', 'a', 'very', 'ch@@', 'ar@@', 'r@@', 'un@@', 'ed', 'to', 'be', 're@@', 'ali@@', 'z@@', 'ed', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 17:57:35,428 - INFO - joeynmt.training - 	Hypothesis: The next next next is going to be a very charruned to be realized the last 25 years.
2024-05-27 17:57:50,236 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.296748, Batch Acc: 0.325646, Tokens per Sec:     4820, Lr: 0.000300
2024-05-27 17:58:04,811 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.383286, Batch Acc: 0.339086, Tokens per Sec:     4894, Lr: 0.000300
2024-05-27 17:58:19,786 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.396488, Batch Acc: 0.338529, Tokens per Sec:     4751, Lr: 0.000300
2024-05-27 17:58:34,205 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.336190, Batch Acc: 0.343500, Tokens per Sec:     5032, Lr: 0.000300
2024-05-27 17:58:48,974 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.369265, Batch Acc: 0.352027, Tokens per Sec:     4891, Lr: 0.000300
2024-05-27 17:58:48,974 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 17:58:48,974 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 17:59:59,956 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.34, ppl:  10.34, acc:   0.34, generation: 70.9739[sec], evaluation: 0.0000[sec]
2024-05-27 17:59:59,959 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:00:00,110 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/1500.ckpt
2024-05-27 18:00:00,111 - INFO - joeynmt.training - Example #0
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', 'just', 'just', 'show', 'you', 'these', 'these', 'f@@', 'li@@', 'gh@@', 'ts', 'of', 'the', 'c@@', 'are', 'to', 'show', 'the', 'c@@', 'lim@@', 'ate', 'the', 'se@@', 'x@@', 'i@@', 'al', 'of', 'years', 'to', 'three', 'million', 'years', 'ag@@', 'o', 'of', 'years', 'ag@@', 'o', 'of', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'is', '4@@', '0@@', '.', '</s>']
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Hypothesis: They just just show you these these flights of the care to show the climate the sexial of years to three million years ago of years ago of years of the United States of the United States is 40.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - Example #1
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 't', 'this', 'is', 'the', 's@@', 'qu@@', 'ite', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 's@@', 'li@@', 'gh@@', 't.', '</s>']
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - 	Hypothesis: At this is the squite of the problem of the problem of the problem of the slight.
2024-05-27 18:00:00,111 - INFO - joeynmt.training - Example #2
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:00:00,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'th@@', 'ir@@', 'd', 'g@@', 'l@@', 'ac@@', 'k', 'is', 'in', 'a', 'c@@', 'lim@@', 'b', 'of', 'the', 'c@@', 'li@@', 'li@@', 'p@@', 's', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'system@@', '.', '</s>']
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Hypothesis: The third glack is in a climb of the clilips of the global system of the global system.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - Example #3
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'go', 'and', 're@@', 'al@@', 'ing', 'and', 're@@', 'co@@', 'ver@@', 'ed', 'and', 'c@@', 'rit@@', 'ic@@', 'e.', '</s>']
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Hypothesis: You can go and realing and recovered and critice.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - Example #4
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:00:00,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'next', 'to', 'be', 'a', 'f@@', 'li@@', 'ght', 'to', 'be', 'a', 'f@@', 'oo@@', 't', 'to', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:00:00,112 - INFO - joeynmt.training - 	Hypothesis: The next next next to be a flight to be a foot to the last 25 years.
2024-05-27 18:00:14,876 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.223347, Batch Acc: 0.357428, Tokens per Sec:     4648, Lr: 0.000300
2024-05-27 18:00:30,537 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.258874, Batch Acc: 0.370159, Tokens per Sec:     4691, Lr: 0.000300
2024-05-27 18:00:45,441 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.190531, Batch Acc: 0.374342, Tokens per Sec:     4780, Lr: 0.000300
2024-05-27 18:01:00,822 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.086352, Batch Acc: 0.373621, Tokens per Sec:     4658, Lr: 0.000300
2024-05-27 18:01:04,085 - INFO - joeynmt.training - Epoch   1: total training loss 12555.61
2024-05-27 18:01:04,085 - INFO - joeynmt.training - EPOCH 2
2024-05-27 18:01:14,781 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.091009, Batch Acc: 0.391016, Tokens per Sec:     5129, Lr: 0.000300
2024-05-27 18:01:14,781 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:01:14,781 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:02:37,588 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.48, acc:   0.37, generation: 82.7982[sec], evaluation: 0.0000[sec]
2024-05-27 18:02:37,591 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:02:37,840 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/2000.ckpt
2024-05-27 18:02:37,841 - INFO - joeynmt.training - Example #0
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'next', 'next', 'to', 'show', 'these', 'de@@', 'pen@@', 'si@@', 've', 'to', 'show', 'the', 'se@@', 't', 'that', 'the', 'n@@', 'ac@@', 'i@@', 'al', 'st@@', 'ac@@', 'i@@', 'al', '--', 'for', 'three', 'million', 'years', 'ag@@', 'o', 'that', 'three', 'million', 'years', 'ag@@', 'o', 'was', 'the', '4@@', '0', 'year@@', 's,', 'is', '4@@', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'the', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'si@@', 've', 'year@@']
2024-05-27 18:02:37,841 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:02:37,841 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:02:37,841 - INFO - joeynmt.training - 	Hypothesis: The next next next next to show these depensive to show the set that the nacial stacial -- for three million years ago that three million years ago was the 40 years, is 440 percent of the United States of the United States of the United States in the United States the United States of the United States the the United States of the sive year
2024-05-27 18:02:37,841 - INFO - joeynmt.training - Example #1
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:02:37,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 't', 'this', 'is', 'the', 'f@@', 'ur@@', 'po@@', 'se', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'i@@', 'gh@@', 'bor@@', 'h@@', 'ist@@', 'or@@', 'y', 'of', 'the', 'i@@', 'gh@@', 't.', '</s>']
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Hypothesis: At this is the furpose the problem of the problem of the problem of the ighborhistory of the ight.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - Example #2
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'm@@', 'y@@', 'sel@@', 'f', 'b@@', 'ac@@', 'i@@', 'al', 'is@@', 'su@@', 'e', 'is', 'a', 'sen@@', 'se', 'of', 'the', 'cu@@', 'b@@', 'b@@', 'b@@', 'it@@', 'or', 'the', 'c@@', 'li@@', 'p@@', 's', 'of', 'the', 'glob@@', 'al', 'system@@', '.', '</s>']
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Hypothesis: The myself bacial issue is a sense of the cubbbitor the clips of the global system.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - Example #3
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 're@@', 'ver@@', 's', 'and', 'in@@', 'st@@', 'it@@', 'ing', 'and', 'you', 'get', 'the', 'g@@', 'est@@', '.', '</s>']
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - 	Hypothesis: You go and revers and institing and you get the gest.
2024-05-27 18:02:37,842 - INFO - joeynmt.training - Example #4
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:02:37,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'mon@@', 'ey', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'to', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:02:37,843 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:02:37,843 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:02:37,843 - INFO - joeynmt.training - 	Hypothesis: The next next demoney will be a rapid to the last 25 years.
2024-05-27 18:02:52,304 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.197716, Batch Acc: 0.394010, Tokens per Sec:     4845, Lr: 0.000300
2024-05-27 18:03:06,907 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.095344, Batch Acc: 0.396958, Tokens per Sec:     4972, Lr: 0.000300
2024-05-27 18:03:21,704 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.135881, Batch Acc: 0.400478, Tokens per Sec:     4915, Lr: 0.000300
2024-05-27 18:03:37,034 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.058126, Batch Acc: 0.401671, Tokens per Sec:     4654, Lr: 0.000300
2024-05-27 18:03:53,485 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.017626, Batch Acc: 0.406969, Tokens per Sec:     4408, Lr: 0.000300
2024-05-27 18:03:53,485 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:03:53,485 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:05:10,464 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.75, acc:   0.39, generation: 76.9714[sec], evaluation: 0.0000[sec]
2024-05-27 18:05:10,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:05:10,611 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/2500.ckpt
2024-05-27 18:05:10,612 - INFO - joeynmt.training - Example #0
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'few', 'years', 'I', 'show@@', 'ed', 'these', 's@@', 'qu@@', 'e@@', 've', 'to', 'show', 'that', 'the', 'v@@', 'ari@@', 'ous', 'to', 'the', 'di@@', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'al', 'fi@@', 've', 'million', 'years', 'ag@@', 'o', 'was', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Hypothesis: The next next few years I showed these squeve to show that the various to the diglacial artical five million years ago was the 48 percent of the 48 percent.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - Example #1
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:05:10,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 't', 'this', 'is', 'the', 'val@@', 'u@@', 'es', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'is@@', 'su@@', 'e.', '</s>']
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - 	Hypothesis: At this is the values of the problem of the problem of the problem of the issue.
2024-05-27 18:05:10,612 - INFO - joeynmt.training - Example #2
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'r@@', 'un', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'b@@', 'or', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Hypothesis: The run of the glacial arty is in a sense, the cubor of the climate system.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - Example #3
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'put', 'it', 'in', 'the', 'b@@', 'a@@', 'si@@', 'c', 'and', 'you', 'get', 'up', 'to', 'the', 'ex@@', 't@@', 'ed.', '</s>']
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Hypothesis: You can put it in the basic and you get up to the exted.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - Example #4
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:05:10,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'ece', 'of', 'the', 'car@@', 're@@', 's@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years', 'ag@@', 'o.', '</s>']
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:05:10,613 - INFO - joeynmt.training - 	Hypothesis: The next next to be a rapiece of the carresed by the last 25 years ago.
2024-05-27 18:05:25,911 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.037174, Batch Acc: 0.408736, Tokens per Sec:     4596, Lr: 0.000300
2024-05-27 18:05:39,783 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.125172, Batch Acc: 0.414466, Tokens per Sec:     5154, Lr: 0.000300
2024-05-27 18:05:53,663 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.095001, Batch Acc: 0.416083, Tokens per Sec:     5197, Lr: 0.000300
2024-05-27 18:06:09,154 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.127355, Batch Acc: 0.418214, Tokens per Sec:     4474, Lr: 0.000300
2024-05-27 18:06:23,174 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.039260, Batch Acc: 0.419880, Tokens per Sec:     5092, Lr: 0.000300
2024-05-27 18:06:23,174 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:06:23,174 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:07:40,908 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.12, ppl:   8.30, acc:   0.40, generation: 77.7258[sec], evaluation: 0.0000[sec]
2024-05-27 18:07:40,910 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:07:41,058 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/3000.ckpt
2024-05-27 18:07:41,059 - INFO - joeynmt.training - Example #0
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'they', 'have', 'to', 'show', 'these', 'these', 's@@', 'li@@', 'de', 'of', 'the', 's@@', 'li@@', 'de', 'of', 'the', 'he@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'ar', '--', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'ag@@', 'o', 'that', 'was', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'is', 'is', 'a', 're@@', 'sul@@', 't', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', '4@@', '0@@', '.', '</s>']
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Hypothesis: And they have to show these these slide of the slide of the heacial clear -- that for almost three million years of years ago that was the 40 percent of the United States is is a result of the 40 percent of the 40 percent of the 40.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - Example #1
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', 'all', 'the', 'way', 'that', 'we', 'have', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'i@@', 'gh@@', 't.', '</s>']
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Hypothesis: And that all the way that we have the problem of the problem of the problem of the ight.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - Example #2
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:07:41,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'm@@', 'y@@', 'th@@', 'm@@', 'ic@@', 'al', 'c@@', 'le@@', 'ar', '--', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'cer@@', 'ta@@', 'in', 'the', 'cu@@', 'b@@', 'al', 'system@@', '.', '</s>']
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:07:41,060 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Hypothesis: The mythmical clear -- in a certain a certain a certain the cubal system.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - Example #3
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'w@@', 'alk', 'up', 'and', 'in@@', 'ver@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'the', 're@@', 'st', 'of', 'the', 're@@', 'al@@', 'th@@', '.', '</s>']
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Hypothesis: You walk up and invers and wind and wind the rest of the realth.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - Example #4
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:07:41,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'little', 'bit', 'of', 'the', 'last', '2@@', '5', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:07:41,061 - INFO - joeynmt.training - 	Hypothesis: The next next to be a little bit of the last 25 of the last 25 years.
2024-05-27 18:07:55,751 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.959132, Batch Acc: 0.428753, Tokens per Sec:     4777, Lr: 0.000300
2024-05-27 18:08:09,879 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.785626, Batch Acc: 0.432245, Tokens per Sec:     4994, Lr: 0.000300
2024-05-27 18:08:25,008 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     2.042306, Batch Acc: 0.433518, Tokens per Sec:     4796, Lr: 0.000300
2024-05-27 18:08:38,908 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.151648, Batch Acc: 0.427753, Tokens per Sec:     5187, Lr: 0.000300
2024-05-27 18:08:53,164 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.105582, Batch Acc: 0.434440, Tokens per Sec:     4899, Lr: 0.000300
2024-05-27 18:08:53,165 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:08:53,165 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:10:05,268 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.90, acc:   0.41, generation: 72.0959[sec], evaluation: 0.0000[sec]
2024-05-27 18:10:05,271 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:10:05,421 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/3500.ckpt
2024-05-27 18:10:05,422 - INFO - joeynmt.training - Example #0
2024-05-27 18:10:05,422 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:10:05,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:10:05,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Th@@', 'ey@@', "'re", 'going', 'to', 'show', 'these', 'new', 'de@@', 'mon@@', 'str@@', 'ate', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'l@@', 'ac@@', 'i@@', 'al', 'f@@', 'our@@', 's', 'of', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', '4@@', '0@@', '-@@', 'year@@', '.', '</s>']
2024-05-27 18:10:05,422 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:10:05,422 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:10:05,422 - INFO - joeynmt.training - 	Hypothesis: They're going to show these new demonstrate to show that the calculate that the lacial fours of years has had the size of 40 million years has had the size of 40 percent of 40-year.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - Example #1
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'll', 'this', 'to@@', 'ok', 'is', 'the', 'b@@', 'ad', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', "it's", 'not', 'show', 'it', 'is', 'not', 'show', 'it', 'is', 'not', 'show', 'the', 'l@@', 'ac@@', 'i@@', 'l@@', 'y.', '</s>']
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Hypothesis: All this took is the bad of the problem because it's not show it is not show it is not show the lacily.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - Example #2
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'st@@', 'ro@@', 'p', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 're@@', 'c@@', 'ent', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'cu@@', 'b@@', 'or', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', '.', '</s>']
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Hypothesis: The strop glacial recent is, in a certain a sense, the cucubor system of the global system of global system of global climat.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - Example #3
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:10:05,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 's', 'and', 'the', 're@@', 'al@@', '-@@', 'and', 're@@', 'co@@', 'gn@@', 'i@@', 'ze', 'of', 'the', 'est@@', '.', '</s>']
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:10:05,423 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:10:05,424 - INFO - joeynmt.training - 	Hypothesis: It's expands and the real-and recognize of the est.
2024-05-27 18:10:05,424 - INFO - joeynmt.training - Example #4
2024-05-27 18:10:05,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:10:05,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:10:05,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 'pre@@', 'tt@@', 'y', 'ro@@', 'o@@', 'm', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'to', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:10:05,424 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:10:05,424 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:10:05,424 - INFO - joeynmt.training - 	Hypothesis: The next next to be a pretty room will be a rapid to the last 25 years.
2024-05-27 18:10:20,391 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.070781, Batch Acc: 0.436672, Tokens per Sec:     4711, Lr: 0.000300
2024-05-27 18:10:34,345 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.914600, Batch Acc: 0.444057, Tokens per Sec:     5280, Lr: 0.000300
2024-05-27 18:10:48,775 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.010856, Batch Acc: 0.441486, Tokens per Sec:     5005, Lr: 0.000300
2024-05-27 18:11:03,711 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.978804, Batch Acc: 0.443329, Tokens per Sec:     4655, Lr: 0.000300
2024-05-27 18:11:17,691 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.031550, Batch Acc: 0.440255, Tokens per Sec:     5035, Lr: 0.000300
2024-05-27 18:11:17,692 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:11:17,692 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:12:29,039 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.57, acc:   0.42, generation: 71.3398[sec], evaluation: 0.0000[sec]
2024-05-27 18:12:29,041 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:12:29,190 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/4000.ckpt
2024-05-27 18:12:29,191 - INFO - joeynmt.training - Example #0
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'de@@', 'mon@@', 'str@@', 'ing', 'to', 'de@@', 'mon@@', 'str@@', 'ing', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'al', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'per@@', 'cent@@', ',', 'is', 're@@', 'duc@@', 'ed', 'the', '4@@', '0@@', '.', '</s>']
2024-05-27 18:12:29,191 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:12:29,191 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:12:29,191 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstring to demonstring that the calculate glacial artical artical artical 48 million years has had the size of 48 percent, is reduced the 40.
2024-05-27 18:12:29,191 - INFO - joeynmt.training - Example #1
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:12:29,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'tr@@', 'av@@', 'y', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'not', 'show', 'it', 'because', 'not', 'show', 'it', 'is', 'to', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Hypothesis: And this is the travy of the problem of the problem because not show it because not show it is to the same.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - Example #2
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'st@@', 'o@@', 'p', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'ch@@', ',', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'cer@@', 'ta@@', 'in', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al']
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Hypothesis: The stop of the climatch, in a certain a certain a certain the climate system of the global system of the global system of the global system of the global system of the global system of the climath system of the climath system of the climath system of the global system of the global system of the global
2024-05-27 18:12:29,192 - INFO - joeynmt.training - Example #3
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 'p@@', 'and@@', 's', 'and', 'in@@', 'ver@@', 'se', 'and', 're@@', 'c@@', 't@@', 's.', '</s>']
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - 	Hypothesis: You can expands and inverse and rects.
2024-05-27 18:12:29,192 - INFO - joeynmt.training - Example #4
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:12:29,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'a@@', 'si@@', 'c', 'de@@', '-@@', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 're@@', 'st', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:12:29,193 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:12:29,193 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:12:29,193 - INFO - joeynmt.training - 	Hypothesis: The next next deasic de-rapid rapid of the rest of the last 25 years.
2024-05-27 18:12:43,774 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.797159, Batch Acc: 0.451746, Tokens per Sec:     4905, Lr: 0.000300
2024-05-27 18:12:59,400 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.017384, Batch Acc: 0.454988, Tokens per Sec:     4479, Lr: 0.000300
2024-05-27 18:13:15,483 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.777512, Batch Acc: 0.455362, Tokens per Sec:     4454, Lr: 0.000300
2024-05-27 18:13:30,556 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.012458, Batch Acc: 0.452565, Tokens per Sec:     4732, Lr: 0.000300
2024-05-27 18:13:46,111 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.925775, Batch Acc: 0.453121, Tokens per Sec:     4669, Lr: 0.000300
2024-05-27 18:13:46,111 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:13:46,111 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:14:56,312 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.28, acc:   0.43, generation: 70.1930[sec], evaluation: 0.0000[sec]
2024-05-27 18:14:56,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:14:56,465 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/4500.ckpt
2024-05-27 18:14:56,466 - INFO - joeynmt.training - Example #0
2024-05-27 18:14:56,466 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:14:56,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:14:56,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'de@@', 'gre@@', 'es', 'to', 'de@@', 'mon@@', 'th@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ated', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'r@@', 'y@@', 'ar@@', 'tic@@', 'al', 'of', '4@@', '8', 'h@@', 'our@@', 's', 'of', '4@@', '8', 'per@@', 'cent@@', 's', 'of', '4@@', '8', 'per@@', 'cent@@', ',', 'is', 're@@', 'fl@@', 'ying', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:14:56,466 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Hypothesis: The year I showed these degrees to demonths to show that the calculated glacial cryartical of 48 hours of 48 percents of 48 percent, is reflying the 40 percent.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - Example #1
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', 'is', 'the', 'way', 'that', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'i@@', 'gh@@', 't.', '</s>']
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Hypothesis: And that is the way that the gravity of the problem of the problem of the ight.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - Example #2
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'alc@@', 'ul@@', 'ate', 'the', 'c@@', 'lim@@', 'b@@', 'ed', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', ',', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - 	Hypothesis: The calculate calculate calculate the climbed of the climate system, the climate system.
2024-05-27 18:14:56,467 - INFO - joeynmt.training - Example #3
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:14:56,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 're@@', 'c@@', 'y@@', 'c@@', 'le', 'and', 'they', 're@@', 'c@@', 't@@', 'ed.', '</s>']
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Hypothesis: You go and recycle and they rected.
2024-05-27 18:14:56,468 - INFO - joeynmt.training - Example #4
2024-05-27 18:14:56,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:14:56,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:14:56,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'gre@@', 'e', 'of', 'the', 'next', 'de@@', 'gre@@', 'e', 'of', 'the', 'de@@', 'gre@@', 'e', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:14:56,468 - INFO - joeynmt.training - 	Hypothesis: The next degree of the next degree of the degree of the last 25 years.
2024-05-27 18:15:12,322 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.097501, Batch Acc: 0.459082, Tokens per Sec:     4447, Lr: 0.000300
2024-05-27 18:15:27,146 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.911710, Batch Acc: 0.455815, Tokens per Sec:     4856, Lr: 0.000300
2024-05-27 18:15:42,561 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.782191, Batch Acc: 0.463690, Tokens per Sec:     4665, Lr: 0.000300
2024-05-27 18:15:56,792 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.873044, Batch Acc: 0.455305, Tokens per Sec:     5099, Lr: 0.000300
2024-05-27 18:16:11,221 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.869699, Batch Acc: 0.458105, Tokens per Sec:     4974, Lr: 0.000300
2024-05-27 18:16:11,222 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:16:11,222 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:17:14,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.05, acc:   0.44, generation: 63.7667[sec], evaluation: 0.0000[sec]
2024-05-27 18:17:14,999 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:17:15,145 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/5000.ckpt
2024-05-27 18:17:15,146 - INFO - joeynmt.training - Example #0
2024-05-27 18:17:15,146 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:17:15,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:17:15,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'de@@', 'li@@', 'li@@', 've', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'at@@', 'es', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'z@@', 'y', 'si@@', 'ze', 'that', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', ',', 'is', 're@@', 'ach@@', 'ed', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:17:15,146 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:17:15,146 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Hypothesis: The year I showed these delilive to show that the calculate that the calculates of the glacial sizy size that the size of 40 percent, is reached the size of 40 percent.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - Example #1
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E@@', 'ver@@', 'y@@', 'thing', 'about', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'i@@', 'gh@@', 'bor@@', 'h@@', '.', '</s>']
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Hypothesis: Everything about this is the gravity of the problem of the problem of the ighborh.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - Example #2
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'c@@', 'lim@@', 'ate', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'b', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial arty is in a sense, the heart of the climate system of global climate system climate the global climate climb of the climath climath of the climate climate climate the climath climath of the climate system.
2024-05-27 18:17:15,147 - INFO - joeynmt.training - Example #3
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:17:15,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 'it', 're@@', 'c@@', 'ent', 'and', 'they', 're@@', 'd.', '</s>']
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Hypothesis: It is expands and wind and it recent and they red.
2024-05-27 18:17:15,148 - INFO - joeynmt.training - Example #4
2024-05-27 18:17:15,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:17:15,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:17:15,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'si@@', 'c', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:17:15,148 - INFO - joeynmt.training - 	Hypothesis: The next deasic will be a rapid of the last 25 years.
2024-05-27 18:17:29,320 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.888216, Batch Acc: 0.456938, Tokens per Sec:     4832, Lr: 0.000300
2024-05-27 18:17:43,754 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.844328, Batch Acc: 0.466297, Tokens per Sec:     5082, Lr: 0.000300
2024-05-27 18:17:58,739 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.740163, Batch Acc: 0.469859, Tokens per Sec:     4881, Lr: 0.000300
2024-05-27 18:18:14,650 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.748355, Batch Acc: 0.467589, Tokens per Sec:     4435, Lr: 0.000300
2024-05-27 18:18:30,908 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.885960, Batch Acc: 0.466661, Tokens per Sec:     4550, Lr: 0.000300
2024-05-27 18:18:30,908 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:18:30,908 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:19:46,045 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.44, generation: 75.1295[sec], evaluation: 0.0000[sec]
2024-05-27 18:19:46,048 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:19:46,199 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/5500.ckpt
2024-05-27 18:19:46,202 - INFO - joeynmt.training - Example #0
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'of', 'the', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'at@@', 'es', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', '4@@', '8', 'million', 'years', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:19:46,202 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:19:46,202 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:19:46,202 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide of the slide to show that the calculates that for almost three million years of years of years of years of years of 48 million years of 48 States of 40 percent.
2024-05-27 18:19:46,202 - INFO - joeynmt.training - Example #1
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:19:46,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'c@@', 'tu@@', 'ally,', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'is@@', 'su@@', 'e', 'of', 'the', 'g@@', 'a@@', 'in', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:19:46,202 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Hypothesis: Actually, this is the gravity of the problem because of the problem of the issue of the gain the same.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - Example #2
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', ',', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'b@@', 'or', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial climb, in a sense, the cubor climate system.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - Example #3
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'in@@', 'ver@@', 's', 'and', 'w@@', 'ron@@', 'g', 'w@@', 'ron@@', 'g@@', '.', '</s>']
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - 	Hypothesis: It expands and invers and wrong wrong.
2024-05-27 18:19:46,203 - INFO - joeynmt.training - Example #4
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:19:46,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'al', 'will', 'be', 'a', 's@@', 'li@@', 'de', 'to', 'be', 'a', 'f@@', 'lo@@', 'or', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:19:46,204 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:19:46,204 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:19:46,204 - INFO - joeynmt.training - 	Hypothesis: The next deal will be a slide to be a floor of the last 25 years.
2024-05-27 18:20:01,170 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.868819, Batch Acc: 0.468790, Tokens per Sec:     4617, Lr: 0.000300
2024-05-27 18:20:16,393 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.731079, Batch Acc: 0.467330, Tokens per Sec:     4637, Lr: 0.000300
2024-05-27 18:20:31,594 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.856662, Batch Acc: 0.472753, Tokens per Sec:     4718, Lr: 0.000300
2024-05-27 18:20:47,515 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.795611, Batch Acc: 0.475889, Tokens per Sec:     4390, Lr: 0.000300
2024-05-27 18:21:02,064 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     2.156443, Batch Acc: 0.470542, Tokens per Sec:     4860, Lr: 0.000300
2024-05-27 18:21:02,065 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:21:02,065 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:22:14,454 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.73, acc:   0.45, generation: 72.3822[sec], evaluation: 0.0000[sec]
2024-05-27 18:22:14,457 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:22:14,604 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/6000.ckpt
2024-05-27 18:22:14,605 - INFO - joeynmt.training - Example #0
2024-05-27 18:22:14,605 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:22:14,605 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:22:14,605 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'positi@@', 've', 'positi@@', 've', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'al', 'si@@', 'ze', '4@@', '8', 'million', 'years', 'has', 'had', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Hypothesis: The year I showed these positive positive to show that the calculate that the calculate glacial artical size 48 million years has had the 48 percent of the 48 percent of the 48 percent.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - Example #1
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'y', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'the', 'l@@', 'ac@@', 'i@@', 'a.', '</s>']
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Hypothesis: This is the gravy of the gravity of the problem because it doesn't show it the lacia.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - Example #2
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'b@@', 'or', 'sen@@', 'se,', 'the', 'cu@@', 'b@@', 'or', 'cu@@', 'cu@@', 'b@@', 'er', 'system@@', '.', '</s>']
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - 	Hypothesis: The calculate arty is in a sense, the cubor sense, the cubor cucuber system.
2024-05-27 18:22:14,606 - INFO - joeynmt.training - Example #3
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:22:14,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:22:14,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'n@@', 's', 'out', 'and', 'the', 'in@@', 'ver@@', 'al', 'and', 'they', 'get', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Hypothesis: It turns out and the inveral and they get the expand.
2024-05-27 18:22:14,607 - INFO - joeynmt.training - Example #4
2024-05-27 18:22:14,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:22:14,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:22:14,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'si@@', 'c', 'pro@@', 'mi@@', 'se', 'is', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 're@@', 'st', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:22:14,607 - INFO - joeynmt.training - 	Hypothesis: The next deasic promise is a rapid of the rest 25 years.
2024-05-27 18:22:29,103 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.825309, Batch Acc: 0.467640, Tokens per Sec:     4777, Lr: 0.000300
2024-05-27 18:22:43,597 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.895326, Batch Acc: 0.472196, Tokens per Sec:     4996, Lr: 0.000300
2024-05-27 18:22:58,673 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.854459, Batch Acc: 0.475111, Tokens per Sec:     4779, Lr: 0.000300
2024-05-27 18:23:06,177 - INFO - joeynmt.training - Epoch   2: total training loss 8644.38
2024-05-27 18:23:06,177 - INFO - joeynmt.training - EPOCH 3
2024-05-27 18:23:13,921 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.735818, Batch Acc: 0.488486, Tokens per Sec:     4526, Lr: 0.000300
2024-05-27 18:23:29,628 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     2.063469, Batch Acc: 0.496748, Tokens per Sec:     4512, Lr: 0.000300
2024-05-27 18:23:29,628 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:23:29,628 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:24:42,374 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.54, acc:   0.46, generation: 72.7384[sec], evaluation: 0.0000[sec]
2024-05-27 18:24:42,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:24:42,526 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/6500.ckpt
2024-05-27 18:24:42,527 - INFO - joeynmt.training - Example #0
2024-05-27 18:24:42,527 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:24:42,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:24:42,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'at@@', 'es', 'of', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'ag@@', 'o', 'has', 'been', 're@@', 'ach@@', 'ed', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the calculate that the calculates of the size of 48 million years ago has been reached the size of 48 percent.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - Example #1
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'll', 'this', 'un@@', 'der', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'not', 'show', 'it', 'is', 'not', 'show', 'it', 'to', 'the', 'g@@', 'l@@', 'ac@@', 'e.', '</s>']
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Hypothesis: All this under of the gravity of the problem because not show it is not show it to the glace.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - Example #2
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'lim@@', 'b', 'is', 'the', 'ar@@', 't@@', 'y', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - 	Hypothesis: The climb is the arty arty is in a certain a sense, the climate climate system.
2024-05-27 18:24:42,528 - INFO - joeynmt.training - Example #3
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:24:42,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 's', 'and', 'the', 'in@@', 'ver@@', 'al', 'and', 'they', 're@@', 'd.', '</s>']
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Hypothesis: It's expands and the inveral and they red.
2024-05-27 18:24:42,529 - INFO - joeynmt.training - Example #4
2024-05-27 18:24:42,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:24:42,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:24:42,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'li@@', 'li@@', 'es', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 're@@', 'al@@', '-@@', 'year@@', '-@@', 'ol@@', 'd.', '</s>']
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:24:42,529 - INFO - joeynmt.training - 	Hypothesis: The next delilies will be a rapid rapid of the real-year-old.
2024-05-27 18:24:58,203 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.769502, Batch Acc: 0.496478, Tokens per Sec:     4522, Lr: 0.000300
2024-05-27 18:25:16,328 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.736715, Batch Acc: 0.492355, Tokens per Sec:     4020, Lr: 0.000300
2024-05-27 18:25:33,384 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.598041, Batch Acc: 0.491870, Tokens per Sec:     4230, Lr: 0.000300
2024-05-27 18:25:50,561 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.760589, Batch Acc: 0.488884, Tokens per Sec:     4085, Lr: 0.000300
2024-05-27 18:26:08,437 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.637176, Batch Acc: 0.490922, Tokens per Sec:     3944, Lr: 0.000300
2024-05-27 18:26:08,437 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:26:08,438 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:27:30,471 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.52, acc:   0.46, generation: 82.0263[sec], evaluation: 0.0000[sec]
2024-05-27 18:27:30,474 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:27:30,623 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/7000.ckpt
2024-05-27 18:27:30,624 - INFO - joeynmt.training - Example #0
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 'positi@@', 've', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'he@@', 'at@@', 'h', 't@@', 'ot@@', 'ally', 'three', 'million', 'years', 'of', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'mi@@', 'st@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Hypothesis: The year year, I showed these positive demonstrate that the heath totally three million years of years has had the size of 48 million years has had the size of 48 mistates of 40 percent.
2024-05-27 18:27:30,625 - INFO - joeynmt.training - Example #1
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:27:30,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'un@@', 'der', 'of', 'this', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'prob@@', 'le@@', 'm', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:27:30,625 - INFO - joeynmt.training - 	Hypothesis: And this under of this under the gravity of the problem of the problem of the ice.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - Example #2
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'r@@', 'un', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'sen@@', 'se,', 'is', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'lim@@', 'b', 'sy@@', 'ste@@', 'm', 'c@@', 'li@@', 'mat@@', 'ical', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'lim@@', 'b', 'is', 'a', 'glob@@', 'al', 'sen@@', 'se,', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'rit@@', 'ical', 'c@@', 'alc@@', 'ul@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Hypothesis: The run lacial arty is in a sense, is a sense, the heart of the climatical climatical climb system climatical system of the climatical climatical climatical climatical climb is a global sense, is in a sense, the climatical climate the critical calculate system.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - Example #3
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 'ex@@', 'p@@', 'and@@', 's', 'and', 'in@@', 'ver@@', 's', 'and', 'they', 're@@', 'd.', '</s>']
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Hypothesis: It was expands and invers and they red.
2024-05-27 18:27:30,626 - INFO - joeynmt.training - Example #4
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:27:30,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'li@@', 'es', 'will', 'be', 'a', 're@@', 'e@@', 'f@@', 'y', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:27:30,626 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:27:30,627 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:27:30,627 - INFO - joeynmt.training - 	Hypothesis: The next dealies will be a reefy of the last 25 years.
2024-05-27 18:27:48,380 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.764791, Batch Acc: 0.497023, Tokens per Sec:     3995, Lr: 0.000300
2024-05-27 18:28:05,691 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.796944, Batch Acc: 0.494515, Tokens per Sec:     4113, Lr: 0.000300
2024-05-27 18:28:23,580 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.623255, Batch Acc: 0.495612, Tokens per Sec:     4090, Lr: 0.000300
2024-05-27 18:28:41,706 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.826070, Batch Acc: 0.494174, Tokens per Sec:     3935, Lr: 0.000300
2024-05-27 18:29:00,151 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.702166, Batch Acc: 0.490623, Tokens per Sec:     3928, Lr: 0.000300
2024-05-27 18:29:00,152 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:29:00,152 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:30:18,709 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.39, acc:   0.47, generation: 78.5506[sec], evaluation: 0.0000[sec]
2024-05-27 18:30:18,712 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:30:18,858 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/7500.ckpt
2024-05-27 18:30:18,859 - INFO - joeynmt.training - Example #0
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'at@@', 'es', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to demonstrate that the calculates that for almost three million years has had the three million years has had the size of 48 percent.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - Example #1
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 's@@', 'our@@', 'ce', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'gh@@', 't.', '</s>']
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - 	Hypothesis: This is a source of the problem because it doesn't show it because it doesn't show it of the glacight.
2024-05-27 18:30:18,859 - INFO - joeynmt.training - Example #2
2024-05-27 18:30:18,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'lim@@', 'b@@', 'ed', 'ar@@', 't@@', 'y', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Hypothesis: The climbed arty is in a sense, the heart of the climate climate system of the climate system of the climate system of the climate system of the climate system of the climate climate system.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - Example #3
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'in', 'the', 'w@@', 'in@@', 'ver@@', 'al', 'and', 'they', 're@@', 'tur@@', 'n', 'out', 'of', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Hypothesis: You can get in the winveral and they return out of the expand.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - Example #4
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:30:18,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'si@@', 'c', 'de@@', 'a@@', 'si@@', 'c', 'is', 'going', 'to', 'be', 'a', 're@@', 'co@@', 'ver@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:30:18,860 - INFO - joeynmt.training - 	Hypothesis: The next deasic deasic is going to be a recovered on the last 25 years.
2024-05-27 18:30:33,749 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.723357, Batch Acc: 0.499895, Tokens per Sec:     4763, Lr: 0.000300
2024-05-27 18:30:49,338 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.665962, Batch Acc: 0.491209, Tokens per Sec:     4543, Lr: 0.000300
2024-05-27 18:31:05,148 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.657942, Batch Acc: 0.498570, Tokens per Sec:     4513, Lr: 0.000300
2024-05-27 18:31:19,708 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.844883, Batch Acc: 0.501463, Tokens per Sec:     4859, Lr: 0.000300
2024-05-27 18:31:34,679 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.988148, Batch Acc: 0.496264, Tokens per Sec:     4765, Lr: 0.000300
2024-05-27 18:31:34,679 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:31:34,680 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:32:46,919 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.29, acc:   0.47, generation: 72.2318[sec], evaluation: 0.0000[sec]
2024-05-27 18:32:46,922 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:32:47,068 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/8000.ckpt
2024-05-27 18:32:47,069 - INFO - joeynmt.training - Example #0
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ations', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the calculate that the calculate glacial calculations has had the size of 48 percent of the 48 percent of the 40 percent.
2024-05-27 18:32:47,069 - INFO - joeynmt.training - Example #1
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:32:47,069 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'y', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ice', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'sh@@', '.', '</s>']
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:32:47,069 - INFO - joeynmt.training - 	Hypothesis: This is the gravy of the gravity of the problem because it doesn't show it of the ice of the glacish.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - Example #2
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'alc@@', 'ul@@', 'ate', 'ar@@', 't', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'cer@@', 'ta@@', 'in', 'a', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Hypothesis: The calculate calculate art is in a certain a certain a climate climate system.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - Example #3
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 's', 'and', 'in@@', 'ver@@', 'y,', 'and', 'they', 'get', 'back', 'to', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Hypothesis: It's expands and invery, and they get back to the expand.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - Example #4
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:32:47,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'a@@', 'th@@', 'y', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'car@@', 'e@@', 'fu@@', 'll@@', 'y', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:32:47,070 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:32:47,071 - INFO - joeynmt.training - 	Hypothesis: The next next deathy will be a rapid of carefully the last 25 years.
2024-05-27 18:33:01,209 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.755115, Batch Acc: 0.497506, Tokens per Sec:     4954, Lr: 0.000300
2024-05-27 18:33:15,969 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.829677, Batch Acc: 0.495396, Tokens per Sec:     4782, Lr: 0.000300
2024-05-27 18:33:30,918 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.733089, Batch Acc: 0.505019, Tokens per Sec:     4918, Lr: 0.000300
2024-05-27 18:33:46,782 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.667138, Batch Acc: 0.504615, Tokens per Sec:     4610, Lr: 0.000300
2024-05-27 18:34:02,287 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.767327, Batch Acc: 0.505898, Tokens per Sec:     4636, Lr: 0.000300
2024-05-27 18:34:02,287 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:34:02,287 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:35:17,543 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.48, generation: 75.2490[sec], evaluation: 0.0000[sec]
2024-05-27 18:35:17,546 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:35:17,692 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/8500.ckpt
2024-05-27 18:35:17,693 - INFO - joeynmt.training - Example #0
2024-05-27 18:35:17,693 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:35:17,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:35:17,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ations', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'at@@', 'es', 'of', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Hypothesis: And the year I showed these slides to show that the calculations that the calculates of the size of the 48 million years has had the sizes of 48 States of 48 percent.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - Example #1
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'to', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Hypothesis: This is the gravity of the gravity of the problem because it doesn't show it to the same.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - Example #2
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial climb is in a sense, the climate climate the climate climate system.
2024-05-27 18:35:17,694 - INFO - joeynmt.training - Example #3
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:35:17,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'the', 'w@@', 'in@@', 'd', 'of', 'the', 'w@@', 'in@@', 'd@@', 's.', '</s>']
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Hypothesis: You can expands and wind and wind and wind the wind of the winds.
2024-05-27 18:35:17,695 - INFO - joeynmt.training - Example #4
2024-05-27 18:35:17,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:35:17,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:35:17,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'e,', 'it', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'the', 're@@', 'st', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:35:17,695 - INFO - joeynmt.training - 	Hypothesis: The next next device, it will be a rapid rapid the rest of the last 25 years.
2024-05-27 18:35:33,465 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.626312, Batch Acc: 0.502231, Tokens per Sec:     4533, Lr: 0.000300
2024-05-27 18:35:49,265 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.744396, Batch Acc: 0.502931, Tokens per Sec:     4611, Lr: 0.000300
2024-05-27 18:36:04,744 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.882193, Batch Acc: 0.497500, Tokens per Sec:     4587, Lr: 0.000300
2024-05-27 18:36:20,108 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.478723, Batch Acc: 0.505540, Tokens per Sec:     4470, Lr: 0.000300
2024-05-27 18:36:35,152 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.567564, Batch Acc: 0.502170, Tokens per Sec:     4794, Lr: 0.000300
2024-05-27 18:36:35,152 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:36:35,152 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:37:49,528 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.11, acc:   0.48, generation: 74.3684[sec], evaluation: 0.0000[sec]
2024-05-27 18:37:49,530 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:37:49,678 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/9000.ckpt
2024-05-27 18:37:49,679 - INFO - joeynmt.training - Example #0
2024-05-27 18:37:49,679 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 's', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 's', 'of', 'the', 'ar@@', 'tic@@', 'al', 'si@@', 'z@@', 'e,', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to show that the calots to show that the calots of the artical size, which is almost three million years had had the sizes of 40 percent of the 40 percent.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - Example #1
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'll', 'this', 'un@@', 'der', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'in', 'the', 'ice', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'sh@@', 'ing', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'sh@@', 'ing', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'sh@@', 'ing', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'is@@', 'su@@', 'e.', '</s>']
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Hypothesis: All this under of the gravity of the problem because it doesn't show it in the ice of the glacishing of the glacishing the glacishing the gravity of the issue.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - Example #2
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:37:49,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'm@@', 'att@@', 'er', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'r@@', 'y,', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'b@@', 's', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - 	Hypothesis: The matter glacial calry, in a sense, the climbs of the climate system.
2024-05-27 18:37:49,680 - INFO - joeynmt.training - Example #3
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'know,', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'the', 'w@@', 'est@@', 'er@@', 'n', 'the', 'w@@', 'est@@', '.', '</s>']
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Hypothesis: You know, the wind and wind and the western the west.
2024-05-27 18:37:49,681 - INFO - joeynmt.training - Example #4
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:37:49,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'e,', 'the', 'next', 'de@@', 'gre@@', 'es', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:37:49,681 - INFO - joeynmt.training - 	Hypothesis: The next next device, the next degrees will be a rapid of the last 25 years.
2024-05-27 18:38:06,074 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.561016, Batch Acc: 0.504470, Tokens per Sec:     4409, Lr: 0.000300
2024-05-27 18:38:21,219 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.620576, Batch Acc: 0.504342, Tokens per Sec:     4699, Lr: 0.000300
2024-05-27 18:38:37,662 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.578726, Batch Acc: 0.509027, Tokens per Sec:     4312, Lr: 0.000300
2024-05-27 18:38:53,259 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.634007, Batch Acc: 0.505034, Tokens per Sec:     4611, Lr: 0.000300
2024-05-27 18:39:09,688 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.630391, Batch Acc: 0.507373, Tokens per Sec:     4405, Lr: 0.000300
2024-05-27 18:39:09,688 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:39:09,688 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:40:22,085 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.07, acc:   0.48, generation: 72.3900[sec], evaluation: 0.0000[sec]
2024-05-27 18:40:22,087 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:40:22,241 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/9500.ckpt
2024-05-27 18:40:22,242 - INFO - joeynmt.training - Example #0
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'of', 'the', 'next', 'si@@', 'de', 'of', 'the', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide of the next side of the artical artical artica, which for almost three million years has had the size of the 48 States of the 48 percent of the 40 percent.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - Example #1
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:40:22,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'sub@@', 'ur@@', 'b@@', 's', 'the', 's@@', 'ou@@', 'ch@@', ',', 'because', 'it', "doesn't", 'show', 'it', 'it', 'is', 'the', 's@@', 'am@@', 'pl@@', 'e.', '</s>']
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - 	Hypothesis: And this is the suburbs the souch, because it doesn't show it it is the sample.
2024-05-27 18:40:22,242 - INFO - joeynmt.training - Example #2
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'lim@@', 'b@@', 'ed', 'c@@', 'r@@', 'y@@', 'p@@', 'e', 'is', 'in', 'a', 'sen@@', 'se,', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'b@@', 'ing', 'sy@@', 'ste@@', 'm', 'c@@', 'li@@', 'mat@@', 'ic@@', '.', '</s>']
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Hypothesis: The climbed crype is in a sense, is in a sense, the climbing system climatic.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - Example #3
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 're@@', 'tur@@', 'n', 'back', 'to', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Hypothesis: And the wind and wind and return back to the expand.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - Example #4
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:40:22,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '2@@', '5', 'year@@', '-@@', 'old', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'car@@', 'e@@', 'ful', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:40:22,243 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:40:22,244 - INFO - joeynmt.training - 	Hypothesis: The next next 25 year-old rapid rapid careful carlled on the last 25 years.
2024-05-27 18:40:36,488 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.805745, Batch Acc: 0.505119, Tokens per Sec:     4863, Lr: 0.000300
2024-05-27 18:40:52,069 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.597450, Batch Acc: 0.512379, Tokens per Sec:     4493, Lr: 0.000300
2024-05-27 18:41:07,217 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.822058, Batch Acc: 0.507753, Tokens per Sec:     4718, Lr: 0.000300
2024-05-27 18:41:24,423 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.811147, Batch Acc: 0.511714, Tokens per Sec:     4148, Lr: 0.000300
2024-05-27 18:41:41,219 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.801326, Batch Acc: 0.511992, Tokens per Sec:     4183, Lr: 0.000300
2024-05-27 18:41:41,219 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:41:41,219 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:42:53,281 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.96, acc:   0.48, generation: 72.0546[sec], evaluation: 0.0000[sec]
2024-05-27 18:42:53,282 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:42:53,432 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/10000.ckpt
2024-05-27 18:42:53,433 - INFO - joeynmt.training - Example #0
2024-05-27 18:42:53,433 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:42:53,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:42:53,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:42:53,433 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:42:53,433 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:42:53,433 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the calculate glacial calculus, which for almost three million years had the size of 48 States of 48 States of 48 States of 40 percent.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - Example #1
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'c@@', 'le@@', 'as@@', 'h', 'this', 'un@@', 'der', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Hypothesis: Acleash this under gravity because it doesn't show the gravity because it doesn't show the same.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - Example #2
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', ',', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'lim@@', 'b@@', 's.', '</s>']
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial climb, in a certain sense, the climath climath climath climath climath climath climath climath climath climath climate the climbs.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - Example #3
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:42:53,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', ',', 'and', 'they', 're@@', 'tur@@', 'n', 'out', 'of', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - 	Hypothesis: You expands of inver, and they return out of the expand.
2024-05-27 18:42:53,434 - INFO - joeynmt.training - Example #4
2024-05-27 18:42:53,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:42:53,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:42:53,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'the', 'next', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:42:53,435 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:42:53,435 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:42:53,435 - INFO - joeynmt.training - 	Hypothesis: The next next to the next 25 years.
2024-05-27 18:43:10,008 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.914038, Batch Acc: 0.509084, Tokens per Sec:     4294, Lr: 0.000300
2024-05-27 18:43:25,723 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.715808, Batch Acc: 0.514296, Tokens per Sec:     4460, Lr: 0.000300
2024-05-27 18:43:41,226 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.577394, Batch Acc: 0.508007, Tokens per Sec:     4693, Lr: 0.000300
2024-05-27 18:43:56,218 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.648818, Batch Acc: 0.515580, Tokens per Sec:     4748, Lr: 0.000300
2024-05-27 18:44:11,977 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.606685, Batch Acc: 0.511830, Tokens per Sec:     4519, Lr: 0.000300
2024-05-27 18:44:11,978 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:44:11,978 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:45:20,635 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.91, acc:   0.49, generation: 68.6505[sec], evaluation: 0.0000[sec]
2024-05-27 18:45:20,636 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:45:20,781 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/10500.ckpt
2024-05-27 18:45:20,782 - INFO - joeynmt.training - Example #0
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'dimen@@', 'sion@@', ',', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '0@@', '-@@', 'h@@', 'our@@', 's', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:45:20,782 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:45:20,782 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:45:20,782 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to demonstrate that the glacial glacial dimension, that for almost three million years has had the size of 40 percent of the 40-hours of 40 percent.
2024-05-27 18:45:20,782 - INFO - joeynmt.training - Example #1
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:45:20,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 're@@', 'mark@@', 'able', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'not', 'show', 'the', 'ex@@', 'ce@@', 'p@@', 't', 'of', 'the', 'ice', 'because', 'not', 'show@@', 's', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'of', 'the', 'ice', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 've', 'gr@@', 'av@@', 'ity', 'of', 'the', 'is@@', 'su@@', 'e', 'of', 'the', 'is@@', 'su@@', 'e', 'of', 'the', 'is@@', 'su@@', 'e', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'not', 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Hypothesis: And this is a remarkable of the problem because not show the except of the ice because not shows the ice of the ice ice of the ice ice of the ice ice ice of the glacive gravity of the issue of the issue of the issue of the problem because not show the same.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - Example #2
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Hypothesis: The glacial calculate is in a sense, the customer of the global climate system.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - Example #3
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'y,', 'and', 're@@', 'tur@@', 'n', 'out', 'of', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - 	Hypothesis: You expands of invery, and return out of the expand.
2024-05-27 18:45:20,783 - INFO - joeynmt.training - Example #4
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:45:20,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:45:20,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'the', 'next', '2@@', '5', 'year@@', 's,', 'it', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 're@@', 's@@', 'ent', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:45:20,784 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:45:20,784 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:45:20,784 - INFO - joeynmt.training - 	Hypothesis: The next next to the next 25 years, it will be a rapid resent on the last 25 years.
2024-05-27 18:45:36,283 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.635592, Batch Acc: 0.510430, Tokens per Sec:     4721, Lr: 0.000300
2024-05-27 18:45:51,511 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.628043, Batch Acc: 0.515208, Tokens per Sec:     4722, Lr: 0.000300
2024-05-27 18:46:03,686 - INFO - joeynmt.training - Epoch   3: total training loss 7622.57
2024-05-27 18:46:03,686 - INFO - joeynmt.training - EPOCH 4
2024-05-27 18:46:07,114 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.464968, Batch Acc: 0.540877, Tokens per Sec:     4739, Lr: 0.000300
2024-05-27 18:46:22,448 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.598958, Batch Acc: 0.528094, Tokens per Sec:     4674, Lr: 0.000300
2024-05-27 18:46:37,396 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.615834, Batch Acc: 0.529731, Tokens per Sec:     4882, Lr: 0.000300
2024-05-27 18:46:37,396 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:46:37,396 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:47:53,627 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.80, acc:   0.49, generation: 76.2239[sec], evaluation: 0.0000[sec]
2024-05-27 18:47:53,630 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:47:53,776 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/11000.ckpt
2024-05-27 18:47:53,777 - INFO - joeynmt.training - Example #0
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'positi@@', 've', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'h@@', 'o@@', 't', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'of', 'the', 'ar@@', 'tic@@', 'al', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '0@@', '.', '</s>']
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Hypothesis: The year I showed these positive demonstrate that the hot calculate that the type of the artical 48 million years has had the size of 48 States of the 40.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - Example #1
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'under@@', 'ne@@', 'at@@', 'h', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'not', 'show', 'the', 'be@@', 'au@@', 'ti@@', 'ful', 'sp@@', 'ex@@', '.', '</s>']
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - 	Hypothesis: And this underneath the gravity of the problem because it doesn't show it not show the beautiful spex.
2024-05-27 18:47:53,777 - INFO - joeynmt.training - Example #2
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:47:53,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is, in a sense, the heart climate the climate climate system.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - Example #3
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', ',', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Hypothesis: You expands of inver, and you get right.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - Example #4
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:47:53,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'to', 'be', 'a', 're@@', 'mark@@', 'able', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'fu@@', 'el@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:47:53,778 - INFO - joeynmt.training - 	Hypothesis: The next next to be a remarkable careful carefueled on the last 25 years.
2024-05-27 18:48:09,367 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.639053, Batch Acc: 0.533780, Tokens per Sec:     4544, Lr: 0.000300
2024-05-27 18:48:25,456 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.521369, Batch Acc: 0.530435, Tokens per Sec:     4525, Lr: 0.000300
2024-05-27 18:48:40,313 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.588175, Batch Acc: 0.526129, Tokens per Sec:     4843, Lr: 0.000300
2024-05-27 18:48:55,625 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.906016, Batch Acc: 0.534560, Tokens per Sec:     4641, Lr: 0.000300
2024-05-27 18:49:10,783 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.597883, Batch Acc: 0.530296, Tokens per Sec:     4650, Lr: 0.000300
2024-05-27 18:49:10,784 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:49:10,784 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:50:17,083 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.81, acc:   0.50, generation: 66.2926[sec], evaluation: 0.0000[sec]
2024-05-27 18:50:17,229 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/11500.ckpt
2024-05-27 18:50:17,230 - INFO - joeynmt.training - Example #0
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'positi@@', 've', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'est', 't@@', 'y@@', 'p@@', 'e', 'that', 'the', 't@@', 'est', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'v@@', 'es,', 'which', 'is', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0@@', ',', 'is', 're@@', 'stre@@', 't@@', 't@@', 'ed.', '</s>']
2024-05-27 18:50:17,230 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:50:17,230 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:50:17,230 - INFO - joeynmt.training - 	Hypothesis: The year I showed these positive demonstrate that the test type that the test glacial calves, which is almost three million years of the 48 States of 40, is restretted.
2024-05-27 18:50:17,230 - INFO - joeynmt.training - Example #1
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:50:17,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'come', 'of', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'ice', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'sh@@', '.', '</s>']
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Hypothesis: And this is the gravity of the problem because it doesn't show the become of the ice of the ice ice ice of the ice ice ice of the ice ice ice ice of the glacish.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - Example #2
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 't@@', 'y@@', 'pic@@', 'al', 'c@@', 'lim@@', 'b@@', 'ed', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'or', 'c@@', 'le@@', 'an', 'ad@@', 's', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Hypothesis: The art typical climbed is, in a sense, the cuor clean ads of the global system of the global system climate system.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - Example #3
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:50:17,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', ',', 'and', 'I', 're@@', 'tur@@', 'ned', 'out', 'of', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - 	Hypothesis: It expands of inver, and I returned out of expand.
2024-05-27 18:50:17,231 - INFO - joeynmt.training - Example #4
2024-05-27 18:50:17,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:50:17,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:50:17,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'next', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'e@@', 'er', 'will', 'be', 'a', 'car@@', 'ri@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:50:17,232 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:50:17,232 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:50:17,232 - INFO - joeynmt.training - 	Hypothesis: The next next next to be a rapid career will be a carrilled on the last 25 years.
2024-05-27 18:50:32,554 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.737148, Batch Acc: 0.533370, Tokens per Sec:     4614, Lr: 0.000300
2024-05-27 18:50:48,774 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.406280, Batch Acc: 0.527524, Tokens per Sec:     4384, Lr: 0.000300
2024-05-27 18:51:04,982 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.580676, Batch Acc: 0.523369, Tokens per Sec:     4448, Lr: 0.000300
2024-05-27 18:51:19,831 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.620128, Batch Acc: 0.525816, Tokens per Sec:     4753, Lr: 0.000300
2024-05-27 18:51:35,486 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.488878, Batch Acc: 0.529298, Tokens per Sec:     4638, Lr: 0.000300
2024-05-27 18:51:35,487 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:51:35,487 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:52:50,045 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.50, generation: 74.5510[sec], evaluation: 0.0000[sec]
2024-05-27 18:52:50,046 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:52:50,199 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/12000.ckpt
2024-05-27 18:52:50,200 - INFO - joeynmt.training - Example #0
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'was', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0@@', '-@@', 'St@@', 'at@@', 'es', 'of', '4@@', '0@@', '.', '</s>']
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides for demonstrate that the glacial calculation that for almost three million years of years was in the United States of 48 States of 40-States of 40.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - Example #1
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:52:50,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'ere', 'is', 'this', 'un@@', 'der', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'ice', 'the', 'g@@', 'l@@', 'ac@@', 'e.', '</s>']
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - 	Hypothesis: Here is this under gravity of the problem because it doesn't show it of the ice of ice ice ice ice ice the glace.
2024-05-27 18:52:50,200 - INFO - joeynmt.training - Example #2
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Hypothesis: The glacial climb is, in a sense, the heart of global climate the climate climate system.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - Example #3
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'the', 'w@@', 'est@@', '.', '</s>']
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Hypothesis: You expands and wind and wind and wind the west.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - Example #4
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:52:50,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'e,', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'co@@', 'ol@@', 'ed', 'car@@', 'e@@', 'ful', 're@@', 'mark@@', 'able', 'so@@', 'un@@', 'd', 'came', 'up', 'with', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:52:50,201 - INFO - joeynmt.training - 	Hypothesis: The next next device, will be a quickly cooled careful remarkable sound came up with the last 25 years.
2024-05-27 18:53:06,469 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.595735, Batch Acc: 0.526626, Tokens per Sec:     4305, Lr: 0.000300
2024-05-27 18:53:22,147 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.759014, Batch Acc: 0.530089, Tokens per Sec:     4627, Lr: 0.000300
2024-05-27 18:53:37,528 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.720622, Batch Acc: 0.532494, Tokens per Sec:     4604, Lr: 0.000300
2024-05-27 18:53:52,977 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.505073, Batch Acc: 0.531066, Tokens per Sec:     4601, Lr: 0.000300
2024-05-27 18:54:08,941 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.689054, Batch Acc: 0.528693, Tokens per Sec:     4502, Lr: 0.000300
2024-05-27 18:54:08,941 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:54:08,941 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:55:28,608 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.50, generation: 79.6588[sec], evaluation: 0.0000[sec]
2024-05-27 18:55:28,610 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:55:28,761 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/12500.ckpt
2024-05-27 18:55:28,762 - INFO - joeynmt.training - Example #0
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 's@@', 'li@@', 'de', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'm', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:55:28,762 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:55:28,762 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:55:28,762 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to show that the slide that the glacial calm that for almost three million years had the size of 40 million years of 40 percent of the United States of 40 percent.
2024-05-27 18:55:28,762 - INFO - joeynmt.training - Example #1
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:55:28,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'as', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'um@@', '.', '</s>']
2024-05-27 18:55:28,762 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Hypothesis: This is the gravity of the gravity of the problem because it doesn't show it as the glacium.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - Example #2
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', ',', 'the', 'ar@@', 't', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'or', 'c@@', 'le@@', 'an', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Hypothesis: The art glacial climb, the art is in a sense, the cuor clean global climate system.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - Example #3
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'se', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Hypothesis: You expands of inverse and wind and wind the same.
2024-05-27 18:55:28,763 - INFO - joeynmt.training - Example #4
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:55:28,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'b@@', 'or', 'is', 'going', 'to', 'be', 'a', 'f@@', 'lo@@', 'or@@', ',', 'the', 'next', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:55:28,763 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:55:28,764 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:55:28,764 - INFO - joeynmt.training - 	Hypothesis: The next debor is going to be a floor, the next 25 years.
2024-05-27 18:55:44,702 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.723669, Batch Acc: 0.532125, Tokens per Sec:     4561, Lr: 0.000300
2024-05-27 18:56:00,757 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.566588, Batch Acc: 0.528535, Tokens per Sec:     4376, Lr: 0.000300
2024-05-27 18:56:15,508 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.636320, Batch Acc: 0.537449, Tokens per Sec:     4935, Lr: 0.000300
2024-05-27 18:56:30,751 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.664538, Batch Acc: 0.528849, Tokens per Sec:     4584, Lr: 0.000300
2024-05-27 18:56:45,908 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.542474, Batch Acc: 0.528419, Tokens per Sec:     4670, Lr: 0.000300
2024-05-27 18:56:45,908 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:56:45,908 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 18:57:59,467 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.63, acc:   0.50, generation: 73.5514[sec], evaluation: 0.0000[sec]
2024-05-27 18:57:59,470 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 18:57:59,618 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/13000.ckpt
2024-05-27 18:57:59,619 - INFO - joeynmt.training - Example #0
2024-05-27 18:57:59,619 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 18:57:59,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 18:57:59,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'of', 'the', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'for', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'been', 're@@', 'stre@@', 't@@', 't@@', 't@@', 'en', 'from', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 18:57:59,619 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 18:57:59,619 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 18:57:59,619 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide of the slide to demonstrate that the calculus for three million years has had the size of 48 million years has been restrettten from 40 percent.
2024-05-27 18:57:59,619 - INFO - joeynmt.training - Example #1
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 't', 'this', 'under@@', 'l@@', 'ying', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'not', 'show', 'the', 's@@', 'k@@', 'y', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'as', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'it', 'in', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'it', 'it', 'is', 'not', 'show', 'it', 'that', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Hypothesis: At this underlying the gravity of the problem because it doesn't show it not show the sky of the ice of ice ice ice as ice of the ice of ice ice as the ice of ice problem because it doesn't show it it in the ice of the ice of the ice of the ice of ice because it doesn't show it it is not show it that the ice of the ice ice ice of the ice.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - Example #2
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ic', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'of', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'e.', '</s>']
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial artic is in a certain sense, the heart of global climath climath of global climath of global climath of global climath climath of global climath of global climate.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - Example #3
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 18:57:59,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'of', 'the', 'w@@', 'in@@', 'd@@', 's.', '</s>']
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 18:57:59,620 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 18:57:59,621 - INFO - joeynmt.training - 	Hypothesis: You get the wind and wind and returns out of the winds.
2024-05-27 18:57:59,621 - INFO - joeynmt.training - Example #4
2024-05-27 18:57:59,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 18:57:59,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 18:57:59,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'li@@', 'li@@', 'ght', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 18:57:59,621 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 18:57:59,621 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 18:57:59,621 - INFO - joeynmt.training - 	Hypothesis: The next next delilight will be a rapid carlled on the last 25 years.
2024-05-27 18:58:15,059 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.704394, Batch Acc: 0.529998, Tokens per Sec:     4695, Lr: 0.000300
2024-05-27 18:58:31,070 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.642907, Batch Acc: 0.533025, Tokens per Sec:     4603, Lr: 0.000300
2024-05-27 18:58:45,631 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.529251, Batch Acc: 0.534827, Tokens per Sec:     4943, Lr: 0.000300
2024-05-27 18:59:00,918 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.486916, Batch Acc: 0.534732, Tokens per Sec:     4748, Lr: 0.000300
2024-05-27 18:59:16,149 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.599290, Batch Acc: 0.532974, Tokens per Sec:     4771, Lr: 0.000300
2024-05-27 18:59:16,150 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 18:59:16,150 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:00:26,235 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.55, acc:   0.50, generation: 70.0780[sec], evaluation: 0.0000[sec]
2024-05-27 19:00:26,237 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:00:26,387 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/14000.ckpt
2024-05-27 19:00:26,388 - INFO - joeynmt.training - Example #0
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'y@@', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'at@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to show that the tyglacial calculat, which for almost three million years had the size of 40 million years of the United States of 40 percent.
2024-05-27 19:00:26,388 - INFO - joeynmt.training - Example #1
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:00:26,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 'very', 'sub@@', 'val@@', 'ue', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'au@@', 'ti@@', 'ful', 'to', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'd.', '</s>']
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:00:26,388 - INFO - joeynmt.training - 	Hypothesis: And this is a very subvalue of the problem because it doesn't show the beautiful to the glacid.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - Example #2
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'y@@', 'c@@', 'le', 'is', '--', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Hypothesis: The glacial cycle is -- in a sense, the heart of the climate system.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - Example #3
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'n@@', 's', 'out', 'the', 'in@@', 'ver@@', 'se', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'of', 'the', 'ex@@', 't@@', 'rem@@', 'end@@', 'ous', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Hypothesis: It turns out the inverse and returns out of the extremendous expand.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - Example #4
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:00:26,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'e,', 'it', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'r@@', 'ying', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:00:26,389 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:00:26,390 - INFO - joeynmt.training - 	Hypothesis: The next next device, it will be a rapid carrying the last 25 years.
2024-05-27 19:00:41,320 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.768479, Batch Acc: 0.534242, Tokens per Sec:     4867, Lr: 0.000300
2024-05-27 19:00:56,415 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.622836, Batch Acc: 0.529197, Tokens per Sec:     4530, Lr: 0.000300
2024-05-27 19:01:11,807 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.706539, Batch Acc: 0.532667, Tokens per Sec:     4668, Lr: 0.000300
2024-05-27 19:01:27,170 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.588449, Batch Acc: 0.534557, Tokens per Sec:     4659, Lr: 0.000300
2024-05-27 19:01:41,885 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.725889, Batch Acc: 0.529362, Tokens per Sec:     4758, Lr: 0.000300
2024-05-27 19:01:41,885 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:01:41,885 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:02:54,778 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.52, acc:   0.51, generation: 72.8858[sec], evaluation: 0.0000[sec]
2024-05-27 19:02:54,780 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:02:54,927 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/13500.ckpt
2024-05-27 19:02:54,928 - INFO - joeynmt.training - Example #0
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'in', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:02:54,928 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:02:54,928 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:02:54,928 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the glacial calculus that the glacial calculus in the size of 40 million years of the 40 percent of the United States of 40 percent.
2024-05-27 19:02:54,928 - INFO - joeynmt.training - Example #1
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:02:54,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'un@@', 'der', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'not', 'show', 'it', 'of', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'a.', '</s>']
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Hypothesis: But this under of the gravity of the problem because not show it of the glacia.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - Example #2
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 's', 'are', 'ar@@', 't', '--', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'ri@@', 'or', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Hypothesis: The artics are art -- in a sense, the curior of the global climate system.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - Example #3
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'of', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - 	Hypothesis: You get the wind and wind and wind and wind of the expand.
2024-05-27 19:02:54,929 - INFO - joeynmt.training - Example #4
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:02:54,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'fu@@', 'el', 'came', 'up', 'with', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:02:54,930 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:02:54,930 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:02:54,930 - INFO - joeynmt.training - 	Hypothesis: The next next slide will be a rapid careful carefuel came up with the last 25 years.
2024-05-27 19:03:09,935 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.710347, Batch Acc: 0.525861, Tokens per Sec:     4597, Lr: 0.000300
2024-05-27 19:03:26,222 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.746264, Batch Acc: 0.539805, Tokens per Sec:     4371, Lr: 0.000300
2024-05-27 19:03:41,529 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.442277, Batch Acc: 0.535760, Tokens per Sec:     4588, Lr: 0.000300
2024-05-27 19:03:56,151 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.721654, Batch Acc: 0.539610, Tokens per Sec:     4857, Lr: 0.000300
2024-05-27 19:04:11,400 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.564158, Batch Acc: 0.534325, Tokens per Sec:     4646, Lr: 0.000300
2024-05-27 19:04:11,400 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:04:11,400 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:05:20,047 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.50, acc:   0.51, generation: 68.6397[sec], evaluation: 0.0000[sec]
2024-05-27 19:05:20,049 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:05:20,196 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/14500.ckpt
2024-05-27 19:05:20,196 - INFO - joeynmt.training - Example #0
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ate', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 're@@', 'stre@@', 't@@', 'ed.', '</s>']
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide for demonstrate that calculate glacial calculate that for almost three million years had been restreted.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - Example #1
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'lot', 'of', 'this', 'sub@@', 'val@@', 'u@@', 'es', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'in', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Hypothesis: This is a lot of this subvalues of the problem because it doesn't show it in the ice.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - Example #2
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:05:20,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', ',', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:05:20,197 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Hypothesis: The glacial climb, is in a certain sense, the heart of the climate system.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - Example #3
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', ',', 'and', 'they', 're@@', 'tur@@', 'ned', 'out', 'of', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Hypothesis: You expands of inver, and they returned out of the expand.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - Example #4
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:05:20,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'lo@@', 'or@@', ',', 'it', 'will', 'be', 'a', 'f@@', 'lo@@', 'or@@', 's', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:05:20,198 - INFO - joeynmt.training - 	Hypothesis: The next next slide will be a floor, it will be a floors of the last 25 years.
2024-05-27 19:05:35,892 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.574336, Batch Acc: 0.541794, Tokens per Sec:     4641, Lr: 0.000300
2024-05-27 19:05:50,124 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.405226, Batch Acc: 0.535322, Tokens per Sec:     4969, Lr: 0.000300
2024-05-27 19:06:04,772 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     1.441217, Batch Acc: 0.538035, Tokens per Sec:     4982, Lr: 0.000300
2024-05-27 19:06:20,570 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     1.520685, Batch Acc: 0.538230, Tokens per Sec:     4613, Lr: 0.000300
2024-05-27 19:06:37,076 - INFO - joeynmt.training - Epoch   4, Step:    17500, Batch Loss:     1.391944, Batch Acc: 0.538644, Tokens per Sec:     4272, Lr: 0.000300
2024-05-27 19:06:37,076 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:06:37,076 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:07:50,444 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.44, acc:   0.51, generation: 73.3610[sec], evaluation: 0.0000[sec]
2024-05-27 19:07:50,445 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:07:50,592 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/15000.ckpt
2024-05-27 19:07:50,592 - INFO - joeynmt.training - Example #0
2024-05-27 19:07:50,592 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:07:50,592 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:07:50,592 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'ot@@', 'ally', 'c@@', 'lu@@', 'st@@', 'er', 'to', 'show', 'that', 'the', 't@@', 'ot@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lu@@', 'e,', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'been', 're@@', 'stre@@', 't@@', 'ed.', '</s>']
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to show that the totally cluster to show that the totate glacial clue, which for almost three million years has been restreted.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - Example #1
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'sub@@', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'come', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Hypothesis: This is a subvalues the gravity of the problem because it doesn't show the become of the ice.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - Example #2
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'way,', 'in', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'c@@', 'lu@@', 'e', 'system@@', '.', '</s>']
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial is in a way, in certain sense, the clean clue system.
2024-05-27 19:07:50,593 - INFO - joeynmt.training - Example #3
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:07:50,593 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', ',', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Hypothesis: You expands of inver, and you get right.
2024-05-27 19:07:50,594 - INFO - joeynmt.training - Example #4
2024-05-27 19:07:50,594 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:07:50,594 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:07:50,594 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:07:50,594 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:08:06,705 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     1.462168, Batch Acc: 0.535277, Tokens per Sec:     4387, Lr: 0.000300
2024-05-27 19:08:21,230 - INFO - joeynmt.training - Epoch   4: total training loss 7096.11
2024-05-27 19:08:21,231 - INFO - joeynmt.training - EPOCH 5
2024-05-27 19:08:21,530 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.481852, Batch Acc: 0.534451, Tokens per Sec:     3600, Lr: 0.000300
2024-05-27 19:08:37,776 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.515240, Batch Acc: 0.554683, Tokens per Sec:     4444, Lr: 0.000300
2024-05-27 19:08:53,566 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.359656, Batch Acc: 0.558652, Tokens per Sec:     4500, Lr: 0.000300
2024-05-27 19:09:08,653 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.534048, Batch Acc: 0.554969, Tokens per Sec:     4725, Lr: 0.000300
2024-05-27 19:09:08,653 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:09:08,653 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:10:14,331 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.46, acc:   0.51, generation: 65.6712[sec], evaluation: 0.0000[sec]
2024-05-27 19:10:14,488 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/15500.ckpt
2024-05-27 19:10:14,489 - INFO - joeynmt.training - Example #0
2024-05-27 19:10:14,489 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:10:14,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:10:14,489 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'h@@', 'o@@', 't', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'been', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:10:14,489 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:10:14,489 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:10:14,489 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slide to show that the hot calculation that for almost three million years had the size of 48 million years had been the size of 48 States of 40 percent.
2024-05-27 19:10:14,489 - INFO - joeynmt.training - Example #1
2024-05-27 19:10:14,489 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 's@@', 'li@@', 'ght', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'is@@', 'su@@', 'e', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'gin@@', 'ning', 'of', 'the', 'ice', 'ice', 'ice', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'ess@@', '.', '</s>']
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Hypothesis: This is the slight of the problem because it doesn't show it of the issue because it doesn't show the beginning of the ice ice ice ice because it doesn't show the ice of the ice because it doesn't show it of the ice because it doesn't show the sess.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - Example #2
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'the', 'ar@@', 't', 'is,', 'in', 'a', 'sen@@', 'se', 'of', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'of', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'the', 'glob@@', 'al', 'sy@@', 'ste@@', 'm', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Hypothesis: The calculation is the art is, in a sense of the climate climate the global system of the climate climate the global system climate system.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - Example #3
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:10:14,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'n@@', 's', 'out', 'of', 'the', 'w@@', 'in@@', 'd', 'and', 're@@', 'tur@@', 'ned', 'ou@@', 't.', '</s>']
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - 	Hypothesis: It turns out of the wind and returned out.
2024-05-27 19:10:14,490 - INFO - joeynmt.training - Example #4
2024-05-27 19:10:14,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:10:14,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:10:14,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:10:14,491 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:10:14,491 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:10:14,491 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:10:30,646 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.406715, Batch Acc: 0.556398, Tokens per Sec:     4349, Lr: 0.000300
2024-05-27 19:10:46,275 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.512838, Batch Acc: 0.556455, Tokens per Sec:     4623, Lr: 0.000300
2024-05-27 19:11:00,989 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.472830, Batch Acc: 0.552301, Tokens per Sec:     4782, Lr: 0.000300
2024-05-27 19:11:16,107 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.437142, Batch Acc: 0.553589, Tokens per Sec:     4698, Lr: 0.000300
2024-05-27 19:11:31,725 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.370436, Batch Acc: 0.558395, Tokens per Sec:     4566, Lr: 0.000300
2024-05-27 19:11:31,725 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:11:31,725 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:12:39,738 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.43, acc:   0.51, generation: 68.0056[sec], evaluation: 0.0000[sec]
2024-05-27 19:12:39,740 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:12:39,889 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/16000.ckpt
2024-05-27 19:12:39,890 - INFO - joeynmt.training - Example #0
2024-05-27 19:12:39,890 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:12:39,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:12:39,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to show that the glacial calculation that for almost three million years of years has been the size of 48 States of 40 States of 40 percent.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - Example #1
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['this', 'under@@', 'est@@', 'er@@', 'n', 'is', 'that', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Hypothesis: this underestern is that gravity of the problem because it doesn't show it of the ice.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - Example #2
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - 	Hypothesis: The art glacial climbing is in a sense, the climate customer of global climate system.
2024-05-27 19:12:39,891 - INFO - joeynmt.training - Example #3
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:12:39,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'w@@', 'er@@', 'en@@', '.', '</s>']
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and weren.
2024-05-27 19:12:39,892 - INFO - joeynmt.training - Example #4
2024-05-27 19:12:39,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:12:39,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:12:39,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:12:39,892 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid carlled on the last 25 years.
2024-05-27 19:12:56,447 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.531004, Batch Acc: 0.556280, Tokens per Sec:     4312, Lr: 0.000300
2024-05-27 19:13:13,127 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.518053, Batch Acc: 0.549999, Tokens per Sec:     4368, Lr: 0.000300
2024-05-27 19:13:28,889 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.466123, Batch Acc: 0.551573, Tokens per Sec:     4442, Lr: 0.000300
2024-05-27 19:13:45,420 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.571237, Batch Acc: 0.549942, Tokens per Sec:     4327, Lr: 0.000300
2024-05-27 19:14:01,361 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.428472, Batch Acc: 0.551843, Tokens per Sec:     4423, Lr: 0.000300
2024-05-27 19:14:01,363 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:14:01,363 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:15:12,200 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.38, acc:   0.51, generation: 70.8300[sec], evaluation: 0.0000[sec]
2024-05-27 19:15:12,201 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:15:12,351 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/16500.ckpt
2024-05-27 19:15:12,352 - INFO - joeynmt.training - Example #0
2024-05-27 19:15:12,352 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'the', 'ar@@', 'tic@@', 'al', '4@@', '0', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to show that the artical calculation of the artical 40 million years has had the size of 48 million years of the 48 States of 40 percent of the United States of 40 percent.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - Example #1
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e,', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Hypothesis: This is under the gravity of the problem because it doesn't show the same, because it doesn't show the same.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - Example #2
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:15:12,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'c@@', 'lim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'er@@', 's.', '</s>']
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is, in a sense, the heart climate customers.
2024-05-27 19:15:12,353 - INFO - joeynmt.training - Example #3
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'and', 'the', 'w@@', 'in@@', 'd', 'of', 'the', 'w@@', 'in@@', 'd', 'and', 'the', 'w@@', 'est@@', '.', '</s>']
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Hypothesis: You can get the wind and wind and the wind of the wind and the west.
2024-05-27 19:15:12,354 - INFO - joeynmt.training - Example #4
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:15:12,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'f@@', 'ast@@', '-@@', 's@@', 'li@@', 'ght@@', 'ly', 're@@', 'co@@', 'ver@@', 'ed', 'so@@', 'un@@', 'ds', 'of', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:15:12,354 - INFO - joeynmt.training - 	Hypothesis: The next slides will be a fast-slightly recovered sounds of 25 years.
2024-05-27 19:15:27,670 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.450844, Batch Acc: 0.550278, Tokens per Sec:     4622, Lr: 0.000300
2024-05-27 19:15:43,100 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.454883, Batch Acc: 0.553023, Tokens per Sec:     4546, Lr: 0.000300
2024-05-27 19:15:58,424 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.423274, Batch Acc: 0.550282, Tokens per Sec:     4756, Lr: 0.000300
2024-05-27 19:16:14,051 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.489255, Batch Acc: 0.555758, Tokens per Sec:     4680, Lr: 0.000300
2024-05-27 19:16:29,456 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.526235, Batch Acc: 0.552050, Tokens per Sec:     4679, Lr: 0.000300
2024-05-27 19:16:29,457 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:16:29,457 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:17:37,934 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.30, acc:   0.52, generation: 68.4697[sec], evaluation: 0.0000[sec]
2024-05-27 19:17:37,936 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:17:38,083 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/17000.ckpt
2024-05-27 19:17:38,084 - INFO - joeynmt.training - Example #0
2024-05-27 19:17:38,084 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:17:38,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:17:38,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'last', 'year@@', ',', 'and', 'then', 'that', 'the', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'c@@', 'alc@@', 'ul@@', 'ation', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Hypothesis: I showed these slide last year, and then that the slide to demonstrate that calculation for almost three million years had the size of 48 States of 48 States of 40 percent of the 440 percent.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - Example #1
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'the', 's@@', 'k@@', 'y', 'of', 'the', 'ice', 'ice', 'ice', 'as', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'ice', '</s>']
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Hypothesis: This is under the gravity of the problem because it doesn't show it the sky of the ice ice ice as the ice of the ice ice ice
2024-05-27 19:17:38,085 - INFO - joeynmt.training - Example #2
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calculation is in a sense, the heart of the global climate system.
2024-05-27 19:17:38,085 - INFO - joeynmt.training - Example #3
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:17:38,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:17:38,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'n@@', 's', 'out', 'and', 'w@@', 'in@@', 'ver@@', 'se', 'and', 'w@@', 'in@@', 'd', 'the', 'w@@', 'in@@', 'd', 'of', 'the', 'w@@', 'in@@', 'd', 'w@@', 'ell@@', '.', '</s>']
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Hypothesis: It turns out and winverse and wind the wind of the wind well.
2024-05-27 19:17:38,086 - INFO - joeynmt.training - Example #4
2024-05-27 19:17:38,086 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:17:38,086 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:17:38,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'lo@@', 'or@@', ',', 'it', 'will', 'be', 'a', 'f@@', 'lo@@', 'or@@', 's', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:17:38,086 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a floor, it will be a floors of the last 25 years.
2024-05-27 19:17:53,556 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.519374, Batch Acc: 0.560828, Tokens per Sec:     4634, Lr: 0.000300
2024-05-27 19:18:10,208 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.474986, Batch Acc: 0.552300, Tokens per Sec:     4384, Lr: 0.000300
2024-05-27 19:18:26,394 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.465009, Batch Acc: 0.554673, Tokens per Sec:     4365, Lr: 0.000300
2024-05-27 19:18:41,846 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.564294, Batch Acc: 0.554821, Tokens per Sec:     4682, Lr: 0.000300
2024-05-27 19:18:57,285 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.611801, Batch Acc: 0.550254, Tokens per Sec:     4712, Lr: 0.000300
2024-05-27 19:18:57,285 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:18:57,285 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:20:18,469 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.52, generation: 81.1765[sec], evaluation: 0.0000[sec]
2024-05-27 19:20:18,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:20:18,617 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/18000.ckpt
2024-05-27 19:20:18,618 - INFO - joeynmt.training - Example #0
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'that', 'the', 't@@', 'y@@', 'al', 'cal@@', 'm', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:20:18,618 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:20:18,618 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:20:18,618 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slides to show that the type that the tyal calm that for almost three million years had had the size of 48 States of 48 States of 40 percent.
2024-05-27 19:20:18,618 - INFO - joeynmt.training - Example #1
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:20:18,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E@@', 'x@@', 'p@@', 'atter@@', 'n@@', 's', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'as', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Hypothesis: Expatterns the gravity of the problem because it doesn't show it as the ice.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - Example #2
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'is', 'ar@@', 't@@', 'ic@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'li@@', 'mat@@', 'ical', 'system@@', '.', '</s>']
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Hypothesis: The art is artical climb is, in a certain sense, the heart of the climatical system.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - Example #3
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'the', 'ver@@', 'su@@', 's', 'of', 'the', 'w@@', 'in@@', 'd', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'of', 'the', 'out@@', 'si@@', 'de.', '</s>']
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - 	Hypothesis: You can get the versus of the wind and returns out of the outside.
2024-05-27 19:20:18,619 - INFO - joeynmt.training - Example #4
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:20:18,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:20:18,620 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:20:18,620 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:20:18,620 - INFO - joeynmt.training - 	Hypothesis: The next next slides will be a rapid carlled on the last 25 years.
2024-05-27 19:20:34,301 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.558497, Batch Acc: 0.551569, Tokens per Sec:     4456, Lr: 0.000300
2024-05-27 19:20:50,686 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.441647, Batch Acc: 0.551816, Tokens per Sec:     4415, Lr: 0.000300
2024-05-27 19:21:07,220 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.561105, Batch Acc: 0.558402, Tokens per Sec:     4373, Lr: 0.000300
2024-05-27 19:21:23,340 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.543530, Batch Acc: 0.551051, Tokens per Sec:     4453, Lr: 0.000300
2024-05-27 19:21:39,453 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.490179, Batch Acc: 0.552728, Tokens per Sec:     4431, Lr: 0.000300
2024-05-27 19:21:39,453 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:21:39,453 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:22:53,648 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.52, generation: 74.1883[sec], evaluation: 0.0000[sec]
2024-05-27 19:22:53,651 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:22:53,799 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/17500.ckpt
2024-05-27 19:22:53,800 - INFO - joeynmt.training - Example #0
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'ot@@', 'ally', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'contin@@', 'ent@@', ',', 'is', 're@@', 'stre@@', 't@@', 't@@', 't@@', 't@@', 'y@@', 'p@@', 'e', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:22:53,800 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:22:53,800 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:22:53,800 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to demonstrate that the totally glacial calculation of the 48 dimensions of the 48 States of the 48 States of the 48 percent of the United continent, is restrettttype of the 40 percent.
2024-05-27 19:22:53,800 - INFO - joeynmt.training - Example #1
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:22:53,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'under@@', 'l@@', 'ying', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'ou@@', 'l@@', "dn't", 'show', 'it', 'of', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'ou@@', 'l@@', '.', '</s>']
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Hypothesis: And this underlying the gravity of the problem because it doesn't show the souldn't show it of the ice of the ice because it doesn't show the soul.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - Example #2
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'v@@', 'ing', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calving is in a sense, the heart of global climate system.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - Example #3
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'the', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'h@@', 'it', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - 	Hypothesis: You can get the win-win-win-hit and you get right.
2024-05-27 19:22:53,801 - INFO - joeynmt.training - Example #4
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:22:53,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:22:53,802 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:22:53,802 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:22:53,802 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid carlled on the last 25 years.
2024-05-27 19:23:09,217 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.623968, Batch Acc: 0.552705, Tokens per Sec:     4520, Lr: 0.000300
2024-05-27 19:23:24,702 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.416380, Batch Acc: 0.546118, Tokens per Sec:     4528, Lr: 0.000300
2024-05-27 19:23:41,870 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.574134, Batch Acc: 0.550244, Tokens per Sec:     4156, Lr: 0.000300
2024-05-27 19:23:59,370 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.553580, Batch Acc: 0.548221, Tokens per Sec:     4072, Lr: 0.000300
2024-05-27 19:24:14,854 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.584629, Batch Acc: 0.548123, Tokens per Sec:     4638, Lr: 0.000300
2024-05-27 19:24:14,855 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:24:14,855 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:25:21,882 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.25, acc:   0.52, generation: 67.0201[sec], evaluation: 0.0000[sec]
2024-05-27 19:25:21,886 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:25:22,032 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/18500.ckpt
2024-05-27 19:25:22,033 - INFO - joeynmt.training - Example #0
2024-05-27 19:25:22,033 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:25:22,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:25:22,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to show that the calculation glacial glacial glacial sizes of 48 percent of the United States of 40 percent of the 40 percent.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - Example #1
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'little', 'bit', 'of', 'this', 'under@@', 'l@@', 'ying', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'gin@@', 'ning', 'of', 'the', 'ice', '--', '</s>']
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Hypothesis: This is a little bit of this underlying the gravity of the problem because it doesn't show the beginning of the ice --
2024-05-27 19:25:22,034 - INFO - joeynmt.training - Example #2
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'lim@@', 'b@@', 'ed', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'c@@', 'li@@', 'mat@@', 'ic@@', '.', '</s>']
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - 	Hypothesis: The climbed glacial climate is, in a sense, the heart of the climate system climatic.
2024-05-27 19:25:22,034 - INFO - joeynmt.training - Example #3
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:25:22,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'w@@', 'in@@', 'd', 'and', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus wind and the expand.
2024-05-27 19:25:22,035 - INFO - joeynmt.training - Example #4
2024-05-27 19:25:22,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:25:22,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:25:22,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:25:22,035 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a couple of the last 25 years.
2024-05-27 19:25:38,439 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.431338, Batch Acc: 0.554003, Tokens per Sec:     4419, Lr: 0.000300
2024-05-27 19:25:55,366 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.566183, Batch Acc: 0.551144, Tokens per Sec:     4150, Lr: 0.000300
2024-05-27 19:26:10,963 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.624317, Batch Acc: 0.548870, Tokens per Sec:     4513, Lr: 0.000300
2024-05-27 19:26:27,951 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.430794, Batch Acc: 0.552643, Tokens per Sec:     4327, Lr: 0.000300
2024-05-27 19:26:44,354 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.577787, Batch Acc: 0.560030, Tokens per Sec:     4378, Lr: 0.000300
2024-05-27 19:26:44,355 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:26:44,355 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:27:55,654 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.53, generation: 71.2912[sec], evaluation: 0.0000[sec]
2024-05-27 19:27:55,657 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:27:55,804 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/19000.ckpt
2024-05-27 19:27:55,805 - INFO - joeynmt.training - Example #0
2024-05-27 19:27:55,805 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:27:55,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:27:55,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 't@@', 'ot@@', 'ally', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 're@@', 'stre@@', 't@@', 'ch@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'contin@@', 'ent@@', ',', 'is', 're@@', 'stre@@', 't@@', 'ed.', '</s>']
2024-05-27 19:27:55,805 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slides to show that the totally glacial calculation that for almost three million years had been restretches of 48 States of 40 percent of the United continent, is restreted.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - Example #1
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'in', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Hypothesis: This is under the gravity of the problem because it doesn't show it in the ice of the ice.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - Example #2
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'sen@@', 'se', 'of', 'the', 'c@@', 'lim@@', 'ate', 'cu@@', 'm@@', 'p', 'c@@', 'lim@@', 'ate', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is, in a sense of the climate cump climate the climate system.
2024-05-27 19:27:55,806 - INFO - joeynmt.training - Example #3
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:27:55,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'w@@', 'er@@', 'en@@', "'t", 'ex@@', 'p@@', 'and@@', 'ing', 'do@@', 'wn@@', '.', '</s>']
2024-05-27 19:27:55,806 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:27:55,807 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:27:55,807 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and weren't expanding down.
2024-05-27 19:27:55,807 - INFO - joeynmt.training - Example #4
2024-05-27 19:27:55,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:27:55,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:27:55,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:27:55,807 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:27:55,807 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:27:55,807 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid carlled on the last 25 years.
2024-05-27 19:28:11,578 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     1.407418, Batch Acc: 0.548369, Tokens per Sec:     4526, Lr: 0.000300
2024-05-27 19:28:27,731 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     1.353076, Batch Acc: 0.553921, Tokens per Sec:     4424, Lr: 0.000300
2024-05-27 19:28:43,459 - INFO - joeynmt.training - Epoch   5, Step:    21800, Batch Loss:     1.584823, Batch Acc: 0.549752, Tokens per Sec:     4633, Lr: 0.000300
2024-05-27 19:28:59,377 - INFO - joeynmt.training - Epoch   5, Step:    21900, Batch Loss:     1.698251, Batch Acc: 0.554292, Tokens per Sec:     4578, Lr: 0.000300
2024-05-27 19:29:14,557 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     1.475813, Batch Acc: 0.554538, Tokens per Sec:     4698, Lr: 0.000300
2024-05-27 19:29:14,557 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:29:14,557 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:30:34,401 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.13, acc:   0.52, generation: 79.8366[sec], evaluation: 0.0000[sec]
2024-05-27 19:30:34,402 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:30:34,552 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/19500.ckpt
2024-05-27 19:30:34,553 - INFO - joeynmt.training - Example #0
2024-05-27 19:30:34,553 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:30:34,553 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:30:34,553 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'this', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', '4@@', '0', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'of', 'years', 'of', '4@@', '0', 'million', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:30:34,553 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:30:34,553 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Hypothesis: And I showed these slide this slide to show that the calculation glacial calculation of 40 million years had the size of 48 million years of years of 40 million years of the United States of 40 percent.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - Example #1
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'had', 'this', 'un@@', 'der', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Hypothesis: And I had this under under the gravity of the problem because it doesn't show the same.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - Example #2
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Hypothesis: The calculate glacial calculation is, in a sense, the heart heart of the global climate system.
2024-05-27 19:30:34,554 - INFO - joeynmt.training - Example #3
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:30:34,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'w@@', 'in@@', 'n@@', 'er', 'and', 'w@@', 'in@@', 'd', 'the', 'w@@', 'in@@', 'd', 'of', 'the', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'n@@', 'er', 'and', 'w@@', 'in@@', 'n@@', 'er', 'and', 'w@@', 'in@@', 'd', 'of', 'the', 'w@@', 'in@@', 'd@@', 's.', '</s>']
2024-05-27 19:30:34,554 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:30:34,555 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:30:34,555 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and winner and wind the wind of the wind and winner and winner and wind of the winds.
2024-05-27 19:30:34,555 - INFO - joeynmt.training - Example #4
2024-05-27 19:30:34,555 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:30:34,555 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:30:34,555 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:30:34,555 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:30:34,555 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:30:34,555 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:30:50,438 - INFO - joeynmt.training - Epoch   5, Step:    22100, Batch Loss:     1.541736, Batch Acc: 0.552993, Tokens per Sec:     4372, Lr: 0.000300
2024-05-27 19:30:53,428 - INFO - joeynmt.training - Epoch   5: total training loss 6749.48
2024-05-27 19:30:53,429 - INFO - joeynmt.training - EPOCH 6
2024-05-27 19:31:06,594 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.494091, Batch Acc: 0.573025, Tokens per Sec:     4589, Lr: 0.000300
2024-05-27 19:31:22,303 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.322015, Batch Acc: 0.576296, Tokens per Sec:     4538, Lr: 0.000300
2024-05-27 19:31:37,072 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.469749, Batch Acc: 0.578829, Tokens per Sec:     4949, Lr: 0.000300
2024-05-27 19:31:52,280 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.437141, Batch Acc: 0.576784, Tokens per Sec:     4750, Lr: 0.000300
2024-05-27 19:31:52,280 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:31:52,280 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:33:06,922 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.52, generation: 74.6341[sec], evaluation: 0.0000[sec]
2024-05-27 19:33:07,073 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/20000.ckpt
2024-05-27 19:33:07,074 - INFO - joeynmt.training - Example #0
2024-05-27 19:33:07,074 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:33:07,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:33:07,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'ot@@', 'ally', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Hypothesis: And this last year I showed these slide to show that the totally glacial calculation that for three million years had had the size of 40 million years had had the size of 40 percent of the United States of 40 percent.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - Example #1
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'lot', 'of', 'this', 'under@@', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'li@@', 'de.', '</s>']
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Hypothesis: This is a lot of this undervalues the gravity of the problem because it doesn't show the ice of ice ice ice as the ice of ice ice ice ice ice as the ice of ice because it doesn't show the slide.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - Example #2
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'lim@@', 'ate', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - 	Hypothesis: The climate glacial climate is, in a certain sense, the heart of the global climate system.
2024-05-27 19:33:07,075 - INFO - joeynmt.training - Example #3
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:33:07,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 're@@', 'tur@@', 'ned', 'ou@@', 't.', '</s>']
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and returned out.
2024-05-27 19:33:07,076 - INFO - joeynmt.training - Example #4
2024-05-27 19:33:07,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:33:07,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:33:07,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:33:07,076 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid of the last 25 years.
2024-05-27 19:33:22,227 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.425466, Batch Acc: 0.568825, Tokens per Sec:     4801, Lr: 0.000300
2024-05-27 19:33:38,886 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.263676, Batch Acc: 0.579149, Tokens per Sec:     4236, Lr: 0.000300
2024-05-27 19:33:54,490 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.527152, Batch Acc: 0.567795, Tokens per Sec:     4433, Lr: 0.000300
2024-05-27 19:34:10,101 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.396008, Batch Acc: 0.568863, Tokens per Sec:     4601, Lr: 0.000300
2024-05-27 19:34:25,001 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.442066, Batch Acc: 0.564190, Tokens per Sec:     4728, Lr: 0.000300
2024-05-27 19:34:25,001 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:34:25,001 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:35:45,506 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.16, acc:   0.53, generation: 80.4973[sec], evaluation: 0.0000[sec]
2024-05-27 19:35:45,654 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/20500.ckpt
2024-05-27 19:35:45,654 - INFO - joeynmt.training - Example #0
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Hypothesis: I showed these slide these slide to show that the type calculation glacial calculation for almost three million years had had the sizes of 48 percent of the United States of 40 percent.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - Example #1
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'is', 'un@@', 'der', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem@@', ',', 'because', 'it', "doesn't", 'show', 'it', 'to', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'as', 'ice', 'as', 'the', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'it', 'show', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'is@@', 'su@@', 'e', 'because', 'it', "doesn't", 'show', 'it', 'to', 'the', 's@@', 'li@@', 'p@@', '.', '</s>']
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Hypothesis: And yet this is under gravity of the problem, because it doesn't show it to the ice of ice because it doesn't show the ice of ice as ice as ice as ice as ice as the ice of ice as ice as the ice as the ice of ice because it doesn't show it show the gravity of the issue because it doesn't show it to the slip.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - Example #2
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:35:45,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'al', 'c@@', 'lim@@', 'b', 'is', '--', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'c@@', 'le@@', 'an', 'c@@', 'le@@', 'an', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:35:45,655 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Hypothesis: The artical climb is -- in a certain sense, the heart clean clean climate system.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - Example #3
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd@@', 'l@@', 'ed', 'to', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'in@@', 'd', 'of', 'the', 'in@@', 'ver@@', 'se', 'and', 'w@@', 'in@@', 'd@@', 'l@@', 'es.', '</s>']
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and wind and windled to the summer and wind of the inverse and windles.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - Example #4
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:35:45,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:35:45,656 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 19:36:01,070 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.617625, Batch Acc: 0.566260, Tokens per Sec:     4575, Lr: 0.000300
2024-05-27 19:36:17,189 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.432410, Batch Acc: 0.564708, Tokens per Sec:     4439, Lr: 0.000300
2024-05-27 19:36:33,472 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.530576, Batch Acc: 0.568119, Tokens per Sec:     4441, Lr: 0.000300
2024-05-27 19:36:48,932 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.452918, Batch Acc: 0.567270, Tokens per Sec:     4642, Lr: 0.000300
2024-05-27 19:37:04,202 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.496530, Batch Acc: 0.571603, Tokens per Sec:     4725, Lr: 0.000300
2024-05-27 19:37:04,202 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:37:04,202 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:38:17,198 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.13, acc:   0.53, generation: 72.9887[sec], evaluation: 0.0000[sec]
2024-05-27 19:38:17,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:38:17,351 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/21000.ckpt
2024-05-27 19:38:17,352 - INFO - joeynmt.training - Example #0
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'ot@@', 'ice', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 'the', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'has', 'been', 're@@', 'stre@@', 't@@', 't@@', 't@@', 'y@@', 'p@@', 'e', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the totice glacial calculus for almost three million years had been the dimensions of 48 States has been restretttype of 40 percent.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - Example #1
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:38:17,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['A@@', 'll', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'us@@', 'pen@@', 'd@@', 'ent', 'of', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 's@@', 'ou@@', 'l@@', "dn't", 'show', 'it', 'as', 'w@@', 'ell@@', '.', '</s>']
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - 	Hypothesis: All this underestimate the gravity of the problem because it doesn't show the suspendent of the ice of the ice ice ice of the ice ice ice souldn't show it as well.
2024-05-27 19:38:17,352 - INFO - joeynmt.training - Example #2
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is', 'in', 'some', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is in some sense, the heart heart, the heart heart of the global climate system.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - Example #3
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'you', 'get', 'ri@@', 'd', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Hypothesis: You expand you get rid and you get right.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - Example #4
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:38:17,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:38:17,353 - INFO - joeynmt.training - 	Hypothesis: The next next slide will be a rapid carlled on the last 25 years.
2024-05-27 19:38:33,384 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.501228, Batch Acc: 0.564676, Tokens per Sec:     4343, Lr: 0.000300
2024-05-27 19:38:48,419 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.546158, Batch Acc: 0.562734, Tokens per Sec:     4687, Lr: 0.000300
2024-05-27 19:39:04,674 - INFO - joeynmt.training - Epoch   6, Step:    23800, Batch Loss:     1.532855, Batch Acc: 0.566736, Tokens per Sec:     4448, Lr: 0.000300
2024-05-27 19:39:20,188 - INFO - joeynmt.training - Epoch   6, Step:    23900, Batch Loss:     1.447514, Batch Acc: 0.562838, Tokens per Sec:     4573, Lr: 0.000300
2024-05-27 19:39:35,427 - INFO - joeynmt.training - Epoch   6, Step:    24000, Batch Loss:     1.513908, Batch Acc: 0.563855, Tokens per Sec:     4740, Lr: 0.000300
2024-05-27 19:39:35,427 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:39:35,427 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:40:53,085 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.53, generation: 77.6514[sec], evaluation: 0.0000[sec]
2024-05-27 19:40:53,088 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:40:53,236 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/21500.ckpt
2024-05-27 19:40:53,237 - INFO - joeynmt.training - Example #0
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'ot@@', 'i@@', 'ght', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'been', 'the', 'contin@@', 'ent@@', 's', 'of', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:40:53,237 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:40:53,237 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:40:53,237 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to demonstrate that the totight glacial glacial size of 48 million years has been the continents of 48 percent.
2024-05-27 19:40:53,237 - INFO - joeynmt.training - Example #1
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:40:53,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Hypothesis: And this is a gravity of the gravity of the problem because it doesn't show the ice of the ice.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - Example #2
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'or', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Hypothesis: The artor glacial climb is, in a certain sense, the customer of global climate system.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - Example #3
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'ver@@', 'su@@', 's', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'of', 'the', 'su@@', 'm@@', 'er', 'and', 're@@', 't@@', 'ab@@', 'le.', '</s>']
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - 	Hypothesis: You expands of inverversus and returns out of the sumer and retable.
2024-05-27 19:40:53,238 - INFO - joeynmt.training - Example #4
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:40:53,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 're@@', 'li@@', 'e@@', 'f', 'on', 'the', 're@@', 'ven@@', 'u@@', 'es', 'of', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:40:53,239 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:40:53,239 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:40:53,239 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid relief on the revenues of 25 years.
2024-05-27 19:41:08,744 - INFO - joeynmt.training - Epoch   6, Step:    24100, Batch Loss:     1.390351, Batch Acc: 0.574256, Tokens per Sec:     4560, Lr: 0.000300
2024-05-27 19:41:25,069 - INFO - joeynmt.training - Epoch   6, Step:    24200, Batch Loss:     1.433280, Batch Acc: 0.568740, Tokens per Sec:     4424, Lr: 0.000300
2024-05-27 19:41:40,486 - INFO - joeynmt.training - Epoch   6, Step:    24300, Batch Loss:     1.471362, Batch Acc: 0.566056, Tokens per Sec:     4581, Lr: 0.000300
2024-05-27 19:41:57,063 - INFO - joeynmt.training - Epoch   6, Step:    24400, Batch Loss:     1.302726, Batch Acc: 0.563337, Tokens per Sec:     4435, Lr: 0.000300
2024-05-27 19:42:13,214 - INFO - joeynmt.training - Epoch   6, Step:    24500, Batch Loss:     1.408888, Batch Acc: 0.570108, Tokens per Sec:     4382, Lr: 0.000300
2024-05-27 19:42:13,214 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:42:13,214 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:43:21,260 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.07, acc:   0.53, generation: 68.0383[sec], evaluation: 0.0000[sec]
2024-05-27 19:43:21,261 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:43:21,414 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/22500.ckpt
2024-05-27 19:43:21,415 - INFO - joeynmt.training - Example #0
2024-05-27 19:43:21,415 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:43:21,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to show that the glacial calculation that the glacial calculus of 48 million years has had the size of 48 States of 40 percent of the 40 percent.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - Example #1
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'un@@', 'der', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 'ch@@', 'ar@@', 'ge', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Hypothesis: But this is under values the gravity because it doesn't show the charge of the ice.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - Example #2
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:43:21,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - 	Hypothesis: The art glacial calculus is in a certain sense, the heart heart of the global climate system.
2024-05-27 19:43:21,416 - INFO - joeynmt.training - Example #3
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'w@@', 'er@@', 'en@@', '.', '</s>']
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and weren.
2024-05-27 19:43:21,417 - INFO - joeynmt.training - Example #4
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:43:21,417 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:43:21,417 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:43:36,825 - INFO - joeynmt.training - Epoch   6, Step:    24600, Batch Loss:     1.455913, Batch Acc: 0.567070, Tokens per Sec:     4718, Lr: 0.000300
2024-05-27 19:43:52,929 - INFO - joeynmt.training - Epoch   6, Step:    24700, Batch Loss:     1.334307, Batch Acc: 0.559418, Tokens per Sec:     4462, Lr: 0.000300
2024-05-27 19:44:08,986 - INFO - joeynmt.training - Epoch   6, Step:    24800, Batch Loss:     1.426006, Batch Acc: 0.564646, Tokens per Sec:     4395, Lr: 0.000300
2024-05-27 19:44:24,559 - INFO - joeynmt.training - Epoch   6, Step:    24900, Batch Loss:     1.634164, Batch Acc: 0.565612, Tokens per Sec:     4646, Lr: 0.000300
2024-05-27 19:44:41,139 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.338225, Batch Acc: 0.567729, Tokens per Sec:     4220, Lr: 0.000300
2024-05-27 19:44:41,139 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:44:41,139 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:45:56,198 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.06, acc:   0.53, generation: 75.0516[sec], evaluation: 0.0000[sec]
2024-05-27 19:45:56,201 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:45:56,349 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/23000.ckpt
2024-05-27 19:45:56,349 - INFO - joeynmt.training - Example #0
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'op@@', 'ic@@', 's', 'that', 'the', 't@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', '.', 'And', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'on', 'has', 'been', 're@@', 'stre@@', 't@@', 'ed.', '</s>']
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to show that the topics that the tacial calot. And for almost three million years had the size of 48 million years had the size of 40 million years on has been restreted.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - Example #1
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'f@@', 'all@@', 'ing', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'gin@@', 'ning', 'of', 'the', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - 	Hypothesis: This is the falling the gravity of the problem because it doesn't show the of, because it doesn't show the beginning of the ice ice ice of the ice problem because it doesn't show the same.
2024-05-27 19:45:56,350 - INFO - joeynmt.training - Example #2
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:45:56,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'p', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Hypothesis: The art glacial climbing is, in a sense, the heart cup of the global climate system.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - Example #3
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'rit@@', 'ing', 'and', 'w@@', 'rit@@', 'ing', 'the', 'ex@@', 'p@@', 'and@@', '.', '</s>']
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Hypothesis: You expands and writing and writing the expand.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - Example #4
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:45:56,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:45:56,351 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:46:12,997 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.413153, Batch Acc: 0.563474, Tokens per Sec:     4264, Lr: 0.000300
2024-05-27 19:46:29,292 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.412641, Batch Acc: 0.566166, Tokens per Sec:     4407, Lr: 0.000300
2024-05-27 19:46:46,455 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.594901, Batch Acc: 0.570363, Tokens per Sec:     4212, Lr: 0.000300
2024-05-27 19:47:03,090 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.342665, Batch Acc: 0.566776, Tokens per Sec:     4271, Lr: 0.000300
2024-05-27 19:47:19,391 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.598404, Batch Acc: 0.571145, Tokens per Sec:     4508, Lr: 0.000300
2024-05-27 19:47:19,391 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:47:19,391 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:48:30,643 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.06, acc:   0.53, generation: 71.2449[sec], evaluation: 0.0000[sec]
2024-05-27 19:48:30,799 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/22000.ckpt
2024-05-27 19:48:30,800 - INFO - joeynmt.training - Example #0
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'contin@@', 'ent@@', 's,', "it's", 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'by', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Hypothesis: The year last year, I showed these slide to show that the calculation calculation that for almost three million years had been the size of 48 States of 48 States of 40 percent of the United continents, it's restretched by 40 percent.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - Example #1
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'sub@@', 'val@@', 'it@@', 'ting', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'be@@', 'gin@@', 'n@@', 'ing.', '</s>']
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - 	Hypothesis: This is a subvalitting the gravity of the problem because it doesn't show the beginning.
2024-05-27 19:48:30,800 - INFO - joeynmt.training - Example #2
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:48:30,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'ar@@', 't', 'with', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Hypothesis: The calculation is art with a certain sense, the heart of the global climate system.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - Example #3
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'the', 'w@@', 'in@@', 'ver@@', 'se', 'and', 'w@@', 'rit@@', 'ing', 'and', 'w@@', 'rit@@', 'er@@', 's.', '</s>']
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Hypothesis: You can get the winverse and writing and writers.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - Example #4
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:48:30,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:48:30,801 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 19:48:46,632 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.521843, Batch Acc: 0.566667, Tokens per Sec:     4543, Lr: 0.000300
2024-05-27 19:49:03,324 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.452821, Batch Acc: 0.573324, Tokens per Sec:     4226, Lr: 0.000300
2024-05-27 19:49:20,707 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.432895, Batch Acc: 0.576367, Tokens per Sec:     4163, Lr: 0.000300
2024-05-27 19:49:37,906 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.389848, Batch Acc: 0.564473, Tokens per Sec:     4187, Lr: 0.000300
2024-05-27 19:49:54,024 - INFO - joeynmt.training - Epoch   6, Step:    26000, Batch Loss:     1.346015, Batch Acc: 0.567048, Tokens per Sec:     4540, Lr: 0.000300
2024-05-27 19:49:54,024 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:49:54,025 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:51:07,912 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.53, generation: 73.8808[sec], evaluation: 0.0000[sec]
2024-05-27 19:51:07,915 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 19:51:08,064 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/23500.ckpt
2024-05-27 19:51:08,065 - INFO - joeynmt.training - Example #0
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:51:08,065 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:51:08,065 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:51:08,065 - INFO - joeynmt.training - 	Hypothesis: The year last year I showed these slide to show that the calculation calculation that for almost three million years had the size of the 48 percent of the 48 percent of the United States of 40 percent.
2024-05-27 19:51:08,065 - INFO - joeynmt.training - Example #1
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:51:08,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'under@@', 'val@@', 'u@@', 'able', 'for', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 19:51:08,065 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Hypothesis: And yet this undervaluable for the problem because it doesn't show the same.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - Example #2
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'is', 'a', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Hypothesis: The art is a glacial climate is in a sense, the heart of global climate system.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - Example #3
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'the', 'w@@', 'in@@', 'on@@', 'es', 'and', 'w@@', 'rit@@', 'e', 'of', 'ex@@', 'p@@', 'lo@@', 'it@@', 'ed', 'up@@', '.', '</s>']
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Hypothesis: You expand the winones and write of exploited up.
2024-05-27 19:51:08,066 - INFO - joeynmt.training - Example #4
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:51:08,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:51:08,066 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:51:08,067 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:51:08,067 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid of the last 25 years.
2024-05-27 19:51:25,303 - INFO - joeynmt.training - Epoch   6, Step:    26100, Batch Loss:     1.431802, Batch Acc: 0.570649, Tokens per Sec:     4111, Lr: 0.000300
2024-05-27 19:51:41,578 - INFO - joeynmt.training - Epoch   6, Step:    26200, Batch Loss:     1.581991, Batch Acc: 0.570445, Tokens per Sec:     4419, Lr: 0.000300
2024-05-27 19:51:58,777 - INFO - joeynmt.training - Epoch   6, Step:    26300, Batch Loss:     1.402314, Batch Acc: 0.571525, Tokens per Sec:     4143, Lr: 0.000300
2024-05-27 19:52:15,258 - INFO - joeynmt.training - Epoch   6, Step:    26400, Batch Loss:     1.754664, Batch Acc: 0.566277, Tokens per Sec:     4271, Lr: 0.000300
2024-05-27 19:52:32,484 - INFO - joeynmt.training - Epoch   6, Step:    26500, Batch Loss:     1.354683, Batch Acc: 0.563808, Tokens per Sec:     4007, Lr: 0.000300
2024-05-27 19:52:32,484 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:52:32,484 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:53:43,647 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.04, acc:   0.53, generation: 71.1564[sec], evaluation: 0.0000[sec]
2024-05-27 19:53:43,795 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/24000.ckpt
2024-05-27 19:53:43,796 - INFO - joeynmt.training - Example #0
2024-05-27 19:53:43,796 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:53:43,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:53:43,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'in', 'the', 'Un@@', 'it@@', 'ed', 'contin@@', 'ent@@', ',', "it's", 're@@', 'stre@@', 't@@', 'ch@@', 'ing', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to demonstrate that calculation glacial glacial glacial dimensions of 48 percent of the United States in the United continent, it's restretching 40 percent.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - Example #1
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'sub@@', 'val@@', 'u@@', 'able', 'for', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Hypothesis: This is a subvaluable for the problem because it doesn't show it of the ice because it doesn't show the ice of ice because of ice because it doesn't show the ice of ice because it doesn't show the same.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - Example #2
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - 	Hypothesis: The art glacial calculation is in a certain sense, the heart of global climate system.
2024-05-27 19:53:43,797 - INFO - joeynmt.training - Example #3
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:53:43,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'es', 'and', 'w@@', 'in@@', 'd', 'and', 'w@@', 'in@@', 'd', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Hypothesis: You expands of inveries and wind and wind of the summer.
2024-05-27 19:53:43,798 - INFO - joeynmt.training - Example #4
2024-05-27 19:53:43,798 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:53:43,798 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:53:43,798 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:53:43,798 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 19:53:49,110 - INFO - joeynmt.training - Epoch   6: total training loss 6496.74
2024-05-27 19:53:49,110 - INFO - joeynmt.training - EPOCH 7
2024-05-27 19:53:59,880 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.304539, Batch Acc: 0.588430, Tokens per Sec:     4416, Lr: 0.000300
2024-05-27 19:54:16,307 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.376222, Batch Acc: 0.593353, Tokens per Sec:     4559, Lr: 0.000300
2024-05-27 19:54:33,519 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.346357, Batch Acc: 0.588891, Tokens per Sec:     4180, Lr: 0.000300
2024-05-27 19:54:50,435 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.270872, Batch Acc: 0.588709, Tokens per Sec:     4136, Lr: 0.000300
2024-05-27 19:55:07,985 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.383168, Batch Acc: 0.587704, Tokens per Sec:     4045, Lr: 0.000300
2024-05-27 19:55:07,986 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:55:07,986 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:56:06,921 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.03, acc:   0.53, generation: 58.9282[sec], evaluation: 0.0000[sec]
2024-05-27 19:56:07,069 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/24500.ckpt
2024-05-27 19:56:07,070 - INFO - joeynmt.training - Example #0
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:56:07,070 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:56:07,070 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:56:07,070 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide slide to show that the calculation glacial calculation that for almost three million years had the size of the 48 million years had the size of 48 percent of the 40 percent of the 40 percent.
2024-05-27 19:56:07,070 - INFO - joeynmt.training - Example #1
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:56:07,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'k@@', 'y', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', '--', '</s>']
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the sky of the ice because it doesn't show the ice of ice --
2024-05-27 19:56:07,071 - INFO - joeynmt.training - Example #2
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'p@@', 'e', 'is', 'ar@@', 't@@', 'ic@@', ',', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Hypothesis: The artype is artic, in a certain sense, the heart of the global climate system.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - Example #3
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'or', 'and', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', 'n@@', 'er.', '</s>']
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - 	Hypothesis: You expands of inverior and win-win-win-win-win-win-win-win-win-win-winner.
2024-05-27 19:56:07,071 - INFO - joeynmt.training - Example #4
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:56:07,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'co@@', 'o@@', 'l', 's@@', 'li@@', 'ght@@', 'ly', 'the', 're@@', 'al@@', 'ity', 'of', 'the', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2024-05-27 19:56:07,072 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:56:07,072 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:56:07,072 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a cool slightly the reality of the 25 years old.
2024-05-27 19:56:23,122 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.184677, Batch Acc: 0.582851, Tokens per Sec:     4376, Lr: 0.000300
2024-05-27 19:56:38,964 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.428827, Batch Acc: 0.583702, Tokens per Sec:     4476, Lr: 0.000300
2024-05-27 19:56:54,180 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.549831, Batch Acc: 0.580808, Tokens per Sec:     4668, Lr: 0.000300
2024-05-27 19:57:09,449 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.546348, Batch Acc: 0.579050, Tokens per Sec:     4671, Lr: 0.000300
2024-05-27 19:57:26,000 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.697944, Batch Acc: 0.575562, Tokens per Sec:     4411, Lr: 0.000300
2024-05-27 19:57:26,001 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:57:26,001 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 19:58:33,171 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.03, acc:   0.53, generation: 67.1621[sec], evaluation: 0.0000[sec]
2024-05-27 19:58:33,321 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/25500.ckpt
2024-05-27 19:58:33,322 - INFO - joeynmt.training - Example #0
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 's@@', 'li@@', 'de', 'of', 'the', 'ar@@', 'tic@@', 'al', 's@@', 'li@@', 'de', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'th@@', 'es@@', 'e,', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'ed', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the slide of the artical slide that the glacial these, which for almost three million years of the United States of 40 percent of the United ed ed States of 40 percent.
2024-05-27 19:58:33,322 - INFO - joeynmt.training - Example #1
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 19:58:33,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'sub@@', 'val@@', 'it@@', 'able', 'for', 'the', 'f@@', 'lo@@', 'or', 'because', 'it', "doesn't", 'show', 'it', 'in', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'the']
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 19:58:33,322 - INFO - joeynmt.training - 	Hypothesis: And yet this subvalitable for the floor because it doesn't show it in the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as the
2024-05-27 19:58:33,322 - INFO - joeynmt.training - Example #2
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 's@@', 'ac@@', 'i@@', 'al', 'cu@@', 're', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'hear@@', 't,', 'the', 'glob@@', 'al', 'cu@@', 'st@@', 'om@@', 'er', 'sy@@', 'ste@@', 'm', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Hypothesis: The art glacial sacial cure is, in a certain sense, the climate heart, the global customer system of the global climate system.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - Example #3
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', '-@@', 'w@@', 'in@@', 'd@@', 's.', '</s>']
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Hypothesis: You get the win-win-win-win-win-winds.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - Example #4
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 19:58:33,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', 'oun@@', 'd@@', 'ly', 'ap@@', 'pe@@', 'ar@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 19:58:33,323 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quickly fastoundly appeared on the last 25 years.
2024-05-27 19:58:48,915 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.355384, Batch Acc: 0.583022, Tokens per Sec:     4572, Lr: 0.000300
2024-05-27 19:59:04,622 - INFO - joeynmt.training - Epoch   7, Step:    27700, Batch Loss:     1.672160, Batch Acc: 0.576635, Tokens per Sec:     4552, Lr: 0.000300
2024-05-27 19:59:20,718 - INFO - joeynmt.training - Epoch   7, Step:    27800, Batch Loss:     1.516163, Batch Acc: 0.579621, Tokens per Sec:     4488, Lr: 0.000300
2024-05-27 19:59:35,976 - INFO - joeynmt.training - Epoch   7, Step:    27900, Batch Loss:     1.482587, Batch Acc: 0.584045, Tokens per Sec:     4641, Lr: 0.000300
2024-05-27 19:59:51,987 - INFO - joeynmt.training - Epoch   7, Step:    28000, Batch Loss:     1.419880, Batch Acc: 0.585878, Tokens per Sec:     4382, Lr: 0.000300
2024-05-27 19:59:51,987 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 19:59:51,987 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:01:00,465 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.53, generation: 68.4716[sec], evaluation: 0.0000[sec]
2024-05-27 20:01:00,616 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/25000.ckpt
2024-05-27 20:01:00,617 - INFO - joeynmt.training - Example #0
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'the', 't@@', 'y@@', 'p@@', 'e', 'of', 'the', 't@@', 'y@@', 'p@@', 'e', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'U@@', '.@@', 'S@@', '.', 'contin@@', 'ent@@', ',', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'about', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the calculation of the type of the type of 48 million years has had the sizes of 48 million years has had the U.S. continent, is restretched about 40 percent.
2024-05-27 20:01:00,618 - INFO - joeynmt.training - Example #1
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'this', 'sub@@', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'of', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'the', 'ice', 'that', 'we', "don't", 'show']
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Hypothesis: And I have this subvalues the gravity of the problem because it doesn't show the ice of ice because it doesn't show the ice of ice because of ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as ice as the ice that we don't show
2024-05-27 20:01:00,618 - INFO - joeynmt.training - Example #2
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:01:00,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:01:00,618 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is, in a certain sense, the climate heart of the global climate system.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - Example #3
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'get', 'the', 'in@@', 'ver@@', 'se', 'and', 're@@', 'tur@@', 'n', 'out', 'out', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Hypothesis: You can get the inverse and return out out of the summer and you get right.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - Example #4
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:01:00,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:01:00,619 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick carlled on the last 25 years.
2024-05-27 20:01:15,816 - INFO - joeynmt.training - Epoch   7, Step:    28100, Batch Loss:     1.372340, Batch Acc: 0.586848, Tokens per Sec:     4636, Lr: 0.000300
2024-05-27 20:01:32,305 - INFO - joeynmt.training - Epoch   7, Step:    28200, Batch Loss:     1.451525, Batch Acc: 0.585087, Tokens per Sec:     4297, Lr: 0.000300
2024-05-27 20:01:48,293 - INFO - joeynmt.training - Epoch   7, Step:    28300, Batch Loss:     1.389790, Batch Acc: 0.577316, Tokens per Sec:     4415, Lr: 0.000300
2024-05-27 20:02:04,612 - INFO - joeynmt.training - Epoch   7, Step:    28400, Batch Loss:     1.303014, Batch Acc: 0.580383, Tokens per Sec:     4453, Lr: 0.000300
2024-05-27 20:02:21,401 - INFO - joeynmt.training - Epoch   7, Step:    28500, Batch Loss:     1.213211, Batch Acc: 0.583151, Tokens per Sec:     4237, Lr: 0.000300
2024-05-27 20:02:21,401 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:02:21,401 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:03:30,278 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.96, acc:   0.54, generation: 68.8701[sec], evaluation: 0.0000[sec]
2024-05-27 20:03:30,281 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:03:30,429 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/26500.ckpt
2024-05-27 20:03:30,429 - INFO - joeynmt.training - Example #0
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', '4@@', '8', 'percent', 'has', 're@@', 'stre@@', 't@@', 'ent', 'dimen@@', 'sion@@', 's', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the glacial glacial glacial glacial sizes of 48 million years had the United States 48 percent has restretent dimensions of 40 percent.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - Example #1
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - 	Hypothesis: And yet this undervalue of the problem because it doesn't show it show the same.
2024-05-27 20:03:30,430 - INFO - joeynmt.training - Example #2
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:03:30,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial climate is in a certain sense, the climate climate system.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - Example #3
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'all@@', ',', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Hypothesis: You expands of the wall, and you get right.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - Example #4
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:03:30,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:03:30,431 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid rapid on the last 25 years.
2024-05-27 20:03:46,212 - INFO - joeynmt.training - Epoch   7, Step:    28600, Batch Loss:     1.278819, Batch Acc: 0.578185, Tokens per Sec:     4588, Lr: 0.000300
2024-05-27 20:04:01,690 - INFO - joeynmt.training - Epoch   7, Step:    28700, Batch Loss:     1.382208, Batch Acc: 0.575967, Tokens per Sec:     4625, Lr: 0.000300
2024-05-27 20:04:17,082 - INFO - joeynmt.training - Epoch   7, Step:    28800, Batch Loss:     1.373258, Batch Acc: 0.577385, Tokens per Sec:     4619, Lr: 0.000300
2024-05-27 20:04:33,877 - INFO - joeynmt.training - Epoch   7, Step:    28900, Batch Loss:     1.297665, Batch Acc: 0.580895, Tokens per Sec:     4284, Lr: 0.000300
2024-05-27 20:04:49,953 - INFO - joeynmt.training - Epoch   7, Step:    29000, Batch Loss:     1.297020, Batch Acc: 0.579820, Tokens per Sec:     4448, Lr: 0.000300
2024-05-27 20:04:49,953 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:04:49,953 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:05:55,010 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.95, acc:   0.54, generation: 65.0497[sec], evaluation: 0.0000[sec]
2024-05-27 20:05:55,013 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:05:55,162 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/27000.ckpt
2024-05-27 20:05:55,163 - INFO - joeynmt.training - Example #0
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 't@@', 'ear@@', 's', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'z@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'ed', 'ed', 'St@@', 'at@@', 'es', 'for', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to show that the artical calculation glacial tears for almost three million years had the sizes of the United States in the United States in the United States of the United ed ed States for 40 percent.
2024-05-27 20:05:55,163 - INFO - joeynmt.training - Example #1
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:05:55,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'ess@@', 'or', 'of', 'ice', 'ice', '--', '</s>']
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:05:55,163 - INFO - joeynmt.training - 	Hypothesis: However, this subvalues the gravity of the problem because it doesn't show the sessor of ice ice --
2024-05-27 20:05:55,164 - INFO - joeynmt.training - Example #2
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', ',', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Hypothesis: The art glacial calot, is in a sense, the heart of the global climate system.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - Example #3
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'of', 'the', 'in@@', 'ver@@', 'se', 'and', 're@@', 'tur@@', 'n@@', 's', 'ou@@', 't.', '</s>']
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Hypothesis: You expand of the inverse and returns out.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - Example #4
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:05:55,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'to', 'be', 'a', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:05:55,164 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:05:55,165 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid to be a couple of the last 25 years.
2024-05-27 20:06:11,391 - INFO - joeynmt.training - Epoch   7, Step:    29100, Batch Loss:     1.427336, Batch Acc: 0.576427, Tokens per Sec:     4411, Lr: 0.000300
2024-05-27 20:06:28,083 - INFO - joeynmt.training - Epoch   7, Step:    29200, Batch Loss:     1.434015, Batch Acc: 0.577305, Tokens per Sec:     4310, Lr: 0.000300
2024-05-27 20:06:44,377 - INFO - joeynmt.training - Epoch   7, Step:    29300, Batch Loss:     1.398686, Batch Acc: 0.579761, Tokens per Sec:     4441, Lr: 0.000300
2024-05-27 20:07:00,086 - INFO - joeynmt.training - Epoch   7, Step:    29400, Batch Loss:     1.478057, Batch Acc: 0.575407, Tokens per Sec:     4461, Lr: 0.000300
2024-05-27 20:07:15,667 - INFO - joeynmt.training - Epoch   7, Step:    29500, Batch Loss:     1.374973, Batch Acc: 0.578606, Tokens per Sec:     4613, Lr: 0.000300
2024-05-27 20:07:15,667 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:07:15,667 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:08:38,040 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.91, acc:   0.54, generation: 82.3662[sec], evaluation: 0.0000[sec]
2024-05-27 20:08:38,043 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:08:38,193 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/27500.ckpt
2024-05-27 20:08:38,194 - INFO - joeynmt.training - Example #0
2024-05-27 20:08:38,194 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:08:38,194 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:08:38,194 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'c@@', 'alc@@', 'ul@@', 'ation', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 't@@', 'ar@@', 'tic@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to demonstrate that calculation calculation that the tartical calculus for almost three million years had had the size of the 48 percent.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - Example #1
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the same.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - Example #2
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - 	Hypothesis: The glacial climbing is in a certain sense, the heart climate system.
2024-05-27 20:08:38,195 - INFO - joeynmt.training - Example #3
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:08:38,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'you', 're@@', 'si@@', 've', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'ell', 'you', 'know@@', '.', '</s>']
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and you resive the summer and well you know.
2024-05-27 20:08:38,196 - INFO - joeynmt.training - Example #4
2024-05-27 20:08:38,196 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:08:38,196 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:08:38,196 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'to', 'be', 'a', 'co@@', 'ver@@', 'ed', 'd@@', 'ri@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:08:38,196 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid to be a covered dried on the last 25 years.
2024-05-27 20:08:54,313 - INFO - joeynmt.training - Epoch   7, Step:    29600, Batch Loss:     1.401322, Batch Acc: 0.578782, Tokens per Sec:     4416, Lr: 0.000300
2024-05-27 20:09:11,466 - INFO - joeynmt.training - Epoch   7, Step:    29700, Batch Loss:     1.445425, Batch Acc: 0.578817, Tokens per Sec:     4118, Lr: 0.000300
2024-05-27 20:09:27,920 - INFO - joeynmt.training - Epoch   7, Step:    29800, Batch Loss:     1.512184, Batch Acc: 0.576125, Tokens per Sec:     4210, Lr: 0.000300
2024-05-27 20:09:44,449 - INFO - joeynmt.training - Epoch   7, Step:    29900, Batch Loss:     1.411925, Batch Acc: 0.577702, Tokens per Sec:     4364, Lr: 0.000300
2024-05-27 20:10:01,965 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.216884, Batch Acc: 0.576334, Tokens per Sec:     4121, Lr: 0.000300
2024-05-27 20:10:01,967 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:10:01,967 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:11:12,515 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.93, acc:   0.54, generation: 70.5410[sec], evaluation: 0.0000[sec]
2024-05-27 20:11:12,659 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/28000.ckpt
2024-05-27 20:11:12,660 - INFO - joeynmt.training - Example #0
2024-05-27 20:11:12,660 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:11:12,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:11:12,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:11:12,660 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to demonstrate that the glacial calculation of the 48 dimensions of the 48 dimensions of the 48 States of the 48 percent.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - Example #1
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', 'ten@@', '.', '</s>']
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the often.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - Example #2
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'p@@', 'e', 'is', 'ar@@', 't@@', 'ic@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'c@@', 'lim@@', 'ate', 'system@@', ',', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Hypothesis: The artype is artical climbing climate system, the heart heart of global climate system.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - Example #3
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:11:12,661 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'su@@', 'm@@', 'm@@', 'm@@', 'er', 'and', 'the', 'su@@', 'm@@', 'm@@', 'er', 're@@', 'cor@@', 'd.', '</s>']
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:11:12,661 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:11:12,662 - INFO - joeynmt.training - 	Hypothesis: You expands of the summmer and the summer record.
2024-05-27 20:11:12,662 - INFO - joeynmt.training - Example #4
2024-05-27 20:11:12,662 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:11:12,662 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:11:12,662 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 're@@', 'li@@', 'e@@', 'f', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:11:12,662 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:11:12,662 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:11:12,662 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid relief on the last 25 years.
2024-05-27 20:11:28,974 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.435780, Batch Acc: 0.580277, Tokens per Sec:     4498, Lr: 0.000300
2024-05-27 20:11:46,214 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.546551, Batch Acc: 0.576648, Tokens per Sec:     4114, Lr: 0.000300
2024-05-27 20:12:03,226 - INFO - joeynmt.training - Epoch   7, Step:    30300, Batch Loss:     1.403744, Batch Acc: 0.574918, Tokens per Sec:     4245, Lr: 0.000300
2024-05-27 20:12:19,533 - INFO - joeynmt.training - Epoch   7, Step:    30400, Batch Loss:     1.590187, Batch Acc: 0.579171, Tokens per Sec:     4437, Lr: 0.000300
2024-05-27 20:12:36,027 - INFO - joeynmt.training - Epoch   7, Step:    30500, Batch Loss:     1.334075, Batch Acc: 0.577010, Tokens per Sec:     4387, Lr: 0.000300
2024-05-27 20:12:36,027 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:12:36,027 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:13:35,700 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.89, acc:   0.54, generation: 59.6660[sec], evaluation: 0.0000[sec]
2024-05-27 20:13:35,701 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:13:35,845 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/26000.ckpt
2024-05-27 20:13:35,846 - INFO - joeynmt.training - Example #0
2024-05-27 20:13:35,846 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:13:35,846 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:13:35,846 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'million', 'years', 'had', 'had', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'on', 'the', 'contin@@', 'ent@@', ',', 'is', 're@@', 'stre@@', 't@@', 'ed.', '</s>']
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide to demonstrate that the glacial glacial glacial dimensions of 48 million years had had the 48 States of the 48 percent of the United States on the continent, is restreted.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - Example #1
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'a', 'lot', 'of', 'this', 'sub@@', 'val@@', 'it@@', 'y,', 'because', 'it', "doesn't", 'show', 'the', 'ch@@', 'ar@@', 'ac@@', 'ter@@', '.', '</s>']
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Hypothesis: But this is a lot of this subvality, because it doesn't show the character.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - Example #2
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ly', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - 	Hypothesis: The artly glacial climbing is in a sense, the heart of global climate system.
2024-05-27 20:13:35,847 - INFO - joeynmt.training - Example #3
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:13:35,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'the', 'ex@@', 'tr@@', 'ac@@', 't@@', 'ed.', '</s>']
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Hypothesis: You expands of inversus and the extracted.
2024-05-27 20:13:35,848 - INFO - joeynmt.training - Example #4
2024-05-27 20:13:35,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:13:35,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:13:35,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:13:35,848 - INFO - joeynmt.training - 	Hypothesis: The next next slides will be a rapid carlled on the last 25 years.
2024-05-27 20:13:52,601 - INFO - joeynmt.training - Epoch   7, Step:    30600, Batch Loss:     1.423575, Batch Acc: 0.577105, Tokens per Sec:     4171, Lr: 0.000300
2024-05-27 20:14:09,172 - INFO - joeynmt.training - Epoch   7, Step:    30700, Batch Loss:     1.410046, Batch Acc: 0.568399, Tokens per Sec:     4297, Lr: 0.000300
2024-05-27 20:14:25,529 - INFO - joeynmt.training - Epoch   7, Step:    30800, Batch Loss:     1.479347, Batch Acc: 0.579829, Tokens per Sec:     4373, Lr: 0.000300
2024-05-27 20:14:41,301 - INFO - joeynmt.training - Epoch   7, Step:    30900, Batch Loss:     1.465362, Batch Acc: 0.583875, Tokens per Sec:     4714, Lr: 0.000300
2024-05-27 20:14:49,430 - INFO - joeynmt.training - Epoch   7: total training loss 6292.90
2024-05-27 20:14:49,430 - INFO - joeynmt.training - EPOCH 8
2024-05-27 20:14:57,024 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.208581, Batch Acc: 0.598375, Tokens per Sec:     4910, Lr: 0.000300
2024-05-27 20:14:57,024 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:14:57,024 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:16:04,956 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.54, generation: 67.9243[sec], evaluation: 0.0000[sec]
2024-05-27 20:16:04,958 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:16:05,109 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/28500.ckpt
2024-05-27 20:16:05,110 - INFO - joeynmt.training - Example #0
2024-05-27 20:16:05,110 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:16:05,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:16:05,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:16:05,110 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:16:05,110 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:16:05,110 - INFO - joeynmt.training - 	Hypothesis: I showed these last year I showed these slide to show that the glacial calculation for almost three million years had the sizes of 48 million years had the sizes of 48 percent of the United States of 40 percent.
2024-05-27 20:16:05,110 - INFO - joeynmt.training - Example #1
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'lot', 'of', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', 't@@', 'en', 'of', 'ice', 'ice', 'ice', 'ice', 'ice', 'as', 'ice', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'as', 'ice', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'f@@', 'at', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Hypothesis: This is a lot of this underestimate the problem because it doesn't show the often of ice ice ice ice ice as ice ice of ice ice ice ice as ice ice as ice as ice as ice as ice as ice as the ice of ice ice because it doesn't show the fat of the problem because it doesn't show it of the ice.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - Example #2
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Hypothesis: The art glacial climb is, in a sense, the heart of the global climate heart of the global climate system.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - Example #3
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:16:05,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'es', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:16:05,111 - INFO - joeynmt.training - 	Hypothesis: It is expands of inveries and you get right.
2024-05-27 20:16:05,112 - INFO - joeynmt.training - Example #4
2024-05-27 20:16:05,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:16:05,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:16:05,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'ast@@', 'ic', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:16:05,112 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:16:05,112 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:16:05,112 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fastic rapid on the last 25 years.
2024-05-27 20:16:22,142 - INFO - joeynmt.training - Epoch   8, Step:    31100, Batch Loss:     1.375420, Batch Acc: 0.594558, Tokens per Sec:     4217, Lr: 0.000300
2024-05-27 20:16:38,340 - INFO - joeynmt.training - Epoch   8, Step:    31200, Batch Loss:     1.234553, Batch Acc: 0.600760, Tokens per Sec:     4338, Lr: 0.000300
2024-05-27 20:16:54,272 - INFO - joeynmt.training - Epoch   8, Step:    31300, Batch Loss:     1.369406, Batch Acc: 0.597079, Tokens per Sec:     4478, Lr: 0.000300
2024-05-27 20:17:09,218 - INFO - joeynmt.training - Epoch   8, Step:    31400, Batch Loss:     1.368415, Batch Acc: 0.596417, Tokens per Sec:     4964, Lr: 0.000300
2024-05-27 20:17:24,989 - INFO - joeynmt.training - Epoch   8, Step:    31500, Batch Loss:     1.334902, Batch Acc: 0.599199, Tokens per Sec:     4462, Lr: 0.000300
2024-05-27 20:17:24,989 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:17:24,989 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:18:38,130 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.91, acc:   0.54, generation: 73.1340[sec], evaluation: 0.0000[sec]
2024-05-27 20:18:38,279 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/29000.ckpt
2024-05-27 20:18:38,280 - INFO - joeynmt.training - Example #0
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 't@@', 'ot@@', 'y@@', 'p@@', 'e,', 'which', 'for', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Hypothesis: I showed these slide last year I showed these slide to show that the artical totype, which for for almost three million years had had the size of 40 million years of 40 percent of the United States of 40 percent.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - Example #1
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'fre@@', 'e', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Hypothesis: But this is the free of the problem because it doesn't show it of the ice.
2024-05-27 20:18:38,281 - INFO - joeynmt.training - Example #2
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:18:38,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'ar@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:18:38,281 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Hypothesis: The art glacial art is, in a sense, the art of global climate system.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - Example #3
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'it', 'back', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Hypothesis: You expand it back and returns out the summer and you get right.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - Example #4
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:18:38,282 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:18:38,282 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid carlled on the last 25 years.
2024-05-27 20:18:54,020 - INFO - joeynmt.training - Epoch   8, Step:    31600, Batch Loss:     1.471361, Batch Acc: 0.595481, Tokens per Sec:     4473, Lr: 0.000300
2024-05-27 20:19:11,250 - INFO - joeynmt.training - Epoch   8, Step:    31700, Batch Loss:     1.492164, Batch Acc: 0.598407, Tokens per Sec:     4270, Lr: 0.000300
2024-05-27 20:19:26,933 - INFO - joeynmt.training - Epoch   8, Step:    31800, Batch Loss:     1.494579, Batch Acc: 0.588102, Tokens per Sec:     4591, Lr: 0.000300
2024-05-27 20:19:42,084 - INFO - joeynmt.training - Epoch   8, Step:    31900, Batch Loss:     1.206570, Batch Acc: 0.597764, Tokens per Sec:     4658, Lr: 0.000300
2024-05-27 20:19:57,142 - INFO - joeynmt.training - Epoch   8, Step:    32000, Batch Loss:     1.451768, Batch Acc: 0.595164, Tokens per Sec:     4897, Lr: 0.000300
2024-05-27 20:19:57,142 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:19:57,142 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:21:14,578 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.90, acc:   0.54, generation: 77.4286[sec], evaluation: 0.0000[sec]
2024-05-27 20:21:14,731 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/30000.ckpt
2024-05-27 20:21:14,732 - INFO - joeynmt.training - Example #0
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 's@@', 'li@@', 'de', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to show that the artical glacial slide that for almost three million years had the United States of 48 percent.
2024-05-27 20:21:14,732 - INFO - joeynmt.training - Example #1
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:21:14,732 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'f@@', 'lo@@', 'or', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of@@', ',', 'it', "doesn't", 'show', 'it', 'of', 'the', 'ice', 'ice', 'of', 'ice', 'ice', 'ice', '--', '</s>']
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:21:14,732 - INFO - joeynmt.training - 	Hypothesis: And this is the floor of the problem because it doesn't show it of, it doesn't show it of the ice ice of ice ice ice --
2024-05-27 20:21:14,732 - INFO - joeynmt.training - Example #2
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is, in a sense, the heart heart of global climate system.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - Example #3
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ed', 'in', 'the', 'su@@', 'm@@', 'er', 'and', 'you', 'get', 'back', 'to', 'the', 'su@@', 'm@@', 'er', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'you', 'get', 'back', 'to', 'the', 'su@@', 'm@@', 'er', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'you', 'get', 'back', 'of', 'su@@', 'm@@', 'm@@', 'm@@', 'er', 'and', 'you', 'get', 'back', 'of', 'su@@', 'm@@', 'm@@', 'er', 'and', 'and', 'you', 'get', 'back', 'to', 'the', 'su@@', 'm@@', 'est@@', '.', '</s>']
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Hypothesis: You expanded in the sumer and you get back to the sumer of the summer of the summer and you get back to the sumer of the summer and you get back of summmer and you get back of summer and and you get back to the sumest.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - Example #4
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:21:14,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:21:14,733 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 20:21:31,119 - INFO - joeynmt.training - Epoch   8, Step:    32100, Batch Loss:     1.212047, Batch Acc: 0.589569, Tokens per Sec:     4429, Lr: 0.000300
2024-05-27 20:21:46,423 - INFO - joeynmt.training - Epoch   8, Step:    32200, Batch Loss:     1.549728, Batch Acc: 0.586265, Tokens per Sec:     4697, Lr: 0.000300
2024-05-27 20:22:01,658 - INFO - joeynmt.training - Epoch   8, Step:    32300, Batch Loss:     1.283304, Batch Acc: 0.592496, Tokens per Sec:     4848, Lr: 0.000300
2024-05-27 20:22:18,254 - INFO - joeynmt.training - Epoch   8, Step:    32400, Batch Loss:     1.359063, Batch Acc: 0.590193, Tokens per Sec:     4216, Lr: 0.000300
2024-05-27 20:22:34,645 - INFO - joeynmt.training - Epoch   8, Step:    32500, Batch Loss:     1.477060, Batch Acc: 0.587805, Tokens per Sec:     4402, Lr: 0.000300
2024-05-27 20:22:34,645 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:22:34,646 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:23:38,754 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.89, acc:   0.54, generation: 64.1012[sec], evaluation: 0.0000[sec]
2024-05-27 20:23:38,900 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/29500.ckpt
2024-05-27 20:23:38,901 - INFO - joeynmt.training - Example #0
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'of', 'the', 'ar@@', 'tic@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'z@@', 'e,', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', '4@@', '0@@', '-@@', 'year@@', '-@@', 'St@@', 'at@@', 'es', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the type of the artical glacial glacial size, which for almost three million years has had the 40-year-States in the United States of 40 percent.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - Example #1
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'sub@@', 'val@@', 'ent', 'this', 'sub@@', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'it', 'of@@', 't@@', 'en', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - 	Hypothesis: This subvalent this subvalues the gravity because it doesn't show it often the same.
2024-05-27 20:23:38,901 - INFO - joeynmt.training - Example #2
2024-05-27 20:23:38,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us@@', ',', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'ical', 'system@@', '.', '</s>']
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Hypothesis: The art glacial calculus, in a sense, the heart heart of the global climatical system.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - Example #3
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'back', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Hypothesis: You expand the winter and you get back and returns out the summer.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - Example #4
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:23:38,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'ast@@', '-@@', 'h@@', 'ou@@', 'se@@', 'h@@', 'ol@@', 'ding', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:23:38,902 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fast-householding on the last 25 years.
2024-05-27 20:23:54,670 - INFO - joeynmt.training - Epoch   8, Step:    32600, Batch Loss:     1.257762, Batch Acc: 0.590654, Tokens per Sec:     4506, Lr: 0.000300
2024-05-27 20:24:10,664 - INFO - joeynmt.training - Epoch   8, Step:    32700, Batch Loss:     1.250186, Batch Acc: 0.591113, Tokens per Sec:     4511, Lr: 0.000300
2024-05-27 20:24:26,337 - INFO - joeynmt.training - Epoch   8, Step:    32800, Batch Loss:     1.316101, Batch Acc: 0.587575, Tokens per Sec:     4472, Lr: 0.000300
2024-05-27 20:24:42,865 - INFO - joeynmt.training - Epoch   8, Step:    32900, Batch Loss:     1.344770, Batch Acc: 0.592776, Tokens per Sec:     4298, Lr: 0.000300
2024-05-27 20:24:58,375 - INFO - joeynmt.training - Epoch   8, Step:    33000, Batch Loss:     1.304819, Batch Acc: 0.578408, Tokens per Sec:     4528, Lr: 0.000300
2024-05-27 20:24:58,376 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:24:58,376 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:26:10,266 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.89, acc:   0.54, generation: 71.8832[sec], evaluation: 0.0000[sec]
2024-05-27 20:26:10,419 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/31500.ckpt
2024-05-27 20:26:10,420 - INFO - joeynmt.training - Example #0
2024-05-27 20:26:10,420 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:26:10,420 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:26:10,420 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'year@@', 's,', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:26:10,420 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:26:10,420 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:26:10,420 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to demonstrate that the glacial calot, which for almost three million years, has had the size of 48 percent had the size of 40 percent.
2024-05-27 20:26:10,420 - INFO - joeynmt.training - Example #1
2024-05-27 20:26:10,420 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:26:10,420 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Hypothesis: This is the gravity of the gravity because it doesn't show the ice of the ice.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - Example #2
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'p@@', 'e', 'is', 'ar@@', 't@@', 'y', 'in', 'is', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'cu@@', 'ter', 'system@@', '.', '</s>']
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Hypothesis: The artype is arty in is a certain sense, the clean heart of the climate cuter system.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - Example #3
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:26:10,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'in', 'the', 'w@@', 'ver@@', 'su@@', 'm@@', 'er', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - 	Hypothesis: You expand in the wversumer and you get right.
2024-05-27 20:26:10,421 - INFO - joeynmt.training - Example #4
2024-05-27 20:26:10,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:26:10,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:26:10,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'co@@', 'up@@', 'le', 'of', 'the', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:26:10,422 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:26:10,422 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:26:10,422 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a couple of the 25 years.
2024-05-27 20:26:25,642 - INFO - joeynmt.training - Epoch   8, Step:    33100, Batch Loss:     1.410547, Batch Acc: 0.585400, Tokens per Sec:     4665, Lr: 0.000300
2024-05-27 20:26:41,127 - INFO - joeynmt.training - Epoch   8, Step:    33200, Batch Loss:     1.372234, Batch Acc: 0.594256, Tokens per Sec:     4785, Lr: 0.000300
2024-05-27 20:26:56,902 - INFO - joeynmt.training - Epoch   8, Step:    33300, Batch Loss:     1.389568, Batch Acc: 0.593156, Tokens per Sec:     4370, Lr: 0.000300
2024-05-27 20:27:12,034 - INFO - joeynmt.training - Epoch   8, Step:    33400, Batch Loss:     1.503800, Batch Acc: 0.588505, Tokens per Sec:     4747, Lr: 0.000300
2024-05-27 20:27:27,924 - INFO - joeynmt.training - Epoch   8, Step:    33500, Batch Loss:     1.395144, Batch Acc: 0.586436, Tokens per Sec:     4508, Lr: 0.000300
2024-05-27 20:27:27,925 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:27:27,925 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:28:40,661 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.54, generation: 72.7285[sec], evaluation: 0.0000[sec]
2024-05-27 20:28:40,663 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:28:40,811 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/32000.ckpt
2024-05-27 20:28:40,812 - INFO - joeynmt.training - Example #0
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'million', 'years', 'has', '4@@', '0', 'million', 'years', 'has', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to demonstrate that the glacial calculation that for almost three million years has had the size of 40 million years has 40 million years has 40 percent.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - Example #1
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'un@@', 'der', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of@@', 't@@', 'en', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Hypothesis: And this is under the gravity of the problem because it doesn't show the ice often because it doesn't show the ice of ice because it doesn't show the ice of ice ice because it doesn't show the ice of the ice.
2024-05-27 20:28:40,813 - INFO - joeynmt.training - Example #2
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:28:40,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b@@', 'ing', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:28:40,813 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Hypothesis: The art glacial climbing is in a sense, the climate heart of global climate system.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - Example #3
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'you', 'get', 'back', 'and', 're@@', 'tur@@', 'n@@', 's', 'out', 'to', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Hypothesis: You expand you get back and returns out to the summer.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - Example #4
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:28:40,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'd@@', 'd@@', 'le', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:28:40,814 - INFO - joeynmt.training - 	Hypothesis: The next slides will be a rapid carddle on the last 25 years.
2024-05-27 20:28:56,134 - INFO - joeynmt.training - Epoch   8, Step:    33600, Batch Loss:     1.348895, Batch Acc: 0.594122, Tokens per Sec:     4621, Lr: 0.000300
2024-05-27 20:29:11,297 - INFO - joeynmt.training - Epoch   8, Step:    33700, Batch Loss:     1.519457, Batch Acc: 0.583451, Tokens per Sec:     4442, Lr: 0.000300
2024-05-27 20:29:27,264 - INFO - joeynmt.training - Epoch   8, Step:    33800, Batch Loss:     1.319471, Batch Acc: 0.587856, Tokens per Sec:     4458, Lr: 0.000300
2024-05-27 20:29:42,459 - INFO - joeynmt.training - Epoch   8, Step:    33900, Batch Loss:     1.280897, Batch Acc: 0.592082, Tokens per Sec:     4638, Lr: 0.000300
2024-05-27 20:29:57,889 - INFO - joeynmt.training - Epoch   8, Step:    34000, Batch Loss:     1.575049, Batch Acc: 0.587493, Tokens per Sec:     4487, Lr: 0.000300
2024-05-27 20:29:57,889 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:29:57,889 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:31:09,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.54, generation: 71.3157[sec], evaluation: 0.0000[sec]
2024-05-27 20:31:09,214 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:31:09,360 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/32500.ckpt
2024-05-27 20:31:09,361 - INFO - joeynmt.training - Example #0
2024-05-27 20:31:09,361 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:31:09,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:31:09,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'lim@@', 'b@@', 'ing', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'z@@', 'es', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '0', 'million', 'years', 'has', 'been', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'by', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:31:09,361 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:31:09,361 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:31:09,361 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these slide to show that the climbing glacial glacial glacial sizes of 48 million years had the sizes of 40 million years has been restretched by 40 percent of the United States of the 40 percent.
2024-05-27 20:31:09,361 - INFO - joeynmt.training - Example #1
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'it', 'of@@', ',', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'or', 'ice', 'the', 'ice', 'of', 'ice', 'or', 'ice', 'the', 'ice', 'of', 'ice', '--', '</s>']
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show it of, it doesn't show the ice of ice or ice the ice of ice or ice the ice of ice --
2024-05-27 20:31:09,362 - INFO - joeynmt.training - Example #2
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'hear@@', 't', 'of', 'the', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'c@@', 'lim@@', 'ate', 'sy@@', 'ste@@', 'm', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Hypothesis: The artyal climb is, in a sense, the climate heart of the climate system climate system climate system.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - Example #3
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:31:09,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'it', 'back', 'in', 'the', 'w@@', 'ver@@', 'su@@', 'm@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'al@@', 'k@@', 's.', '</s>']
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - 	Hypothesis: You expand it back in the wversummmer and walks.
2024-05-27 20:31:09,362 - INFO - joeynmt.training - Example #4
2024-05-27 20:31:09,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:31:09,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:31:09,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 's@@', 'li@@', 'de', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:31:09,363 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:31:09,363 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:31:09,363 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick slide on the last 25 years.
2024-05-27 20:31:26,225 - INFO - joeynmt.training - Epoch   8, Step:    34100, Batch Loss:     1.379582, Batch Acc: 0.583182, Tokens per Sec:     4286, Lr: 0.000300
2024-05-27 20:31:42,096 - INFO - joeynmt.training - Epoch   8, Step:    34200, Batch Loss:     1.323711, Batch Acc: 0.593800, Tokens per Sec:     4510, Lr: 0.000300
2024-05-27 20:31:58,643 - INFO - joeynmt.training - Epoch   8, Step:    34300, Batch Loss:     1.463470, Batch Acc: 0.588045, Tokens per Sec:     4251, Lr: 0.000300
2024-05-27 20:32:14,548 - INFO - joeynmt.training - Epoch   8, Step:    34400, Batch Loss:     1.342058, Batch Acc: 0.588592, Tokens per Sec:     4330, Lr: 0.000300
2024-05-27 20:32:29,796 - INFO - joeynmt.training - Epoch   8, Step:    34500, Batch Loss:     1.322610, Batch Acc: 0.585487, Tokens per Sec:     4701, Lr: 0.000300
2024-05-27 20:32:29,797 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:32:29,797 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:33:43,362 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.83, acc:   0.55, generation: 73.5580[sec], evaluation: 0.0000[sec]
2024-05-27 20:33:43,365 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:33:43,515 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/33000.ckpt
2024-05-27 20:33:43,516 - INFO - joeynmt.training - Example #0
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'been', 'the', 'si@@', 'z@@', 'ed', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '8', 'percent', 'of', 'the']
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the glacial calculation that for almost three million years has had the size of 48 million years has been the sized 48 percent of the United States of 48 percent of the 40 percent of the United States of 40 percent of the United States of 48 percent of the 48 percent of the United States of the 48 percent of the
2024-05-27 20:33:43,517 - INFO - joeynmt.training - Example #1
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'si@@', 'l@@', 'ac@@', 'i@@', 'um@@', ',', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'as', 'the', 'ice', 'as', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'um@@', '.', '</s>']
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:33:43,517 - INFO - joeynmt.training - 	Hypothesis: However, this subsilacium, because it doesn't show the ice of ice because it doesn't show the ice of ice ice because it doesn't show the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice because it doesn't show the ice of ice ice ice ice because it doesn't show the ice of ice ice as the ice as the glacium.
2024-05-27 20:33:43,517 - INFO - joeynmt.training - Example #2
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:33:43,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an', 'ar@@', 't', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'ar', 'c@@', 'le@@', 'an', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Hypothesis: The art glacial clean art is, in a sense, the clear clean of the global climate system.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - Example #3
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'n@@', 's', 'out', 'the', 'w@@', 'inter@@', ',', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Hypothesis: It turns out the winter, and you get right.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - Example #4
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:33:43,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 're@@', 'cent@@', 'ly', 're@@', 'ven@@', 'u@@', 'es', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:33:43,518 - INFO - joeynmt.training - 	Hypothesis: The next slides will be a quick recently revenues of the last 25 years.
2024-05-27 20:33:59,505 - INFO - joeynmt.training - Epoch   8, Step:    34600, Batch Loss:     1.405938, Batch Acc: 0.589407, Tokens per Sec:     4521, Lr: 0.000300
2024-05-27 20:34:16,366 - INFO - joeynmt.training - Epoch   8, Step:    34700, Batch Loss:     1.300850, Batch Acc: 0.586310, Tokens per Sec:     4319, Lr: 0.000300
2024-05-27 20:34:32,005 - INFO - joeynmt.training - Epoch   8, Step:    34800, Batch Loss:     1.453631, Batch Acc: 0.585183, Tokens per Sec:     4676, Lr: 0.000300
2024-05-27 20:34:47,835 - INFO - joeynmt.training - Epoch   8, Step:    34900, Batch Loss:     1.598635, Batch Acc: 0.591167, Tokens per Sec:     4558, Lr: 0.000300
2024-05-27 20:35:02,726 - INFO - joeynmt.training - Epoch   8, Step:    35000, Batch Loss:     1.419136, Batch Acc: 0.587618, Tokens per Sec:     4761, Lr: 0.000300
2024-05-27 20:35:02,726 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:35:02,727 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:36:19,455 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.83, acc:   0.54, generation: 76.7214[sec], evaluation: 0.0000[sec]
2024-05-27 20:36:19,458 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:36:19,608 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/30500.ckpt
2024-05-27 20:36:19,609 - INFO - joeynmt.training - Example #0
2024-05-27 20:36:19,609 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:36:19,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:36:19,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'last', 'ye@@', 'ar', 'to', 'show', 'that', 'the', 't@@', 'ear@@', 's', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'us', 'for', 'three', 'million', 'years', 'of', 'had', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'mi@@', 'd@@', 'd@@', 'le', 'sc@@', 'ale', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:36:19,609 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:36:19,609 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:36:19,609 - INFO - joeynmt.training - 	Hypothesis: And I showed these slide last year to show that the tears that the type glacial calculus for three million years of had the United States of 48 percent of the United States of 40 percent of the United States of the middle scale of 40 percent.
2024-05-27 20:36:19,609 - INFO - joeynmt.training - Example #1
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'sub@@', 'm@@', 'm@@', 'it@@', 'ting', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'as', 'ice', 'as', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'k@@', 'in', 'this', 'g@@', 'l@@', 'ac@@', 'i@@', 'um@@', '.', '</s>']
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Hypothesis: And yet this submmitting the gravity of the problem because it doesn't show the ice of ice because it doesn't show the ice of ice ice as ice as the ice of ice because it doesn't show the skin this glacium.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - Example #2
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'p@@', 'e', 'is', 'ar@@', 't@@', 'en', 'ar@@', 't@@', 'ic', 'cal@@', 'ot@@', ',', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'of', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Hypothesis: The artype is arten artic calot, in a sense, the clean of the climate system.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - Example #3
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:36:19,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'rit@@', 'er@@', 's.', '</s>']
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - 	Hypothesis: You expanding of the summer and writers.
2024-05-27 20:36:19,610 - INFO - joeynmt.training - Example #4
2024-05-27 20:36:19,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:36:19,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:36:19,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'ast@@', '-@@', 'ra@@', 'pi@@', 'd', 's@@', 'li@@', 'de@@', 's', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:36:19,611 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:36:19,611 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:36:19,611 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fast-rapid slides of the last 25 years.
2024-05-27 20:36:35,236 - INFO - joeynmt.training - Epoch   8, Step:    35100, Batch Loss:     1.291158, Batch Acc: 0.588535, Tokens per Sec:     4499, Lr: 0.000300
2024-05-27 20:36:50,007 - INFO - joeynmt.training - Epoch   8, Step:    35200, Batch Loss:     1.382345, Batch Acc: 0.578513, Tokens per Sec:     4654, Lr: 0.000300
2024-05-27 20:37:06,236 - INFO - joeynmt.training - Epoch   8, Step:    35300, Batch Loss:     1.322156, Batch Acc: 0.591655, Tokens per Sec:     4452, Lr: 0.000300
2024-05-27 20:37:18,390 - INFO - joeynmt.training - Epoch   8: total training loss 6148.50
2024-05-27 20:37:18,390 - INFO - joeynmt.training - EPOCH 9
2024-05-27 20:37:21,898 - INFO - joeynmt.training - Epoch   9, Step:    35400, Batch Loss:     1.276370, Batch Acc: 0.609643, Tokens per Sec:     4316, Lr: 0.000300
2024-05-27 20:37:38,692 - INFO - joeynmt.training - Epoch   9, Step:    35500, Batch Loss:     1.258009, Batch Acc: 0.607762, Tokens per Sec:     4233, Lr: 0.000300
2024-05-27 20:37:38,692 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:37:38,692 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:38:50,224 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.82, acc:   0.55, generation: 71.5244[sec], evaluation: 0.0000[sec]
2024-05-27 20:38:50,225 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:38:50,374 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/31000.ckpt
2024-05-27 20:38:50,375 - INFO - joeynmt.training - Example #0
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', '--', 'that', 'for', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'on', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'U@@', '.@@', 'S@@', '.', '</s>']
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to show that the glacial glacial glacial -- that for for almost three million years had had the size of 48 million years of the United States on the size of 40 percent of the United States of the U.S.
2024-05-27 20:38:50,375 - INFO - joeynmt.training - Example #1
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:38:50,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'sub@@', 'st@@', 'anti@@', 'al', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'the', 'f@@', 'all@@', '.', '</s>']
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:38:50,375 - INFO - joeynmt.training - 	Hypothesis: And yet this substantial gravity of the problem because it doesn't show the ice of ice because it doesn't show the ice of ice ice ice as ice as ice as ice as the fall.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - Example #2
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ica', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'system@@', ',', '</s>']
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Hypothesis: The arty glacial artica is, in a certain sense, the climate system,
2024-05-27 20:38:50,376 - INFO - joeynmt.training - Example #3
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'es', 'and', 'you', 're@@', 'tur@@', 'n', 'out', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'rit@@', 'er@@', 's.', '</s>']
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Hypothesis: And you expands of inveries and you return out of the summer and writers.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - Example #4
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:38:50,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 're@@', 'ven@@', 'u@@', 'es', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:38:50,376 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:38:50,377 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid of revenues on the last 25 years.
2024-05-27 20:39:05,995 - INFO - joeynmt.training - Epoch   9, Step:    35600, Batch Loss:     1.439068, Batch Acc: 0.610916, Tokens per Sec:     4560, Lr: 0.000300
2024-05-27 20:39:22,320 - INFO - joeynmt.training - Epoch   9, Step:    35700, Batch Loss:     1.378712, Batch Acc: 0.611179, Tokens per Sec:     4326, Lr: 0.000300
2024-05-27 20:39:39,723 - INFO - joeynmt.training - Epoch   9, Step:    35800, Batch Loss:     1.207685, Batch Acc: 0.610028, Tokens per Sec:     4098, Lr: 0.000300
2024-05-27 20:39:55,579 - INFO - joeynmt.training - Epoch   9, Step:    35900, Batch Loss:     1.433374, Batch Acc: 0.606061, Tokens per Sec:     4682, Lr: 0.000300
2024-05-27 20:40:11,519 - INFO - joeynmt.training - Epoch   9, Step:    36000, Batch Loss:     1.345750, Batch Acc: 0.606190, Tokens per Sec:     4366, Lr: 0.000300
2024-05-27 20:40:11,520 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:40:11,520 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:41:17,203 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.54, generation: 65.6759[sec], evaluation: 0.0000[sec]
2024-05-27 20:41:17,204 - INFO - joeynmt.training - Example #0
2024-05-27 20:41:17,204 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:41:17,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:41:17,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 's@@', 'li@@', 'de', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'ar@@', 'tic@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 're@@', 'e@@', 'f@@', '.', '</s>']
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Hypothesis: And the year I showed these slide to show that the calculation slide that the calculation of artic, which for almost three million years had the size of 40 percent of the United States of 40 percent reef.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - Example #1
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'y@@', 'et', 'this', 'sub@@', 'st@@', 'anti@@', 'al', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', '--', '</s>']
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Hypothesis: And yet this substantial of the problem because it doesn't show the ice of ice because it doesn't show the ice of ice --
2024-05-27 20:41:17,205 - INFO - joeynmt.training - Example #2
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'p@@', 'e', 'is', 'ar@@', 't@@', 'ic', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - 	Hypothesis: The artype is artic calculation is in a sense, the clean climate system.
2024-05-27 20:41:17,205 - INFO - joeynmt.training - Example #3
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:41:17,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'in@@', 'd', 'and', 're@@', 'tur@@', 'n', 'out', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 're@@', 'si@@', 'd.', '</s>']
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Hypothesis: You expands and wind and return out of the summer and resid.
2024-05-27 20:41:17,206 - INFO - joeynmt.training - Example #4
2024-05-27 20:41:17,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:41:17,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:41:17,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 're@@', 's@@', 'ed', 'on', 'the', 'ent@@', 'er@@', '-@@', 'to@@', '-@@', '2@@', '5', 'year@@', '-@@', 'old', 's@@', 'li@@', 'de@@', 's.', '</s>']
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:41:17,206 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid of resed on the enter-to-25 year-old slides.
2024-05-27 20:41:32,986 - INFO - joeynmt.training - Epoch   9, Step:    36100, Batch Loss:     1.343295, Batch Acc: 0.600159, Tokens per Sec:     4552, Lr: 0.000300
2024-05-27 20:41:50,257 - INFO - joeynmt.training - Epoch   9, Step:    36200, Batch Loss:     1.360466, Batch Acc: 0.603061, Tokens per Sec:     4184, Lr: 0.000300
2024-05-27 20:42:07,615 - INFO - joeynmt.training - Epoch   9, Step:    36300, Batch Loss:     1.354416, Batch Acc: 0.598844, Tokens per Sec:     4047, Lr: 0.000300
2024-05-27 20:42:24,754 - INFO - joeynmt.training - Epoch   9, Step:    36400, Batch Loss:     1.433068, Batch Acc: 0.595510, Tokens per Sec:     4145, Lr: 0.000300
2024-05-27 20:42:40,431 - INFO - joeynmt.training - Epoch   9, Step:    36500, Batch Loss:     1.411072, Batch Acc: 0.601945, Tokens per Sec:     4460, Lr: 0.000300
2024-05-27 20:42:40,432 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:42:40,432 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:43:55,361 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.82, acc:   0.54, generation: 74.9211[sec], evaluation: 0.0000[sec]
2024-05-27 20:43:55,521 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/33500.ckpt
2024-05-27 20:43:55,522 - INFO - joeynmt.training - Example #0
2024-05-27 20:43:55,522 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:43:55,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:43:55,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 't@@', 'ear@@', 's', 'of', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'U@@', '.@@', 'S@@', '.', '</s>']
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to show that the artical calculation that the tears of artical artic, which for almost three million years of the United States of 40 percent of the United States of 40 percent of the U.S.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - Example #1
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'est@@', 'er@@', 'n', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - 	Hypothesis: However, this underestern gravity because it doesn't show the gravity because it doesn't show the ice of the ice because it doesn't show the ice of the ice because it doesn't show the gravity of the ice.
2024-05-27 20:43:55,523 - INFO - joeynmt.training - Example #2
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:43:55,523 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Hypothesis: The art glacial climb is, in a sense, the clean climate system.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - Example #3
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'the', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - 	Hypothesis: It expands of the winter and the right.
2024-05-27 20:43:55,524 - INFO - joeynmt.training - Example #4
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:43:55,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:43:55,525 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:43:55,525 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:43:55,525 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 20:44:11,601 - INFO - joeynmt.training - Epoch   9, Step:    36600, Batch Loss:     1.228126, Batch Acc: 0.601319, Tokens per Sec:     4463, Lr: 0.000300
2024-05-27 20:44:26,953 - INFO - joeynmt.training - Epoch   9, Step:    36700, Batch Loss:     1.338713, Batch Acc: 0.601895, Tokens per Sec:     4785, Lr: 0.000300
2024-05-27 20:44:42,834 - INFO - joeynmt.training - Epoch   9, Step:    36800, Batch Loss:     1.518991, Batch Acc: 0.598639, Tokens per Sec:     4496, Lr: 0.000300
2024-05-27 20:45:00,008 - INFO - joeynmt.training - Epoch   9, Step:    36900, Batch Loss:     1.372135, Batch Acc: 0.601710, Tokens per Sec:     4141, Lr: 0.000300
2024-05-27 20:45:16,327 - INFO - joeynmt.training - Epoch   9, Step:    37000, Batch Loss:     1.472081, Batch Acc: 0.600543, Tokens per Sec:     4313, Lr: 0.000300
2024-05-27 20:45:16,328 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:45:16,328 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:46:19,617 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.55, generation: 63.2819[sec], evaluation: 0.0000[sec]
2024-05-27 20:46:19,618 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:46:19,765 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/34000.ckpt
2024-05-27 20:46:19,766 - INFO - joeynmt.training - Example #0
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'y@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'y@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', '</s>']
2024-05-27 20:46:19,766 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:46:19,766 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:46:19,766 - INFO - joeynmt.training - 	Hypothesis: I showed these last year I showed these slide to demonstrate that the cylacial cylacial size of 48 million years has had the size of 40 percent has had the size of 40 percent of the United States of 40 percent
2024-05-27 20:46:19,766 - INFO - joeynmt.training - Example #1
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:46:19,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'a', 'f@@', 'at', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', 'the', 'be@@', 'au@@', 'ti@@', 'ful', 'of', 'ice', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Hypothesis: But this is a fat the gravity of the problem because it doesn't show the of, the beautiful of ice the ice.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - Example #2
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'c@@', 'y@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'y@@', 'c@@', 'le', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Hypothesis: The art cylacial cycle is in a sense, the clean of the global climate system.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - Example #3
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - 	Hypothesis: You expands of the summer.
2024-05-27 20:46:19,767 - INFO - joeynmt.training - Example #4
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:46:19,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:46:19,768 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:46:19,768 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:46:19,768 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick of the last 25 years.
2024-05-27 20:46:35,186 - INFO - joeynmt.training - Epoch   9, Step:    37100, Batch Loss:     1.309655, Batch Acc: 0.599786, Tokens per Sec:     4563, Lr: 0.000300
2024-05-27 20:46:51,710 - INFO - joeynmt.training - Epoch   9, Step:    37200, Batch Loss:     1.351645, Batch Acc: 0.596548, Tokens per Sec:     4194, Lr: 0.000300
2024-05-27 20:47:08,282 - INFO - joeynmt.training - Epoch   9, Step:    37300, Batch Loss:     1.470128, Batch Acc: 0.601902, Tokens per Sec:     4284, Lr: 0.000300
2024-05-27 20:47:24,484 - INFO - joeynmt.training - Epoch   9, Step:    37400, Batch Loss:     1.202542, Batch Acc: 0.597768, Tokens per Sec:     4304, Lr: 0.000300
2024-05-27 20:47:39,891 - INFO - joeynmt.training - Epoch   9, Step:    37500, Batch Loss:     1.394390, Batch Acc: 0.604230, Tokens per Sec:     4760, Lr: 0.000300
2024-05-27 20:47:39,892 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:47:39,892 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:48:49,730 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.79, acc:   0.55, generation: 69.8309[sec], evaluation: 0.0000[sec]
2024-05-27 20:48:49,731 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:48:49,879 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/34500.ckpt
2024-05-27 20:48:49,880 - INFO - joeynmt.training - Example #0
2024-05-27 20:48:49,880 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:48:49,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'ear@@', 's', 'that', 'the', 't@@', 'ear@@', 's', 'of', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'al', 'h@@', 'o@@', 't', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'has', 'had', 'f@@', 'our', 'to', 'have', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide to show that the tears that the tears of artical artical hot for almost three million years has had the size of 48 percent has had four to have 40 percent of the 40 percent of the 40 percent of the United 40 percent.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - Example #1
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'f@@', 'all@@', 'ing', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'because', 'of', 'the', 'ice', 'ice', 'ice', '--', '</s>']
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Hypothesis: But this is the falling gravity of the problem because it doesn't show the ice of ice because of the ice ice ice --
2024-05-27 20:48:49,881 - INFO - joeynmt.training - Example #2
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:48:49,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic@@', 'al', 'c@@', 'lim@@', 'b', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - 	Hypothesis: The artical climb is in a sense, the clean heart of the global climate system.
2024-05-27 20:48:49,881 - INFO - joeynmt.training - Example #3
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'it', 'ou@@', 't,', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Hypothesis: You expand it out, and you get right.
2024-05-27 20:48:49,882 - INFO - joeynmt.training - Example #4
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:48:49,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'y', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:48:49,882 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fasty carlled on the last 25 years.
2024-05-27 20:49:06,051 - INFO - joeynmt.training - Epoch   9, Step:    37600, Batch Loss:     1.364504, Batch Acc: 0.594580, Tokens per Sec:     4495, Lr: 0.000300
2024-05-27 20:49:21,803 - INFO - joeynmt.training - Epoch   9, Step:    37700, Batch Loss:     1.438966, Batch Acc: 0.599846, Tokens per Sec:     4609, Lr: 0.000300
2024-05-27 20:49:37,537 - INFO - joeynmt.training - Epoch   9, Step:    37800, Batch Loss:     1.268338, Batch Acc: 0.602719, Tokens per Sec:     4572, Lr: 0.000300
2024-05-27 20:49:53,049 - INFO - joeynmt.training - Epoch   9, Step:    37900, Batch Loss:     1.239283, Batch Acc: 0.599742, Tokens per Sec:     4751, Lr: 0.000300
2024-05-27 20:50:09,267 - INFO - joeynmt.training - Epoch   9, Step:    38000, Batch Loss:     1.315492, Batch Acc: 0.595388, Tokens per Sec:     4372, Lr: 0.000300
2024-05-27 20:50:09,267 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:50:09,267 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:51:28,364 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.78, acc:   0.55, generation: 79.0902[sec], evaluation: 0.0000[sec]
2024-05-27 20:51:28,366 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:51:28,517 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/35000.ckpt
2024-05-27 20:51:28,518 - INFO - joeynmt.training - Example #0
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 't@@', 'ear@@', 's', 'that', 'the', 't@@', 'ear@@', 's', 'of', 'ar@@', 'tic@@', 'al', 't@@', 'y@@', 'p@@', 'e', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'has', 'had', 're@@', 'stre@@', 't@@', 'en', 'in', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 't@@', 'ear@@', 'ly', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'year@@', 's.', '</s>']
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to show that the tears that the tears of artical type of 48 million years has had had the size of 48 percent has had restreten in the United States of 40 percent of the 40 percent of the United States of 40 percent of the tearly glacial years.
2024-05-27 20:51:28,518 - INFO - joeynmt.training - Example #1
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'li@@', 'p@@', 's', 'of', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', '--', '</s>']
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:51:28,518 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the slips of ice because it doesn't show the ice of the ice --
2024-05-27 20:51:28,518 - INFO - joeynmt.training - Example #2
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:51:28,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 't@@', 'y@@', 'pic@@', 'al', 'c@@', 'lim@@', 'b', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Hypothesis: The typical climb is, in a sense, the clean heart heart heart of the global climate system.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - Example #3
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 're@@', 'tur@@', 'n', 'out', 'of', 'su@@', 'm@@', 'm@@', 'er', 'and', 're@@', 'tur@@', 'n@@', 's.', '</s>']
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Hypothesis: You expands of the summer and return out of summer and returns.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - Example #4
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:51:28,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'co@@', 'up@@', 'le', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:51:28,519 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a couple of the last 25 years.
2024-05-27 20:51:44,154 - INFO - joeynmt.training - Epoch   9, Step:    38100, Batch Loss:     1.299059, Batch Acc: 0.587973, Tokens per Sec:     4325, Lr: 0.000300
2024-05-27 20:51:59,978 - INFO - joeynmt.training - Epoch   9, Step:    38200, Batch Loss:     1.517283, Batch Acc: 0.596701, Tokens per Sec:     4595, Lr: 0.000300
2024-05-27 20:52:16,675 - INFO - joeynmt.training - Epoch   9, Step:    38300, Batch Loss:     1.254693, Batch Acc: 0.597109, Tokens per Sec:     4160, Lr: 0.000300
2024-05-27 20:52:33,119 - INFO - joeynmt.training - Epoch   9, Step:    38400, Batch Loss:     1.446052, Batch Acc: 0.594736, Tokens per Sec:     4323, Lr: 0.000300
2024-05-27 20:52:48,823 - INFO - joeynmt.training - Epoch   9, Step:    38500, Batch Loss:     1.410123, Batch Acc: 0.592887, Tokens per Sec:     4525, Lr: 0.000300
2024-05-27 20:52:48,823 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:52:48,823 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:53:54,347 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.55, generation: 65.5172[sec], evaluation: 0.0000[sec]
2024-05-27 20:53:54,348 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:53:54,499 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/36500.ckpt
2024-05-27 20:53:54,500 - INFO - joeynmt.training - Example #0
2024-05-27 20:53:54,500 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:53:54,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:53:54,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'been', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:53:54,500 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:53:54,500 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to show that the glacial calculation that for almost three million years had been the size of 48 million years had the size of 48 percent of the United States of 40 percent.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - Example #1
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', 'the', 'be@@', 'au@@', 'ti@@', 'ful', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', '</s>']
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Hypothesis: However, this is the gravity of the problem because it doesn't show the of, the beautiful because it doesn't show the of,
2024-05-27 20:53:54,501 - INFO - joeynmt.training - Example #2
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - 	Hypothesis: The art glacial art is, in a certain sense, the clean heart heart of global climate system.
2024-05-27 20:53:54,501 - INFO - joeynmt.training - Example #3
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:53:54,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'get', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'in@@', 'd@@', 'l@@', 'es.', '</s>']
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Hypothesis: You go and get the summer and windles.
2024-05-27 20:53:54,502 - INFO - joeynmt.training - Example #4
2024-05-27 20:53:54,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:53:54,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:53:54,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:53:54,502 - INFO - joeynmt.training - 	Hypothesis: The next next slides will be a rapid carlled on the last 25 years.
2024-05-27 20:54:10,018 - INFO - joeynmt.training - Epoch   9, Step:    38600, Batch Loss:     1.402206, Batch Acc: 0.592538, Tokens per Sec:     4589, Lr: 0.000300
2024-05-27 20:54:25,665 - INFO - joeynmt.training - Epoch   9, Step:    38700, Batch Loss:     1.275731, Batch Acc: 0.594624, Tokens per Sec:     4537, Lr: 0.000300
2024-05-27 20:54:41,097 - INFO - joeynmt.training - Epoch   9, Step:    38800, Batch Loss:     1.311931, Batch Acc: 0.596851, Tokens per Sec:     4523, Lr: 0.000300
2024-05-27 20:54:56,067 - INFO - joeynmt.training - Epoch   9, Step:    38900, Batch Loss:     1.417444, Batch Acc: 0.593275, Tokens per Sec:     4657, Lr: 0.000300
2024-05-27 20:55:11,954 - INFO - joeynmt.training - Epoch   9, Step:    39000, Batch Loss:     1.399983, Batch Acc: 0.589946, Tokens per Sec:     4500, Lr: 0.000300
2024-05-27 20:55:11,954 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:55:11,955 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:56:23,351 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.76, acc:   0.55, generation: 71.3892[sec], evaluation: 0.0000[sec]
2024-05-27 20:56:23,352 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:56:23,509 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/35500.ckpt
2024-05-27 20:56:23,509 - INFO - joeynmt.training - Example #0
2024-05-27 20:56:23,509 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:56:23,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'y@@', 'n@@', 'al', 't@@', 'ear@@', 's', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'had', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', 'the', 'last', 'ye@@', 'ar', '--', '</s>']
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to demonstrate that the tynal tears for almost three million years had the size of 48 million years had the United States of 48 percent of the United States of 40 percent of the United States of the 40 percent of the United States of the last year --
2024-05-27 20:56:23,510 - INFO - joeynmt.training - Example #1
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'this', 'sub@@', 'val@@', 't', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'as', 'ice', 'as', 'ice', 'as', 'w@@', 'ell@@', '.', '</s>']
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Hypothesis: But this underestimate this subvalt is the gravity of the problem because it doesn't show the ice of ice as ice as ice as well.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - Example #2
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:56:23,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 't@@', 'ear@@', 'ly', 'ar@@', 't', 'is,', 'in', 'a', 'way,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'cu@@', 're', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - 	Hypothesis: The art tearly art is, in a way, the clean heart of the global climate cure of the global climate system.
2024-05-27 20:56:23,510 - INFO - joeynmt.training - Example #3
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'w@@', 'in@@', 'n@@', 'er', 're@@', 'co@@', 'gn@@', 'i@@', 'z@@', 'ed.', '</s>']
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Hypothesis: You expands of the winter and winner recognized.
2024-05-27 20:56:23,511 - INFO - joeynmt.training - Example #4
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:56:23,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'ra@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:56:23,511 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid rapid on the last 25 years.
2024-05-27 20:56:39,396 - INFO - joeynmt.training - Epoch   9, Step:    39100, Batch Loss:     1.328616, Batch Acc: 0.596513, Tokens per Sec:     4490, Lr: 0.000300
2024-05-27 20:56:54,959 - INFO - joeynmt.training - Epoch   9, Step:    39200, Batch Loss:     1.306287, Batch Acc: 0.600758, Tokens per Sec:     4680, Lr: 0.000300
2024-05-27 20:57:12,111 - INFO - joeynmt.training - Epoch   9, Step:    39300, Batch Loss:     1.250265, Batch Acc: 0.589737, Tokens per Sec:     4223, Lr: 0.000300
2024-05-27 20:57:28,633 - INFO - joeynmt.training - Epoch   9, Step:    39400, Batch Loss:     1.349859, Batch Acc: 0.591906, Tokens per Sec:     4376, Lr: 0.000300
2024-05-27 20:57:44,236 - INFO - joeynmt.training - Epoch   9, Step:    39500, Batch Loss:     1.564562, Batch Acc: 0.599272, Tokens per Sec:     4698, Lr: 0.000300
2024-05-27 20:57:44,236 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 20:57:44,236 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 20:58:55,951 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.73, acc:   0.55, generation: 71.7073[sec], evaluation: 0.0000[sec]
2024-05-27 20:58:55,952 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 20:58:56,099 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/37000.ckpt
2024-05-27 20:58:56,100 - INFO - joeynmt.training - Example #0
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'ear@@', 's', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'm', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'has', 'had', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to demonstrate that the tears glacial calm that for almost three million years had had the size of 48 percent had the size of 40 percent has had the 40 percent.
2024-05-27 20:58:56,100 - INFO - joeynmt.training - Example #1
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 20:58:56,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'a', 'lot', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'of@@', 'ten@@', ',', 'of@@', ',', 'it', "doesn't", 'show', 'the', 'of@@', ',', '</s>']
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 20:58:56,100 - INFO - joeynmt.training - 	Hypothesis: But this is a lot of the gravity of the problem because it doesn't show often, of, it doesn't show the of,
2024-05-27 20:58:56,100 - INFO - joeynmt.training - Example #2
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'al', 'c@@', 'lim@@', 'b@@', 's', 'is,', 'in', 'a', 'way,', 'the', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Hypothesis: The artyal climbs is, in a way, the certain sense, the heart of the global climate system.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - Example #3
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'es', 'and', 'I', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - 	Hypothesis: You expands of inveries and I get right.
2024-05-27 20:58:56,101 - INFO - joeynmt.training - Example #4
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 20:58:56,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 20:58:56,102 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 20:58:56,102 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 20:58:56,102 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick carlled on the last 25 years.
2024-05-27 20:59:12,876 - INFO - joeynmt.training - Epoch   9, Step:    39600, Batch Loss:     1.346872, Batch Acc: 0.589432, Tokens per Sec:     4174, Lr: 0.000300
2024-05-27 20:59:29,860 - INFO - joeynmt.training - Epoch   9, Step:    39700, Batch Loss:     1.217419, Batch Acc: 0.597977, Tokens per Sec:     4348, Lr: 0.000300
2024-05-27 20:59:45,375 - INFO - joeynmt.training - Epoch   9, Step:    39800, Batch Loss:     1.380583, Batch Acc: 0.598245, Tokens per Sec:     4673, Lr: 0.000300
2024-05-27 20:59:45,988 - INFO - joeynmt.training - Epoch   9: total training loss 6009.90
2024-05-27 20:59:45,988 - INFO - joeynmt.training - EPOCH 10
2024-05-27 21:00:01,371 - INFO - joeynmt.training - Epoch  10, Step:    39900, Batch Loss:     1.201063, Batch Acc: 0.615562, Tokens per Sec:     4422, Lr: 0.000300
2024-05-27 21:00:17,544 - INFO - joeynmt.training - Epoch  10, Step:    40000, Batch Loss:     1.192679, Batch Acc: 0.618976, Tokens per Sec:     4529, Lr: 0.000300
2024-05-27 21:00:17,545 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:00:17,545 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:01:21,723 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.74, acc:   0.55, generation: 64.1713[sec], evaluation: 0.0000[sec]
2024-05-27 21:01:21,876 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/37500.ckpt
2024-05-27 21:01:21,877 - INFO - joeynmt.training - Example #0
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'of', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'al', 't@@', 'ear@@', 's', 'for', 'al@@', 'most', 'three', 'million', 'years', 'had', 'had', 'the', 'si@@', 'z@@', 'es', 'of', '4@@', '0', 'million', 'years', 'has', 'had', 'the', 'si@@', 'z@@', 'e.', '</s>']
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to demonstrate that the type of artical artical tears for almost three million years had had the sizes of 40 million years has had the size.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - Example #1
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'st@@', 'it@@', 'ut@@', 's', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - 	Hypothesis: But this substituts the gravity of the problem because it doesn't show the ice of the ice.
2024-05-27 21:01:21,877 - INFO - joeynmt.training - Example #2
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:01:21,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'al', 'c@@', 'lim@@', 'b@@', 's', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Hypothesis: The artyal climbs is, in a certain sense, the heart of the global climate system.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - Example #3
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'it', 'from', 'the', 'w@@', 'ver@@', 'su@@', 'm@@', 'er', 'and', 'I', 're@@', 'tur@@', 'ned', 'out', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Hypothesis: You expand it from the wversumer and I returned out of the summer.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - Example #4
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:01:21,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 're@@', 'pi@@', 'd', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:01:21,878 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick repid on the last 25 years.
2024-05-27 21:01:37,701 - INFO - joeynmt.training - Epoch  10, Step:    40100, Batch Loss:     1.425388, Batch Acc: 0.619553, Tokens per Sec:     4486, Lr: 0.000300
2024-05-27 21:01:53,540 - INFO - joeynmt.training - Epoch  10, Step:    40200, Batch Loss:     1.161477, Batch Acc: 0.611885, Tokens per Sec:     4450, Lr: 0.000300
2024-05-27 21:02:09,542 - INFO - joeynmt.training - Epoch  10, Step:    40300, Batch Loss:     1.330903, Batch Acc: 0.607841, Tokens per Sec:     4536, Lr: 0.000300
2024-05-27 21:02:25,234 - INFO - joeynmt.training - Epoch  10, Step:    40400, Batch Loss:     1.474619, Batch Acc: 0.619764, Tokens per Sec:     4589, Lr: 0.000300
2024-05-27 21:02:41,453 - INFO - joeynmt.training - Epoch  10, Step:    40500, Batch Loss:     1.360205, Batch Acc: 0.612979, Tokens per Sec:     4417, Lr: 0.000300
2024-05-27 21:02:41,453 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:02:41,453 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:04:00,097 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.75, acc:   0.55, generation: 78.6363[sec], evaluation: 0.0000[sec]
2024-05-27 21:04:00,246 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/38000.ckpt
2024-05-27 21:04:00,247 - INFO - joeynmt.training - Example #0
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year@@', ',', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'y@@', 'li@@', 'an', 'ar@@', 'tic@@', 'al', 't@@', 'ear@@', 's', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'has', 'had', 'the', '4@@', '8', 'percent', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'percent', 'has', 'had', 'the', '4@@', '0', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es.', '</s>']
2024-05-27 21:04:00,247 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:04:00,247 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:04:00,247 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slide to show that the cylian artical tears for almost three million years has had had the size of 48 percent has had the 48 percent has had the size of 40 percent has had the 40 percent of the United States.
2024-05-27 21:04:00,247 - INFO - joeynmt.training - Example #1
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:04:00,247 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'this', 'sub@@', 'val@@', 'it@@', 'y,', 'because', 'it', "doesn't", 'show', 'it', 'of@@', ',', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'ice', '--', '</s>']
2024-05-27 21:04:00,247 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Hypothesis: But this underestimate this subvality, because it doesn't show it of, because it doesn't show the ice of ice ice --
2024-05-27 21:04:00,248 - INFO - joeynmt.training - Example #2
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 't', 'is,', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Hypothesis: The art glacial art is, in a certain sense, the climate climate system.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - Example #3
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Hypothesis: And it expands of the summer.
2024-05-27 21:04:00,248 - INFO - joeynmt.training - Example #4
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:04:00,248 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast@@', 'er', 'on', 'the', 'f@@', 'ast@@', 'er', 'be@@', 'ing@@', 's', 'of', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:04:00,248 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:04:00,249 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:04:00,249 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a faster on the faster beings of the last 25 years.
2024-05-27 21:04:16,490 - INFO - joeynmt.training - Epoch  10, Step:    40600, Batch Loss:     1.195922, Batch Acc: 0.609581, Tokens per Sec:     4427, Lr: 0.000300
2024-05-27 21:04:33,846 - INFO - joeynmt.training - Epoch  10, Step:    40700, Batch Loss:     1.213902, Batch Acc: 0.608682, Tokens per Sec:     4276, Lr: 0.000300
2024-05-27 21:04:49,094 - INFO - joeynmt.training - Epoch  10, Step:    40800, Batch Loss:     1.257214, Batch Acc: 0.606401, Tokens per Sec:     4608, Lr: 0.000300
2024-05-27 21:05:04,946 - INFO - joeynmt.training - Epoch  10, Step:    40900, Batch Loss:     1.404033, Batch Acc: 0.609734, Tokens per Sec:     4618, Lr: 0.000300
2024-05-27 21:05:21,902 - INFO - joeynmt.training - Epoch  10, Step:    41000, Batch Loss:     1.281126, Batch Acc: 0.606534, Tokens per Sec:     4327, Lr: 0.000300
2024-05-27 21:05:21,903 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:05:21,903 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:06:36,525 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.75, acc:   0.55, generation: 74.6151[sec], evaluation: 0.0000[sec]
2024-05-27 21:06:36,674 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/38500.ckpt
2024-05-27 21:06:36,675 - INFO - joeynmt.training - Example #0
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'last', 'ye@@', 'ar', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 't@@', 'ac@@', 'i@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'al', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'si@@', 'ze', '4@@', '0', 'percent', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:06:36,675 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:06:36,675 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:06:36,675 - INFO - joeynmt.training - 	Hypothesis: I showed these last year showed these slides to show that the calculation tacial glacial artical glacial size 40 percent has had the size of 40 percent.
2024-05-27 21:06:36,675 - INFO - joeynmt.training - Example #1
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:06:36,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'ent', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', 't@@', 'en', 'is', 'of@@', ',', '</s>']
2024-05-27 21:06:36,675 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:06:36,675 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Hypothesis: However, this subvalent of the problem because it doesn't show the often is of,
2024-05-27 21:06:36,676 - INFO - joeynmt.training - Example #2
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an', 'ar@@', 't', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Hypothesis: The art glacial clean art is in a sense, the clean heart of the global climate system.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - Example #3
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'he', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'su@@', 'm@@', 'm@@', 'er', 'and', 'w@@', 'in@@', 'd@@', 's.', '</s>']
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - 	Hypothesis: She expands of the summer and winds.
2024-05-27 21:06:36,676 - INFO - joeynmt.training - Example #4
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:06:36,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 'ro@@', 'l@@', 'ed', 'ro@@', 'o@@', 'm', 'to', 'be', 'a', 'co@@', 'up@@', 'le', 'year@@', 's.', '</s>']
2024-05-27 21:06:36,677 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:06:36,677 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:06:36,677 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick roled room to be a couple years.
2024-05-27 21:06:52,951 - INFO - joeynmt.training - Epoch  10, Step:    41100, Batch Loss:     1.553092, Batch Acc: 0.604114, Tokens per Sec:     4253, Lr: 0.000300
2024-05-27 21:07:07,968 - INFO - joeynmt.training - Epoch  10, Step:    41200, Batch Loss:     1.495224, Batch Acc: 0.611389, Tokens per Sec:     4722, Lr: 0.000300
2024-05-27 21:07:24,350 - INFO - joeynmt.training - Epoch  10, Step:    41300, Batch Loss:     1.315354, Batch Acc: 0.613176, Tokens per Sec:     4431, Lr: 0.000300
2024-05-27 21:07:41,317 - INFO - joeynmt.training - Epoch  10, Step:    41400, Batch Loss:     1.292935, Batch Acc: 0.602372, Tokens per Sec:     4324, Lr: 0.000300
2024-05-27 21:07:57,817 - INFO - joeynmt.training - Epoch  10, Step:    41500, Batch Loss:     1.424842, Batch Acc: 0.608356, Tokens per Sec:     4504, Lr: 0.000300
2024-05-27 21:07:57,817 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:07:57,817 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:09:04,532 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.75, acc:   0.55, generation: 66.7078[sec], evaluation: 0.0000[sec]
2024-05-27 21:09:04,682 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/39000.ckpt
2024-05-27 21:09:04,682 - INFO - joeynmt.training - Example #0
2024-05-27 21:09:04,682 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:09:04,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 't@@', 'y@@', 'p@@', 'e', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slides to show that the calculation that the type for almost three million years has had the size of 48 million years has had the size of 40 percent.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - Example #1
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'a', 'lot', 'of', 'this', 'sub@@', 'val@@', 'it@@', 'ted', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', '--', '</s>']
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Hypothesis: But this is a lot of this subvalitted the gravity of the problem because it doesn't show the ice of ice --
2024-05-27 21:09:04,683 - INFO - joeynmt.training - Example #2
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an@@', 'ing', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - 	Hypothesis: The art glacial cleaning is in a sense, the heart of the global climate system.
2024-05-27 21:09:04,683 - INFO - joeynmt.training - Example #3
2024-05-27 21:09:04,683 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:09:04,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:09:04,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'it', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Hypothesis: You get it out of the winter and the summer.
2024-05-27 21:09:04,684 - INFO - joeynmt.training - Example #4
2024-05-27 21:09:04,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:09:04,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:09:04,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'ra@@', 'pi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:09:04,684 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid carlled on the last 25 years.
2024-05-27 21:09:19,768 - INFO - joeynmt.training - Epoch  10, Step:    41600, Batch Loss:     1.441819, Batch Acc: 0.612221, Tokens per Sec:     4752, Lr: 0.000300
2024-05-27 21:09:34,897 - INFO - joeynmt.training - Epoch  10, Step:    41700, Batch Loss:     1.324052, Batch Acc: 0.606769, Tokens per Sec:     4652, Lr: 0.000300
2024-05-27 21:09:49,726 - INFO - joeynmt.training - Epoch  10, Step:    41800, Batch Loss:     1.293019, Batch Acc: 0.607204, Tokens per Sec:     4883, Lr: 0.000300
2024-05-27 21:10:04,606 - INFO - joeynmt.training - Epoch  10, Step:    41900, Batch Loss:     1.547234, Batch Acc: 0.603894, Tokens per Sec:     4781, Lr: 0.000300
2024-05-27 21:10:21,114 - INFO - joeynmt.training - Epoch  10, Step:    42000, Batch Loss:     1.343166, Batch Acc: 0.605897, Tokens per Sec:     4333, Lr: 0.000300
2024-05-27 21:10:21,115 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:10:21,115 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:11:31,101 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.73, acc:   0.55, generation: 69.9788[sec], evaluation: 0.0000[sec]
2024-05-27 21:11:31,102 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:11:31,248 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/40500.ckpt
2024-05-27 21:11:31,249 - INFO - joeynmt.training - Example #0
2024-05-27 21:11:31,249 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:11:31,249 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:11:31,249 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'show', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Hypothesis: And the last year I showed these slide to show that the glacial calot, which for almost three million years has had the size of 48 million years has had the size of 48 million years has had the size of 40 percent.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - Example #1
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 's@@', 'am@@', 'e,', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'ou@@', 'l@@', "dn't", 'show', 'the', 's@@', 'am@@', 'e.', '</s>']
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Hypothesis: But this is the same, the gravity of the problem because it doesn't show the souldn't show the same.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - Example #2
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an', 'ar@@', 't', 'c@@', 'le@@', 'an', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'h', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - 	Hypothesis: The art glacial clean art clean heart of the global climath heart of the global climate system.
2024-05-27 21:11:31,250 - INFO - joeynmt.training - Example #3
2024-05-27 21:11:31,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:11:31,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:11:31,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'inter@@', ',', 'and', 'I', 'get', 'the', 'su@@', 'm@@', 'm@@', 'er', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Hypothesis: You get the winter, and I get the summer of the summer.
2024-05-27 21:11:31,251 - INFO - joeynmt.training - Example #4
2024-05-27 21:11:31,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:11:31,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:11:31,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:11:31,251 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick carlled on the last 25 years old.
2024-05-27 21:11:47,172 - INFO - joeynmt.training - Epoch  10, Step:    42100, Batch Loss:     1.176046, Batch Acc: 0.608358, Tokens per Sec:     4269, Lr: 0.000300
2024-05-27 21:12:05,241 - INFO - joeynmt.training - Epoch  10, Step:    42200, Batch Loss:     1.312175, Batch Acc: 0.606731, Tokens per Sec:     4063, Lr: 0.000300
2024-05-27 21:12:20,672 - INFO - joeynmt.training - Epoch  10, Step:    42300, Batch Loss:     1.310014, Batch Acc: 0.606708, Tokens per Sec:     4603, Lr: 0.000300
2024-05-27 21:12:37,458 - INFO - joeynmt.training - Epoch  10, Step:    42400, Batch Loss:     1.393135, Batch Acc: 0.604905, Tokens per Sec:     4311, Lr: 0.000300
2024-05-27 21:12:53,067 - INFO - joeynmt.training - Epoch  10, Step:    42500, Batch Loss:     1.318757, Batch Acc: 0.604756, Tokens per Sec:     4578, Lr: 0.000300
2024-05-27 21:12:53,067 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:12:53,067 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:14:00,624 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.55, generation: 67.5499[sec], evaluation: 0.0000[sec]
2024-05-27 21:14:00,626 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:14:00,775 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/41000.ckpt
2024-05-27 21:14:00,776 - INFO - joeynmt.training - Example #0
2024-05-27 21:14:00,776 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:14:00,776 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:14:00,776 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to demonstrate that the glacial calculation that for almost three million years has had the size of 48 million years has had the United States 40 percent.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - Example #1
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 's@@', 'n@@', 'ess', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', '--', '</s>']
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the sness of the ice because it doesn't show the ice of the ice --
2024-05-27 21:14:00,777 - INFO - joeynmt.training - Example #2
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'lim@@', 'ate', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'glob@@', 'al', 'hear@@', 't', 'c@@', 'li@@', 'mat@@', 'h', 'c@@', 'li@@', 'mat@@', 'ical', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'c@@', 'li@@', 'mat@@', 'ical', 'system@@', '.', '</s>']
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - 	Hypothesis: The art glacial climate is in a certain sense, the global heart climath climatical heart of the global climatical climatical climatical system.
2024-05-27 21:14:00,777 - INFO - joeynmt.training - Example #3
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:14:00,777 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'in@@', 'ver@@', 'se', 'and', 'the', 'w@@', 'int@@', 'er', 'of', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Hypothesis: You expand inverse and the winter of the summer.
2024-05-27 21:14:00,778 - INFO - joeynmt.training - Example #4
2024-05-27 21:14:00,778 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:14:00,778 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:14:00,778 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'car@@', 'll@@', 'ed', 'on', 'the', 'f@@', 'ing@@', 'er@@', '-@@', 'to@@', '-@@', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:14:00,778 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast carlled on the finger-to-25 years old.
2024-05-27 21:14:16,358 - INFO - joeynmt.training - Epoch  10, Step:    42600, Batch Loss:     1.227803, Batch Acc: 0.593127, Tokens per Sec:     4659, Lr: 0.000300
2024-05-27 21:14:32,453 - INFO - joeynmt.training - Epoch  10, Step:    42700, Batch Loss:     1.232728, Batch Acc: 0.610441, Tokens per Sec:     4387, Lr: 0.000300
2024-05-27 21:14:49,328 - INFO - joeynmt.training - Epoch  10, Step:    42800, Batch Loss:     1.376591, Batch Acc: 0.601097, Tokens per Sec:     4472, Lr: 0.000300
2024-05-27 21:15:05,336 - INFO - joeynmt.training - Epoch  10, Step:    42900, Batch Loss:     1.477449, Batch Acc: 0.602050, Tokens per Sec:     4438, Lr: 0.000300
2024-05-27 21:15:20,451 - INFO - joeynmt.training - Epoch  10, Step:    43000, Batch Loss:     1.372867, Batch Acc: 0.608132, Tokens per Sec:     4776, Lr: 0.000300
2024-05-27 21:15:20,451 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:15:20,451 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:16:22,297 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.55, generation: 61.8386[sec], evaluation: 0.0000[sec]
2024-05-27 21:16:22,446 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/41500.ckpt
2024-05-27 21:16:22,447 - INFO - joeynmt.training - Example #0
2024-05-27 21:16:22,447 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to show that the calculation that the calculation that for almost three million years has had the 48 dimensions of the 48 States of 40 percent.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - Example #1
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'est@@', 'y@@', 'er', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', '--', '</s>']
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Hypothesis: However, this underestyer the problem because it doesn't show the of, because it doesn't show the ice of the ice --
2024-05-27 21:16:22,448 - INFO - joeynmt.training - Example #2
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'c@@', 'le@@', 'an', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'c@@', 'le@@', 'an', 'c@@', 'lim@@', 'ate', 'hear@@', 't.', '</s>']
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - 	Hypothesis: The artic glacial clean is in a sense, the clean clean climate heart.
2024-05-27 21:16:22,448 - INFO - joeynmt.training - Example #3
2024-05-27 21:16:22,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:16:22,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:16:22,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'i@@', 'es', 'and', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Hypothesis: You expands of inveries and the summer.
2024-05-27 21:16:22,449 - INFO - joeynmt.training - Example #4
2024-05-27 21:16:22,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:16:22,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:16:22,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'f@@', 'ast@@', '-@@', 're@@', 'ach@@', 'ed', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:16:22,449 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fast-reached carlled on the last 25 years.
2024-05-27 21:16:38,506 - INFO - joeynmt.training - Epoch  10, Step:    43100, Batch Loss:     1.342794, Batch Acc: 0.600867, Tokens per Sec:     4496, Lr: 0.000300
2024-05-27 21:16:55,290 - INFO - joeynmt.training - Epoch  10, Step:    43200, Batch Loss:     1.336999, Batch Acc: 0.602526, Tokens per Sec:     4269, Lr: 0.000300
2024-05-27 21:17:11,601 - INFO - joeynmt.training - Epoch  10, Step:    43300, Batch Loss:     1.396218, Batch Acc: 0.600376, Tokens per Sec:     4337, Lr: 0.000300
2024-05-27 21:17:27,181 - INFO - joeynmt.training - Epoch  10, Step:    43400, Batch Loss:     1.370583, Batch Acc: 0.597243, Tokens per Sec:     4507, Lr: 0.000300
2024-05-27 21:17:43,657 - INFO - joeynmt.training - Epoch  10, Step:    43500, Batch Loss:     1.272548, Batch Acc: 0.600421, Tokens per Sec:     4387, Lr: 0.000300
2024-05-27 21:17:43,659 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:17:43,659 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:18:49,423 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.67, acc:   0.55, generation: 65.7572[sec], evaluation: 0.0000[sec]
2024-05-27 21:18:49,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:18:49,576 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/40000.ckpt
2024-05-27 21:18:49,578 - INFO - joeynmt.training - Example #0
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'of', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:18:49,578 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:18:49,578 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:18:49,578 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to demonstrate that the glacial calot, which for almost three million years has had had the size of 48 million years has had the size of 48 percent of the United States of 40 percent.
2024-05-27 21:18:49,578 - INFO - joeynmt.training - Example #1
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:18:49,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'of@@', '.', '</s>']
2024-05-27 21:18:49,578 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:18:49,578 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Hypothesis: However, this subvalue of the problem because it doesn't show the of.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - Example #2
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'cal@@', 'ot@@', 'a@@', 'th@@', 'o@@', 't', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'c@@', 'le@@', 'an', 'hear@@', 't', 'hear@@', 't,', 'the', 'c@@', 'li@@', 'mat@@', 'h', 'hear@@', 't', 'of', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Hypothesis: The art calotathot is in a sense, the clean heart heart, the climath heart of global climate system.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - Example #3
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'in@@', 'n@@', 'er', 'and', 'the', 'su@@', 'm@@', 'mer@@', '.', '</s>']
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Hypothesis: You expands of the winner and the summer.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - Example #4
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:18:49,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'of', 'the', 'last', '2@@', '5', 'years', 'be@@', 'came', 'to', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:18:49,579 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:18:49,580 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a rapid of the last 25 years became to the last 25 years.
2024-05-27 21:19:05,674 - INFO - joeynmt.training - Epoch  10, Step:    43600, Batch Loss:     1.516666, Batch Acc: 0.600025, Tokens per Sec:     4351, Lr: 0.000300
2024-05-27 21:19:21,705 - INFO - joeynmt.training - Epoch  10, Step:    43700, Batch Loss:     1.224750, Batch Acc: 0.602426, Tokens per Sec:     4459, Lr: 0.000300
2024-05-27 21:19:37,070 - INFO - joeynmt.training - Epoch  10, Step:    43800, Batch Loss:     1.117408, Batch Acc: 0.598410, Tokens per Sec:     4569, Lr: 0.000300
2024-05-27 21:19:51,659 - INFO - joeynmt.training - Epoch  10, Step:    43900, Batch Loss:     1.201483, Batch Acc: 0.599687, Tokens per Sec:     4824, Lr: 0.000300
2024-05-27 21:20:06,295 - INFO - joeynmt.training - Epoch  10, Step:    44000, Batch Loss:     1.359896, Batch Acc: 0.598541, Tokens per Sec:     4776, Lr: 0.000300
2024-05-27 21:20:06,296 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:20:06,296 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:21:03,266 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.55, generation: 56.9638[sec], evaluation: 0.0000[sec]
2024-05-27 21:21:03,419 - INFO - joeynmt.helpers - delete models/bpe_level_model_2000/39500.ckpt
2024-05-27 21:21:03,421 - INFO - joeynmt.training - Example #0
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'qu@@', 'asi', 'tre', 'milioni', 'di', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'ye@@', 'ar', 'I', 'show@@', 'ed', 'these', 's@@', 'li@@', 'de@@', 's', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'g@@', 'l@@', 'ac@@', 'i@@', 'al', 'h@@', 'o@@', 't', 'that', 'for', 'al@@', 'most', 'three', 'million', 'years', 'has', 'had', 'the', 'si@@', 'z@@', 'e,', 'the', 'si@@', 'z@@', 'es', 'of', 'the', 'Un@@', 'it@@', 'ed', 'St@@', 'at@@', 'es', 'on', 'the', '4@@', '0', 'percent', 'of', 'the', '4@@', '0', 'per@@', 'cent@@', '.', '</s>']
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to demonstrate that the glacial hot that for almost three million years has had the size, the sizes of the United States on the 40 percent of the 40 percent.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - Example #1
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'acci@@', 'o.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'at@@', 'es', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'ul@@', 'ar', 'prob@@', 'le@@', 'm', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'it@@', 'y,', 'because', 'it', "doesn't", 'show', 'the', 'of@@', ',', 'because', 'it', "doesn't", 'show', 'the', 'of@@', '.', '</s>']
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - 	Hypothesis: However, this subvality, because it doesn't show the of, because it doesn't show the of.
2024-05-27 21:21:03,421 - INFO - joeynmt.training - Example #2
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'g@@', 'l@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'c@@', 'li@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-27 21:21:03,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y@@', 'pic@@', 'al', 'cal@@', 'ot@@', '.', 'A@@', 'r@@', 'c@@', 'ti@@', 'c', 'c@@', 'le@@', 'an', 'is', 'in', 'a', 'cer@@', 'ta@@', 'in', 'sen@@', 'se,', 'the', 'c@@', 'lim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Hypothesis: The artypical calot. Arctic clean is in a certain sense, the climate system.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - Example #3
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'and@@', 's', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'su@@', 'm@@', 'mer@@', '.']
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'ri@@', 'gh@@', 't.', '</s>']
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Hypothesis: You expands of the winter and you get right.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - Example #4
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sar@@', 'à', 'una', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-27 21:21:03,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 's@@', 'li@@', 'de@@', 's', 'will', 'be', 'a', 'qu@@', 'ic@@', 'k', 's@@', 'li@@', 'de@@', 's', 'on', 'the', 'last', '2@@', '5', 'year@@', 's.', '</s>']
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:21:03,422 - INFO - joeynmt.training - 	Hypothesis: The next slides will be a quick slides on the last 25 years.
2024-05-27 21:21:20,133 - INFO - joeynmt.training - Epoch  10, Step:    44100, Batch Loss:     1.358189, Batch Acc: 0.600186, Tokens per Sec:     4204, Lr: 0.000300
2024-05-27 21:21:37,036 - INFO - joeynmt.training - Epoch  10, Step:    44200, Batch Loss:     1.496097, Batch Acc: 0.599695, Tokens per Sec:     4191, Lr: 0.000300
2024-05-27 21:21:39,326 - INFO - joeynmt.training - Epoch  10: total training loss 5867.38
2024-05-27 21:21:39,326 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-27 21:21:39,326 - INFO - joeynmt.training - Best validation result (greedy) at step    43500:   4.67 ppl.
2024-05-27 21:21:39,335 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-27 21:21:39,370 - INFO - joeynmt.model - Enc-dec model built.
2024-05-27 21:21:39,397 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_2000/43500.ckpt.
2024-05-27 21:21:39,399 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1991),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1991),
	loss_function=None)
2024-05-27 21:21:39,400 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-27 21:21:39,400 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:21:39,400 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:23:15,316 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 95.9096[sec], evaluation: 0.0000[sec]
2024-05-27 21:23:15,320 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_2000/00043500.hyps.dev.
2024-05-27 21:23:15,320 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-27 21:23:15,320 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:23:15,320 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:25:51,402 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 156.0716[sec], evaluation: 0.0000[sec]
2024-05-27 21:25:51,407 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_2000/00043500.hyps.test.
