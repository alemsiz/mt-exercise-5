2024-05-28 09:00:03,489 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:00:03,595 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:00:03,665 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:00:03,702 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:00:03,711 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:00:03,711 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:00:03,714 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:00:03,714 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:01:13,083 - INFO - joeynmt.prediction - Generation took 69.3604[sec]. (No references given)
2024-05-28 09:48:26,297 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:48:26,403 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:48:26,462 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:48:26,492 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:48:26,502 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:48:26,502 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:48:26,505 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:48:26,505 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:48:55,204 - INFO - joeynmt.prediction - Generation took 28.6906[sec]. (No references given)
2024-05-28 09:48:56,675 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:48:56,783 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:48:56,826 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:48:56,846 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:48:56,854 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:48:56,854 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:48:56,857 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:48:56,857 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:49:51,111 - INFO - joeynmt.prediction - Generation took 54.2452[sec]. (No references given)
2024-05-28 09:49:52,581 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:49:52,689 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:49:52,731 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:49:52,751 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:49:52,759 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:49:52,759 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:49:52,761 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:49:52,761 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:51:14,197 - INFO - joeynmt.prediction - Generation took 81.4270[sec]. (No references given)
2024-05-28 09:51:15,818 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:51:15,924 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:51:15,967 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:51:15,996 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:51:16,004 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:51:16,004 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:51:16,006 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:51:16,006 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:53:12,774 - INFO - joeynmt.prediction - Generation took 116.7585[sec]. (No references given)
2024-05-28 09:53:14,462 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:53:14,569 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:53:14,613 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:53:14,642 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:53:14,650 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:53:14,650 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:53:14,652 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:53:14,652 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:55:41,204 - INFO - joeynmt.prediction - Generation took 146.5438[sec]. (No references given)
2024-05-28 09:55:42,815 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:55:42,921 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:55:42,964 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:55:42,994 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:55:43,002 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:55:43,002 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:55:43,005 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:55:43,005 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=12, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 09:58:36,911 - INFO - joeynmt.prediction - Generation took 173.8975[sec]. (No references given)
2024-05-28 09:58:38,516 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 09:58:38,623 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 09:58:38,666 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 09:58:38,695 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 09:58:38,703 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:58:38,704 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 09:58:38,706 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 09:58:38,706 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=14, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 10:01:53,125 - INFO - joeynmt.prediction - Generation took 194.4105[sec]. (No references given)
2024-05-28 10:01:54,805 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 10:01:54,913 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 10:01:54,956 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 10:01:54,986 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 10:01:54,994 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:01:54,994 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:01:54,997 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 10:01:54,997 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=16, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 10:05:45,208 - INFO - joeynmt.prediction - Generation took 230.2025[sec]. (No references given)
2024-05-28 10:05:46,826 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 10:05:46,934 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 10:05:46,977 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 10:05:47,006 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 10:05:47,015 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:05:47,015 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:05:47,017 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 10:05:47,017 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=18, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 10:10:25,192 - INFO - joeynmt.prediction - Generation took 278.1663[sec]. (No references given)
2024-05-28 10:10:26,914 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-28 10:10:27,023 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 10:10:27,066 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 10:10:27,095 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 10:10:27,104 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:10:27,104 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-28 10:10:27,107 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 10:10:27,107 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=20, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 10:15:41,614 - INFO - joeynmt.prediction - Generation took 314.4990[sec]. (No references given)
