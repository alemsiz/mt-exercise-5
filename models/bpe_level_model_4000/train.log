2024-05-27 21:31:31,345 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                           cfg.name : bpe_level_model_4000
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                     cfg.data.train : data/head100k.train.it-en
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.it-en
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.it-en
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-27 21:31:31,345 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : bpe_data/4000/bpe_vocab.it-en.joint.cleaned
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 4000
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : bpe_data/4000/codes.BPE
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : bpe_data/4000/bpe_vocab.it-en.joint.cleaned
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 4000
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : bpe_data/4000/codes.BPE
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_level_model_4000
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-27 21:31:31,346 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-27 21:31:31,347 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-27 21:31:31,349 - INFO - joeynmt.data - Building tokenizer...
2024-05-27 21:31:31,354 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-27 21:31:31,354 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-27 21:31:31,354 - INFO - joeynmt.data - Loading train set...
2024-05-27 21:31:31,439 - INFO - joeynmt.data - Building vocabulary...
2024-05-27 21:31:31,544 - INFO - joeynmt.data - Loading dev set...
2024-05-27 21:31:31,547 - INFO - joeynmt.data - Loading test set...
2024-05-27 21:31:31,549 - INFO - joeynmt.data - Data loaded.
2024-05-27 21:31:31,549 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 21:31:31,549 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 21:31:31,549 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2024-05-27 21:31:31,549 - INFO - joeynmt.data - First training example:
	[SRC] Al G@@ or@@ e: ar@@ rest@@ iamo il ri@@ scal@@ d@@ amento glob@@ ale
	[TRG] Al G@@ or@@ e: A@@ ver@@ ting the clim@@ ate cri@@ si@@ s
2024-05-27 21:31:31,549 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) the (6) di (7) to (8) in (9) e
2024-05-27 21:31:31,549 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) the (6) di (7) to (8) in (9) e
2024-05-27 21:31:31,549 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 3939
2024-05-27 21:31:31,549 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 3939
2024-05-27 21:31:31,550 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-27 21:31:31,594 - INFO - joeynmt.model - Enc-dec model built.
2024-05-27 21:31:31,596 - INFO - joeynmt.model - Total params: 3907584
2024-05-27 21:31:31,596 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2024-05-27 21:31:31,596 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3939),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3939),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-27 21:31:31,596 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-27 21:31:31,596 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-27 21:31:31,597 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-27 21:31:31,597 - INFO - joeynmt.training - EPOCH 1
2024-05-27 21:31:47,493 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.305237, Batch Acc: 0.041047, Tokens per Sec:     4342, Lr: 0.000300
2024-05-27 21:32:03,741 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.182383, Batch Acc: 0.063486, Tokens per Sec:     4189, Lr: 0.000300
2024-05-27 21:32:19,898 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.054934, Batch Acc: 0.074312, Tokens per Sec:     4270, Lr: 0.000300
2024-05-27 21:32:35,937 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.964517, Batch Acc: 0.082883, Tokens per Sec:     4193, Lr: 0.000300
2024-05-27 21:32:52,793 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.904845, Batch Acc: 0.086959, Tokens per Sec:     4210, Lr: 0.000300
2024-05-27 21:32:52,793 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:32:52,793 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:34:30,381 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.92, ppl:  50.52, acc:   0.09, generation: 97.5694[sec], evaluation: 0.0000[sec]
2024-05-27 21:34:30,383 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:34:30,551 - INFO - joeynmt.training - Example #0
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2024-05-27 21:34:30,551 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:34:30,551 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:34:30,551 - INFO - joeynmt.training - 	Hypothesis: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2024-05-27 21:34:30,551 - INFO - joeynmt.training - Example #1
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:34:30,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'to', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Hypothesis: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the of to the the the the the the the the the the the the the the the the the the the the the
2024-05-27 21:34:30,552 - INFO - joeynmt.training - Example #2
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Hypothesis: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2024-05-27 21:34:30,552 - INFO - joeynmt.training - Example #3
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'of', 'of', 'of', 'of', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:34:30,552 - INFO - joeynmt.training - 	Hypothesis: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the of of of of the the the the the the the the the the the the the the the the the the the the
2024-05-27 21:34:30,552 - INFO - joeynmt.training - Example #4
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:34:30,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'to', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2024-05-27 21:34:30,553 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:34:30,553 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:34:30,553 - INFO - joeynmt.training - 	Hypothesis: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the to the the the the the the the the the the the the the the the the the the the the the the the
2024-05-27 21:34:47,943 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.787278, Batch Acc: 0.089259, Tokens per Sec:     3985, Lr: 0.000300
2024-05-27 21:35:04,266 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.802762, Batch Acc: 0.093819, Tokens per Sec:     4221, Lr: 0.000300
2024-05-27 21:35:20,439 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.768658, Batch Acc: 0.097257, Tokens per Sec:     4312, Lr: 0.000300
2024-05-27 21:35:37,535 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.751175, Batch Acc: 0.097863, Tokens per Sec:     4154, Lr: 0.000300
2024-05-27 21:35:53,837 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.651083, Batch Acc: 0.101553, Tokens per Sec:     4283, Lr: 0.000300
2024-05-27 21:35:53,837 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:35:53,837 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:37:26,406 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.74, ppl:  42.07, acc:   0.10, generation: 92.5542[sec], evaluation: 0.0000[sec]
2024-05-27 21:37:26,408 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:37:26,573 - INFO - joeynmt.training - Example #0
2024-05-27 21:37:26,573 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:37:26,573 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'I', 'can', 'be', 'a', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first']
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Hypothesis: And I think I think I can be a first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first
2024-05-27 21:37:26,574 - INFO - joeynmt.training - Example #1
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'and', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'and', 'I', 'think', 'I', 'think', 'I', 'think', 'and', 'I', 'think', 'to', 'be', 'a', 'be', 'a', 'be', 'a', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first']
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Hypothesis: And I think I think I think I think I think I think and I think I think I think I think I think I think I think I think I think I think and I think I think I think and I think to be a be a be a first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first
2024-05-27 21:37:26,574 - INFO - joeynmt.training - Example #2
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'can', 'be', 'a', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the']
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:37:26,574 - INFO - joeynmt.training - 	Hypothesis: And I think I can be a first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the
2024-05-27 21:37:26,574 - INFO - joeynmt.training - Example #3
2024-05-27 21:37:26,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:37:26,575 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:37:26,575 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'think', 'and', 'I', 'think', 'I', 'think', 'I', 'think', 'and', 'I', 'think', 'to', 'be', 'a', 'be', 'a', 'be', 'a', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the']
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Hypothesis: And I think I think I think I think I think I think I think I think I think I think I think I think I think I think I think I think I think and I think I think I think and I think to be a be a be a first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the
2024-05-27 21:37:26,575 - INFO - joeynmt.training - Example #4
2024-05-27 21:37:26,575 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:37:26,575 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:37:26,575 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'I', 'think', 'I', 'think', 'I', 'can', 'be', 'a', 'be', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the']
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:37:26,575 - INFO - joeynmt.training - 	Hypothesis: And I think I think I think I can be a be the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the
2024-05-27 21:37:43,881 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.729427, Batch Acc: 0.105728, Tokens per Sec:     3930, Lr: 0.000300
2024-05-27 21:38:00,583 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.658097, Batch Acc: 0.106149, Tokens per Sec:     4192, Lr: 0.000300
2024-05-27 21:38:16,636 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.868534, Batch Acc: 0.111072, Tokens per Sec:     4299, Lr: 0.000300
2024-05-27 21:38:32,893 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.640445, Batch Acc: 0.116553, Tokens per Sec:     4443, Lr: 0.000300
2024-05-27 21:38:50,012 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.614034, Batch Acc: 0.117040, Tokens per Sec:     4029, Lr: 0.000300
2024-05-27 21:38:50,013 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:38:50,013 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:40:26,713 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.64, ppl:  38.17, acc:   0.11, generation: 96.6857[sec], evaluation: 0.0000[sec]
2024-05-27 21:40:26,715 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:40:26,885 - INFO - joeynmt.training - Example #0
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'a', 'way', 'that', 'I', 'was', 'a', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same']
2024-05-27 21:40:26,885 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:40:26,885 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:40:26,885 - INFO - joeynmt.training - 	Hypothesis: And I was a way that I was a same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same
2024-05-27 21:40:26,885 - INFO - joeynmt.training - Example #1
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:40:26,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not']
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Hypothesis: And I was not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not
2024-05-27 21:40:26,886 - INFO - joeynmt.training - Example #2
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'a', 'b@@', 'es', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of']
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Hypothesis: And I was a bes of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of
2024-05-27 21:40:26,886 - INFO - joeynmt.training - Example #3
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'can', 'have', 'the', 'the', 're@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', 'a@@', '.', '</s>']
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - 	Hypothesis: And you can have the the reaaaaaaaaaaaaaa.
2024-05-27 21:40:26,886 - INFO - joeynmt.training - Example #4
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:40:26,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'a', 'b@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'e@@', 'd.', '</s>']
2024-05-27 21:40:26,887 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:40:26,887 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:40:26,887 - INFO - joeynmt.training - 	Hypothesis: And I was a beeeeeeeeeeeeeeeeeeeeeeed.
2024-05-27 21:40:43,820 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.598281, Batch Acc: 0.119191, Tokens per Sec:     4032, Lr: 0.000300
2024-05-27 21:41:01,498 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.568716, Batch Acc: 0.123636, Tokens per Sec:     3879, Lr: 0.000300
2024-05-27 21:41:19,100 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.483302, Batch Acc: 0.128715, Tokens per Sec:     3974, Lr: 0.000300
2024-05-27 21:41:35,899 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.661308, Batch Acc: 0.134879, Tokens per Sec:     4078, Lr: 0.000300
2024-05-27 21:41:53,647 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.417270, Batch Acc: 0.136165, Tokens per Sec:     3889, Lr: 0.000300
2024-05-27 21:41:53,649 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:41:53,649 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:43:30,049 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.51, ppl:  33.60, acc:   0.14, generation: 96.3890[sec], evaluation: 0.0000[sec]
2024-05-27 21:43:30,050 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:43:30,215 - INFO - joeynmt.training - Example #0
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'think', 'that', 'we', 'can', 'be', 'a', 'lot', 'of', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'to', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'to', 'be', 'the', 'world', 'to', 'be', 'the', 'world', 'to', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'to', 'be', 'the', 're@@', 'p@@', 'es', 'to', 'be', 'the']
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Hypothesis: And I think that we can be a lot of the world that we can be the world that we can be the world that we can be the world that we can be the world that we can be the world that we can be the world that we can be the world to be the world that we can be the world to be the world to be the world to be the world that we can be the world to be the repes to be the
2024-05-27 21:43:30,215 - INFO - joeynmt.training - Example #1
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:43:30,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['So', 'we', "don't", 'have', 'a', 'lot', 'of', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', 'can', 'be', 'the', 'world', 'that', 'we', "don't", 'have', 'to', 'the', 'world', 'that', 'we', "don't", 'have', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not']
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:43:30,215 - INFO - joeynmt.training - 	Hypothesis: So we don't have a lot of the world that we can be the world that we can be the world that we don't have to the world that we don't have not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not
2024-05-27 21:43:30,216 - INFO - joeynmt.training - Example #2
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'is', 'a', 'little', 'little', 'be', 'a', 'little', 'little', 'little', 'be', 'a', 'little', 'little', 'little', 'be', 'a', 'little', 'little', 'little', 'little', 'be', 'a', 'little', 'little', 'be', 'a', 'little', 'little', 'be', 'a', 'little', 'be', 'a', 'little', 'little', 'little', 'be', 'a', 'little', 'be', 'a', 'little', 'little', 'be', 'a', 'little', 'little', 'little', 'little', 're@@', 's.', '</s>']
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Hypothesis: The first is a little little be a little little little be a little little little be a little little little little be a little little be a little little be a little be a little little little be a little be a little little be a little little little little res.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - Example #3
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['and', 'we', 'have', 'the', 'world', 'and', 'we', 'have', 'the', 'world', 'and', 'we', 'have', 'the', 'world', 'and', 'we', 'have', 'the', 'world', '</s>']
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Hypothesis: and we have the world and we have the world and we have the world and we have the world
2024-05-27 21:43:30,216 - INFO - joeynmt.training - Example #4
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:43:30,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'a', 'lot', 'of', 'the', 'world', 'is', 'the', 'world', 'that', 'we', 'have', 'a', 'lot', 'of', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', 'is', 'the', 'world', 'is', 'a', 'new', 'of', 'the', 'world', 'to', 'the', 'world', '</s>']
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:43:30,216 - INFO - joeynmt.training - 	Hypothesis: And I was a lot of the world is the world that we have a lot of the world is the world is the world is the world is the world is a new of the world to the world
2024-05-27 21:43:48,135 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.437001, Batch Acc: 0.144664, Tokens per Sec:     3938, Lr: 0.000300
2024-05-27 21:44:05,832 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.418796, Batch Acc: 0.151998, Tokens per Sec:     3998, Lr: 0.000300
2024-05-27 21:44:23,137 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.403833, Batch Acc: 0.160992, Tokens per Sec:     4006, Lr: 0.000300
2024-05-27 21:44:40,289 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.307842, Batch Acc: 0.176055, Tokens per Sec:     4123, Lr: 0.000300
2024-05-27 21:44:57,967 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.241083, Batch Acc: 0.181458, Tokens per Sec:     3908, Lr: 0.000300
2024-05-27 21:44:57,969 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:44:57,969 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:46:30,520 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.30, ppl:  27.01, acc:   0.18, generation: 92.5421[sec], evaluation: 0.0000[sec]
2024-05-27 21:46:30,522 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:46:30,693 - INFO - joeynmt.training - Example #0
2024-05-27 21:46:30,693 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'second', 'of', 'the', 'same', 'people', 'I', 'was', 'the', 'same', 'to', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'of', 'the', 'same', 'to', 'the', 'same', 'million', 'million', 'years', '</s>']
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Hypothesis: The second of the same people I was the same to the first of the first of the first of the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same of the same to the same million million years
2024-05-27 21:46:30,694 - INFO - joeynmt.training - Example #1
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'same', 'thing', 'that', 'the', 'same', 'is', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not']
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Hypothesis: And the same thing that the same is not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not
2024-05-27 21:46:30,694 - INFO - joeynmt.training - Example #2
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:46:30,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'of', 'the', 'same', 'is', 'a', 'little', 'bit', 'of', 'the', 'same', 'to', 'the', 'same', 'way', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'is', 'the', 'same', 'to', 'the', 'same', 'to', 'the', 'same', 'of', 'the', 'same', 'de@@', 'de@@', 'de@@', 'de@@', 'de@@', 'de@@', 's.', '</s>']
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:46:30,694 - INFO - joeynmt.training - 	Hypothesis: The first of the same is a little bit of the same to the same way to the same to the same to the same to the same to the same is the same to the same to the same of the same dedededededes.
2024-05-27 21:46:30,695 - INFO - joeynmt.training - Example #3
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['and', 'they', 'are', 'the', 'f@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', 'le@@', '?', '</s>']
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Hypothesis: and they are the flelelelelelelelelelele?
2024-05-27 21:46:30,695 - INFO - joeynmt.training - Example #4
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:46:30,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'of', 'the', 'same', 'way', 'to', 'the', 'same', 'way', 'to', 'the', 'same', 'way', 'to', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'thing.', '</s>']
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:46:30,695 - INFO - joeynmt.training - 	Hypothesis: The first of the same way to the same way to the same way to the same of the same of the same of the same thing.
2024-05-27 21:46:48,454 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.264621, Batch Acc: 0.193386, Tokens per Sec:     3681, Lr: 0.000300
2024-05-27 21:47:05,967 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.146232, Batch Acc: 0.197127, Tokens per Sec:     3924, Lr: 0.000300
2024-05-27 21:47:22,902 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.309408, Batch Acc: 0.207001, Tokens per Sec:     4106, Lr: 0.000300
2024-05-27 21:47:39,356 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.076333, Batch Acc: 0.211501, Tokens per Sec:     4098, Lr: 0.000300
2024-05-27 21:47:57,094 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.874799, Batch Acc: 0.220012, Tokens per Sec:     3884, Lr: 0.000300
2024-05-27 21:47:57,095 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:47:57,095 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:49:28,399 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.09, ppl:  22.03, acc:   0.21, generation: 91.2942[sec], evaluation: 0.0000[sec]
2024-05-27 21:49:28,402 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:49:28,569 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/500.ckpt
2024-05-27 21:49:28,570 - INFO - joeynmt.training - Example #0
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'to', 'have', 'a', 'lot', 'of', 'these', 'people', 'are', 'the', 's@@', 's@@', 'low@@', 'er', 'of', 'the', 's@@', 'ou@@', 'ght', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', '19@@', '8@@', '0', 'percent', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'people', 'who', 'was', 'the', 're@@', 'duc@@', 'ed', 'in', 'the', 'first', 'of', 'the', 'first']
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Hypothesis: And I was to have a lot of these people are the sslower of the sought of the first of the first of the first of the first of the first of the first of the 1980 percent of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first of the first people who was the reduced in the first of the first
2024-05-27 21:49:28,570 - INFO - joeynmt.training - Example #1
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'same', 'thing', 'that', 'the', 'same', 'thing', 'is', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'same', 'same', 'of', 'the', 'f@@', 'oun@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'ou@@', 'ght', 'to', 'be', 'the', 'f@@', 'oun@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'y.', '</s>']
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:49:28,570 - INFO - joeynmt.training - 	Hypothesis: And the same thing that the same thing is not not not not not not not the world of the world of the world of the same same of the foundddddddddddddddddddddddddddddddought to be the foundddddddy.
2024-05-27 21:49:28,570 - INFO - joeynmt.training - Example #2
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:49:28,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'one', 'of', 'the', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 'low@@', '-@@', 'h@@', 'ot@@', 's', 'in', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 're@@', 'ach@@', '.', '</s>']
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Hypothesis: The first one of the sssssssssslow-hots in the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the reach.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - Example #3
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'be', 'a', 'f@@', 'ing@@', 'ing@@', '-@@', 'and', 'and', 'and', 'they', 'can', 'be', 'able', 'and', '</s>']
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Hypothesis: You can be a finging-and and and they can be able and
2024-05-27 21:49:28,571 - INFO - joeynmt.training - Example #4
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:49:28,571 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'one', 'of', 'a', 'little', 'bit', 'of', 'a', 's@@', 's@@', 's@@', 's@@', 's@@', 'am@@', 'y', 'of', 'the', 'first', 'of', 'the', 'last', 'years', 'ago,', '</s>']
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:49:28,571 - INFO - joeynmt.training - 	Hypothesis: The first one of a little bit of a sssssamy of the first of the last years ago,
2024-05-27 21:49:45,313 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.951465, Batch Acc: 0.223795, Tokens per Sec:     4110, Lr: 0.000300
2024-05-27 21:50:02,805 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.227274, Batch Acc: 0.230519, Tokens per Sec:     4011, Lr: 0.000300
2024-05-27 21:50:20,103 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.922409, Batch Acc: 0.237473, Tokens per Sec:     4027, Lr: 0.000300
2024-05-27 21:50:37,552 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.991942, Batch Acc: 0.237771, Tokens per Sec:     4020, Lr: 0.000300
2024-05-27 21:50:55,003 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.018200, Batch Acc: 0.244388, Tokens per Sec:     3995, Lr: 0.000300
2024-05-27 21:50:55,005 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:50:55,005 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:52:25,455 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.94, ppl:  18.98, acc:   0.24, generation: 90.4441[sec], evaluation: 0.0000[sec]
2024-05-27 21:52:25,458 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:52:25,625 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/1000.ckpt
2024-05-27 21:52:25,627 - INFO - joeynmt.training - Example #0
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'was', 'the', 'first', 'time', 'I', 'had', 'a', 'little', 'bit', 'of', 'these', 'these', 'h@@', 'all@@', 's', 'that', 'you', 'can', 'be', 'the', 'h@@', 'all@@', 's', 'that', 'the', 'h@@', 'all@@', 'er', 'of', 'the', 'last', 'years', 'ago.', '</s>']
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Hypothesis: I was the first time I had a little bit of these these halls that you can be the halls that the haller of the last years ago.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - Example #1
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:52:25,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'second', 'second', 'is', 'the', 's@@', 'low@@', 'er', 'of', 'the', 'world', 'is', 'not', 'the', 't@@', 'ac@@', 'ac@@', 'tion', 'of', 'the', 'h@@', 'all@@', 's', 'of', 'the', 'h@@', 'all@@', '.', '</s>']
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - 	Hypothesis: The second second is the slower of the world is not the tacaction of the halls of the hall.
2024-05-27 21:52:25,627 - INFO - joeynmt.training - Example #2
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 's@@', 'low@@', 'er', 'is', 'a', 'lot', 'of', 'a', 'lot', 'of', 'a', 'lot', 'of', 'the', 's@@', 'am@@', 'ong', 'the', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'human', 'ac@@', 't.', '</s>']
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Hypothesis: The sslower is a lot of a lot of a lot of the samong the human human human human human human human human human human human human human human human human human human human human human human human human act.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - Example #3
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'be', 'a', 's@@', 'am@@', 'am@@', 'am@@', 'am@@', 'am@@', 'in@@', 'in@@', 'in@@', 'in@@', 'in@@', 'ing.', '</s>']
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Hypothesis: You can be a samamamamamininininining.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - Example #4
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:52:25,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'next', 'to', 'the', 'next', 'to', 'the', 'next', 'to', 'the', 'next', 'to', 'the', 'last', 'years', 'ago.', '</s>']
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:52:25,628 - INFO - joeynmt.training - 	Hypothesis: The next next next to the next to the next to the next to the last years ago.
2024-05-27 21:52:42,691 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.801664, Batch Acc: 0.252969, Tokens per Sec:     4006, Lr: 0.000300
2024-05-27 21:52:59,961 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.831071, Batch Acc: 0.255276, Tokens per Sec:     3905, Lr: 0.000300
2024-05-27 21:53:17,115 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.840978, Batch Acc: 0.264914, Tokens per Sec:     4094, Lr: 0.000300
2024-05-27 21:53:34,329 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.785746, Batch Acc: 0.266669, Tokens per Sec:     4087, Lr: 0.000300
2024-05-27 21:53:44,139 - INFO - joeynmt.training - Epoch   1: total training loss 13636.15
2024-05-27 21:53:44,140 - INFO - joeynmt.training - EPOCH 2
2024-05-27 21:53:51,483 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.744310, Batch Acc: 0.278574, Tokens per Sec:     3896, Lr: 0.000300
2024-05-27 21:53:51,484 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:53:51,484 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:55:21,935 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.82, ppl:  16.77, acc:   0.26, generation: 90.4427[sec], evaluation: 0.0000[sec]
2024-05-27 21:55:21,938 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:55:22,108 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/1500.ckpt
2024-05-27 21:55:22,109 - INFO - joeynmt.training - Example #0
2024-05-27 21:55:22,109 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:55:22,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:55:22,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'had', 'to', 'show', 'these', 'little', 'little', 'b@@', 'rou@@', 'ght', 'to', 'show', 'you', 'the', 's@@', 'low', 'that', 'the', 's@@', 'am@@', ',', 'which', 'is', 'a', 'year', 'for', 'three', 'years', 'ago,', 'a', 'year', 'of', 'years', 'ago,', 'the', 'U.@@', 'S.', '</s>']
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Hypothesis: And I had to show these little little brought to show you the slow that the sam, which is a year for three years ago, a year of years ago, the U.S.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - Example #1
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'idea', 'of', 'this', 'is', 'the', 'way', 'of', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'idea', 'of', 'the', 'h@@', 'ands', 'of', 'the', 'h@@', 'ands', 'of', 'the', 'h@@', 'ands', 'of', 'the', 'gro@@', 'un@@', 'd.', '</s>']
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Hypothesis: The idea of this is the way of the most of the most of the idea of the hands of the hands of the hands of the ground.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - Example #2
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 'av@@', 'y', 'is', 'a', 're@@', '-@@', '-@@', 'h@@', 'old', 'h@@', 'ands', 'in', 'a', 'c@@', 'ut', 'of', 'the', 'ex@@', 'ist@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'econom@@', 'y.', '</s>']
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - 	Hypothesis: The savy is a re--hold hands in a cut of the existic of the economic of the economic economy.
2024-05-27 21:55:22,110 - INFO - joeynmt.training - Example #3
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:55:22,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:55:22,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', 'are', 'in', 'the', 'g@@', 'an@@', 'g', 'and', 'and', 'they', 'get', 'a', 'b@@', 'on@@', 'd', 'of', 'the', 'h@@', 'on@@', 'on@@', 'd', 'of', 'it.', '</s>']
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Hypothesis: They are in the gang and and they get a bond of the honond of it.
2024-05-27 21:55:22,111 - INFO - joeynmt.training - Example #4
2024-05-27 21:55:22,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:55:22,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:55:22,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'way', 'to', 'a', 'little', 'bit', 'of', 'a', 'a', 'little', 'bit', 'of', 'the', 'last', '20', 'years', 'ago.', '</s>']
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:55:22,111 - INFO - joeynmt.training - 	Hypothesis: The next next way to a little bit of a a little bit of the last 20 years ago.
2024-05-27 21:55:39,552 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     2.743380, Batch Acc: 0.283038, Tokens per Sec:     3997, Lr: 0.000300
2024-05-27 21:55:57,070 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.608447, Batch Acc: 0.286082, Tokens per Sec:     3924, Lr: 0.000300
2024-05-27 21:56:14,422 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     2.593254, Batch Acc: 0.295872, Tokens per Sec:     4131, Lr: 0.000300
2024-05-27 21:56:31,899 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.501703, Batch Acc: 0.297040, Tokens per Sec:     4019, Lr: 0.000300
2024-05-27 21:56:49,350 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.591939, Batch Acc: 0.304284, Tokens per Sec:     4021, Lr: 0.000300
2024-05-27 21:56:49,352 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:56:49,352 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 21:58:15,427 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.70, ppl:  14.89, acc:   0.29, generation: 86.0682[sec], evaluation: 0.0000[sec]
2024-05-27 21:58:15,430 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 21:58:15,705 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/2000.ckpt
2024-05-27 21:58:15,706 - INFO - joeynmt.training - Example #0
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'time', 'I', 'started', 'these', 'things', 'that', 'I', 'had', 'these', 'different', 'different', 'of', 'the', 'mo@@', 'un@@', 'ta@@', 'il@@', 's', 'to', 'the', 's@@', 'av@@', 'ed', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'years', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'which', 'is', 'the', 'United', 'St@@', 'at@@', 'es,', 'which', 'is', '3@@', '0@@', 's.', '</s>']
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Hypothesis: The first time I started these things that I had these different different of the mountails to the saved of years of years of years of years of years of the United States, which is the United States, which is 30s.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - Example #1
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'course,', 'this', 'is', 'the', 're@@', 'duc@@', 'e', 'of', 'the', 'problem', 'because', 'the', 'problem', 'of', 'the', 'same', 'thing', 'because', 'it', "doesn't", 'be', 'the', 'same', 'thing', 'of', 'the', 'same', 'thing', 'of', 'the', 'same', 'way.', '</s>']
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Hypothesis: All of course, this is the reduce of the problem because the problem of the same thing because it doesn't be the same thing of the same thing of the same way.
2024-05-27 21:58:15,707 - INFO - joeynmt.training - Example #2
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 21:58:15,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 'av@@', 'y', 'of', 'the', 're@@', 'spec@@', 't', 'is', 'in', 'a', 'lot', 'of', 'the', 'con@@', 'sum@@', 'er', 'of', 'the', 'power', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 'econom@@', 'ic', 'of', 'the', 're@@', 'fer@@', 'ti@@', 'p@@', 's.', '</s>']
2024-05-27 21:58:15,707 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Hypothesis: The savy of the respect is in a lot of the consumer of the power of the economic of the economic of the economic of the economic of the economic of the refertips.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - Example #3
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'a', 's@@', 'am@@', 'in', 'and', 'it', 'and', 'you', 'get', 'to', 'be', 'a', 'f@@', 'air@@', '.', '</s>']
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Hypothesis: It's a samin and it and you get to be a fair.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - Example #4
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 21:58:15,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'thing', 'is', 'going', 'to', 'be', 'a', 'lot', 'of', 'a', 'lot', 'of', 'the', 'last', 'year.', '</s>']
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 21:58:15,708 - INFO - joeynmt.training - 	Hypothesis: The next thing is going to be a lot of a lot of the last year.
2024-05-27 21:58:33,044 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.552193, Batch Acc: 0.311970, Tokens per Sec:     3929, Lr: 0.000300
2024-05-27 21:58:50,379 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.615770, Batch Acc: 0.317153, Tokens per Sec:     3972, Lr: 0.000300
2024-05-27 21:59:08,041 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.473118, Batch Acc: 0.326918, Tokens per Sec:     3966, Lr: 0.000300
2024-05-27 21:59:24,998 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.333021, Batch Acc: 0.327103, Tokens per Sec:     4052, Lr: 0.000300
2024-05-27 21:59:41,904 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.396963, Batch Acc: 0.337150, Tokens per Sec:     4004, Lr: 0.000300
2024-05-27 21:59:41,905 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 21:59:41,905 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:01:09,662 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.58, ppl:  13.14, acc:   0.32, generation: 87.7491[sec], evaluation: 0.0000[sec]
2024-05-27 22:01:09,665 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:01:09,832 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/2500.ckpt
2024-05-27 22:01:09,834 - INFO - joeynmt.training - Example #0
2024-05-27 22:01:09,834 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:01:09,834 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:01:09,834 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'started', 'these', 'di@@', 'di@@', 't', 'of', 'these', 'con@@', 'sci@@', 'ous', 're@@', 'spec@@', 't', 'that', 'the', 's@@', 'av@@', 'ing', 'the', 's@@', 'av@@', 'ed', 'by', 'three', 'million', 'years', 'that', 'has', 'been', 'about', '4@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'American', 'American', 'American', 'American', 'American', 'American', '--', '</s>']
2024-05-27 22:01:09,834 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:01:09,834 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:01:09,834 - INFO - joeynmt.training - 	Hypothesis: The last year I started these didit of these conscious respect that the saving the saved by three million years that has been about 4-year-year-year-year-year-American American American American American American --
2024-05-27 22:01:09,835 - INFO - joeynmt.training - Example #1
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'the', 'de@@', 'tec@@', 't', 'of', 'the', 'problem', 'because', 'the', 'problem', 'because', 'the', 'problem', 'you', "don't", 'know', 'that', 'the', 'so@@', 'und', 'of', 'the', 'pol@@', 'ic@@', 'y.', '</s>']
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Hypothesis: All of the detect of the problem because the problem because the problem you don't know that the sound of the policy.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - Example #2
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ul@@', 'ating', 'the', 're@@', 'spec@@', 't', 'is', 'a', 'con@@', 'sci@@', 'ous', 'of', 'the', 'con@@', 'struc@@', 'tu@@', 'ral', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'econom@@', 'ic', 'econom@@', 'ic', 'econom@@', 'ic', 'econom@@', 'y.', '</s>']
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Hypothesis: The calcululating the respect is a conscious of the constructural of the power of the power of the power of the power of the power of the power of the power of the power of the economic economic economic economy.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - Example #3
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:01:09,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'a', 'p@@', 'and@@', '-@@', 'and', 'the', 'g@@', 'ame', 'and', 'the', 'g@@', 'ame', 'of', 'the', 's@@', 'av@@', 'i@@', 'es.', '</s>']
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:01:09,835 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:01:09,836 - INFO - joeynmt.training - 	Hypothesis: It is a pand-and the game and the game of the savies.
2024-05-27 22:01:09,836 - INFO - joeynmt.training - Example #4
2024-05-27 22:01:09,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:01:09,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:01:09,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'de@@', 'al', 'is', 'going', 'to', 'be', 'a', 'very', 'much', 'from', 'the', 'last', '20', 'years', 'of', 'the', 'last', '50', 'years.', '</s>']
2024-05-27 22:01:09,836 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:01:09,836 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:01:09,836 - INFO - joeynmt.training - 	Hypothesis: The next dedeal is going to be a very much from the last 20 years of the last 50 years.
2024-05-27 22:01:26,479 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.388504, Batch Acc: 0.345530, Tokens per Sec:     4076, Lr: 0.000300
2024-05-27 22:01:42,621 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.361731, Batch Acc: 0.350098, Tokens per Sec:     4342, Lr: 0.000300
2024-05-27 22:02:00,809 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.346550, Batch Acc: 0.352344, Tokens per Sec:     3860, Lr: 0.000300
2024-05-27 22:02:18,753 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.282207, Batch Acc: 0.360930, Tokens per Sec:     3877, Lr: 0.000300
2024-05-27 22:02:36,419 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.429885, Batch Acc: 0.371294, Tokens per Sec:     3958, Lr: 0.000300
2024-05-27 22:02:36,421 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:02:36,421 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:03:53,953 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.46, ppl:  11.73, acc:   0.35, generation: 77.5250[sec], evaluation: 0.0000[sec]
2024-05-27 22:03:53,956 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:03:54,128 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/3000.ckpt
2024-05-27 22:03:54,129 - INFO - joeynmt.training - Example #0
2024-05-27 22:03:54,129 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:03:54,129 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:03:54,129 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'di@@', 'a@@', 'chi@@', 'eve', 'to', 'show', 'that', 'the', 'h@@', 'ot@@', 'ing', 'that', 'the', 'h@@', 'ot@@', 's', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'three', 'million', 'years', 'of', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'is', 'the', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'the', 'contin@@', 'ent@@', 's,', 'the', 'midd@@', 'le', 'of', 'the', 'midd@@', 'le', 'of', 'the', 'midd@@', 'le', 'of', 'the', 'percent@@', '.', '</s>']
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these diachieve to show that the hoting that the hots of the United States has three million years of years of the United States is the 48 percent of the United States, the continents, the middle of the middle of the middle of the percent.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - Example #1
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'o@@', "'s", 'the', 'same', 'g@@', 'ame', 'of', 'the', 'problem', 'because', 'you', "don't", 'have', 'a', 'good', 'good', 'thing', 'that', 'you', "don't", 'see', 'the', 'h@@', 'and', 'the', 'same', 'time.', '</s>']
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Hypothesis: Hoo's the same game of the problem because you don't have a good good thing that you don't see the hand the same time.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - Example #2
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'very', 'much', 'of', 'the', 'societ@@', 'y', 'is', 'in', 'a', 'sense', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'power', 'of', 'the', 'econom@@', 'ic', 'system@@', '.', '</s>']
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - 	Hypothesis: The calculation is in a very much of the society is in a sense of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the economic system.
2024-05-27 22:03:54,130 - INFO - joeynmt.training - Example #3
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:03:54,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'to', 'the', 'h@@', 'and@@', ',', 'and', 'you', 'get', 'the', 's@@', 'mar@@', 't', 'of', 'the', 's@@', 'outh', 'ar@@', 'm@@', 'ing', 'the', 's@@', 'and@@', '.', '</s>']
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Hypothesis: You go to the hand, and you get the smart of the south arming the sand.
2024-05-27 22:03:54,131 - INFO - joeynmt.training - Example #4
2024-05-27 22:03:54,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:03:54,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:03:54,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'mon@@', 'str@@', 'ate', 'is', 'going', 'to', 'be', 'a', 'very', 'much', 'from', 'the', 'last', '20', 'years.', '</s>']
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:03:54,131 - INFO - joeynmt.training - 	Hypothesis: The next demonstrate is going to be a very much from the last 20 years.
2024-05-27 22:04:10,566 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.417182, Batch Acc: 0.379848, Tokens per Sec:     4291, Lr: 0.000300
2024-05-27 22:04:28,738 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.366511, Batch Acc: 0.375982, Tokens per Sec:     3760, Lr: 0.000300
2024-05-27 22:04:45,786 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     2.282346, Batch Acc: 0.379741, Tokens per Sec:     4038, Lr: 0.000300
2024-05-27 22:05:03,079 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.319359, Batch Acc: 0.391122, Tokens per Sec:     4049, Lr: 0.000300
2024-05-27 22:05:20,244 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.221038, Batch Acc: 0.398941, Tokens per Sec:     4005, Lr: 0.000300
2024-05-27 22:05:20,245 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:05:20,246 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:06:32,888 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.38, ppl:  10.84, acc:   0.37, generation: 72.6349[sec], evaluation: 0.0000[sec]
2024-05-27 22:06:32,890 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:06:33,059 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/3500.ckpt
2024-05-27 22:06:33,060 - INFO - joeynmt.training - Example #0
2024-05-27 22:06:33,060 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:06:33,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:06:33,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'these', 'di@@', 'a@@', 'war@@', 'ds', 'to', 'show', 'you', 'that', 'the', 'h@@', 'ot@@', 's', 'of', 'the', 'n@@', 'ar@@', 'tic@@', 'e,', 'which', 'is', 'almost', 'about', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'percent@@', ',', 'in', 'the', 'contin@@', 'ent@@', 's,', 'the', 'contin@@', 'ent@@', 's,', "it's", 'the', '4@@', '0@@', 's.', '</s>']
2024-05-27 22:06:33,060 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:06:33,060 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:06:33,060 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these these diawards to show you that the hots of the nartice, which is almost about three million years of the 48 percent, in the continents, the continents, it's the 40s.
2024-05-27 22:06:33,060 - INFO - joeynmt.training - Example #1
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'the', 'same', 'thing', 'that', 'is', 'the', 'same', 'thing', 'of', 'the', 'problem', 'because', 'not', 'shows', 'you', 'the', 'wat@@', 'er.', '</s>']
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Hypothesis: But the same thing that is the same thing of the problem because not shows you the water.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - Example #2
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 'ot@@', '-@@', 'gl@@', 'ori@@', 'al', 'h@@', 'ar@@', 't@@', 'y', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'st@@', 'er', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system@@', '.', '</s>']
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Hypothesis: The sot-glorial harty is, in a sense, the custer system system system system system system system system system.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - Example #3
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:06:33,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'and@@', 'ing', 'and', 'you', 'get', 'the', 's@@', 'av@@', 'ing', 'of', 'the', 'est@@', '.', '</s>']
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:06:33,061 - INFO - joeynmt.training - 	Hypothesis: You go and sanding and you get the saving of the est.
2024-05-27 22:06:33,062 - INFO - joeynmt.training - Example #4
2024-05-27 22:06:33,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:06:33,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:06:33,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'ther', 'is', 'going', 'to', 'be', 'a', 'long', 'long', 'as', 'a', 'little', 'bit', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:06:33,062 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:06:33,062 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:06:33,062 - INFO - joeynmt.training - 	Hypothesis: The next deather is going to be a long long as a little bit of the last 25 years.
2024-05-27 22:06:50,222 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.362050, Batch Acc: 0.395223, Tokens per Sec:     4027, Lr: 0.000300
2024-05-27 22:07:07,138 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.303258, Batch Acc: 0.402173, Tokens per Sec:     4065, Lr: 0.000300
2024-05-27 22:07:23,909 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.243307, Batch Acc: 0.405733, Tokens per Sec:     4098, Lr: 0.000300
2024-05-27 22:07:42,614 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.337758, Batch Acc: 0.407981, Tokens per Sec:     3647, Lr: 0.000300
2024-05-27 22:07:59,602 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.174045, Batch Acc: 0.412534, Tokens per Sec:     4123, Lr: 0.000300
2024-05-27 22:07:59,603 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:07:59,603 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:09:11,372 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.30, ppl:   9.96, acc:   0.39, generation: 71.7614[sec], evaluation: 0.0000[sec]
2024-05-27 22:09:11,375 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:09:11,544 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/4000.ckpt
2024-05-27 22:09:11,546 - INFO - joeynmt.training - Example #0
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'di@@', 'a@@', 'chi@@', 'ev@@', 'ing', 'for', 'de@@', 'vic@@', 'es', 'to', 'show', 'that', 'the', 're@@', 'ac@@', 'tic@@', 'e,', 'which', 'is', 'almost', 'for', 'three', 'million', 'years', 'of', 'years', 'of', 'a', '4@@', '8', 'percent', 'of', 'the', 'contin@@', 'ent@@', 's,', 'is', 'a', 'different', 'different', 'percent@@', ',', "it's", 'a', 'long', 'percent@@', '.', '</s>']
2024-05-27 22:09:11,546 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:09:11,546 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:09:11,546 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these diachieving for devices to show that the reactice, which is almost for three million years of years of a 48 percent of the continents, is a different different percent, it's a long percent.
2024-05-27 22:09:11,546 - INFO - joeynmt.training - Example #1
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:09:11,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'all', 'of', 'this', 'is', 'the', 'se@@', 'ver@@', 'al', 'of', 'the', 'problem', 'because', 'you', "don't", 'understand', 'the', 'cho@@', 'ice', 'of', 'the', 'cho@@', 'ice', 'of', 'the', 'cho@@', 'ice.', '</s>']
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Hypothesis: And all of this is the several of the problem because you don't understand the choice of the choice of the choice.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - Example #2
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'pa@@', 'th@@', 'y', 'of', 'the', 're@@', 'ac@@', 'tion', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cu@@', 'st@@', 'er', 'of', 'the', 'cli@@', 'mat@@', 'h', 'of', 'the', 'cli@@', 'mat@@', 'e.', '</s>']
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Hypothesis: The pathy of the reaction is, in a sense, the custer of the climath of the climate.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - Example #3
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 're@@', 'ver@@', 'ver@@', 'b@@', 'b@@', 'b@@', 'b@@', 'y@@', 'et', 'and', 'you', 'get', 'to', 'the', 'ex@@', 'pensi@@', 've.', '</s>']
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - 	Hypothesis: You go and reververbbbbyet and you get to the expensive.
2024-05-27 22:09:11,547 - INFO - joeynmt.training - Example #4
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:09:11,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'gre@@', 'e', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 're@@', 'duc@@', 'ed', 'by', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:09:11,548 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:09:11,548 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:09:11,548 - INFO - joeynmt.training - 	Hypothesis: The next degree is going to be a fast reduced by the last 25 years.
2024-05-27 22:09:29,095 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.153938, Batch Acc: 0.414114, Tokens per Sec:     3902, Lr: 0.000300
2024-05-27 22:09:48,400 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.216839, Batch Acc: 0.411892, Tokens per Sec:     3626, Lr: 0.000300
2024-05-27 22:10:06,895 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.185560, Batch Acc: 0.416734, Tokens per Sec:     3675, Lr: 0.000300
2024-05-27 22:10:25,579 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.148396, Batch Acc: 0.423840, Tokens per Sec:     3743, Lr: 0.000300
2024-05-27 22:10:44,619 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.212034, Batch Acc: 0.430578, Tokens per Sec:     3695, Lr: 0.000300
2024-05-27 22:10:44,620 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:10:44,620 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:11:51,233 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.24, ppl:   9.42, acc:   0.40, generation: 66.6055[sec], evaluation: 0.0000[sec]
2024-05-27 22:11:51,237 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:11:51,408 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/4500.ckpt
2024-05-27 22:11:51,409 - INFO - joeynmt.training - Example #0
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'de@@', 'al', 'with', 'these', 'de@@', 'al', 'de@@', 'al', 'to', 'show', 'that', 'the', 'sc@@', 'ale', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', "it's", 'been', 'in', 'the', 'United', 'St@@', 'at@@', 'es,', "it's", 'been', 'in', 'the', '4@@', '0@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'year@@', '-@@', 'old', 'old', 'percent@@', '.', '</s>']
2024-05-27 22:11:51,409 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:11:51,409 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:11:51,409 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these deal with these deal deal to show that the scale of the size of the size of the size of the size of the size of the United States, it's been in the United States, it's been in the 40-year-year-year-old old percent.
2024-05-27 22:11:51,409 - INFO - joeynmt.training - Example #1
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:11:51,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'the', 'fact', 'this', 'sub@@', 'jec@@', 't', 'of', 'the', 'problem', 'because', 'it', 'is', 'not', 'a', 'lot', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Hypothesis: But the fact this subject of the problem because it is not a lot of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - Example #2
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'is', 'a', 'gl@@', 'aci@@', 'al', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'b@@', 's', 'of', 'the', 'cli@@', 'mat@@', 'h', 'of', 'the', 'global', 'war@@', 'm@@', 'ing.', '</s>']
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Hypothesis: The glacial is a glacial is in a sense, the heart of the climbs of the climath of the global warming.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - Example #3
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 'going', 'to', 'go', 'and', 're@@', 'ver@@', 'sion', 'and', 'it', 'will', 'be', 'in', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - 	Hypothesis: It was going to go and reversion and it will be in the summer.
2024-05-27 22:11:51,410 - INFO - joeynmt.training - Example #4
2024-05-27 22:11:51,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:11:51,411 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:11:51,411 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '25', 'years', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:11:51,411 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:11:51,411 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:11:51,411 - INFO - joeynmt.training - 	Hypothesis: The next 25 years of the last 25 years.
2024-05-27 22:12:10,689 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.132688, Batch Acc: 0.426340, Tokens per Sec:     3496, Lr: 0.000300
2024-05-27 22:12:29,691 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     2.103980, Batch Acc: 0.428018, Tokens per Sec:     3654, Lr: 0.000300
2024-05-27 22:12:48,054 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.972604, Batch Acc: 0.432753, Tokens per Sec:     3740, Lr: 0.000300
2024-05-27 22:13:05,625 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     2.112305, Batch Acc: 0.435623, Tokens per Sec:     3885, Lr: 0.000300
2024-05-27 22:13:23,455 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.830340, Batch Acc: 0.435727, Tokens per Sec:     3906, Lr: 0.000300
2024-05-27 22:13:23,456 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:13:23,456 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:14:43,479 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.19, ppl:   8.97, acc:   0.41, generation: 80.0158[sec], evaluation: 0.0000[sec]
2024-05-27 22:14:43,482 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:14:43,651 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/5000.ckpt
2024-05-27 22:14:43,652 - INFO - joeynmt.training - Example #0
2024-05-27 22:14:43,652 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:14:43,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:14:43,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'went', 'back', 'to', 'the', 'right', 'right', 'now', 'now', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'te', 'of', 'the', 'h@@', 'ot@@', 'ter', 'gl@@', 'aci@@', 'al', 'in', 'a', 'few', 'million', 'years', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'S.', 'and', 'the', 'size', 'of', 'the', '4@@', '8', 'percent@@', ',', 'which', 'is', 're@@', 'ro@@', 't@@', '-@@', 'year@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:14:43,652 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Hypothesis: And I went back to the right right now now to show that the calote of the hotter glacial in a few million years of the size of the 48 S. and the size of the 48 percent, which is rerot-year-percent.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - Example #1
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'all', 'the', 'sub@@', 'si@@', 'si@@', 'z@@', 'ing', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', 'is', 'not', 'the', 'right', 'right', 'right', 'now.', '</s>']
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Hypothesis: But all the subsisizing the gravity of the problem because it is not the right right right now.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - Example #2
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'gl@@', 'aci@@', 'al', 'ar@@', 't@@', 'able', 'is', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'cli@@', 'mat@@', 'h', 'of', 'the', 'cli@@', 'mat@@', 'ical', 'system@@', '.', '</s>']
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - 	Hypothesis: The calculation glacial artable is a certain sense, the heart of the climath of the climatical system.
2024-05-27 22:14:43,653 - INFO - joeynmt.training - Example #3
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:14:43,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'and@@', 'p', 'and', 'you', 'get', 'the', 'ext@@', 'ent', 'of', 'the', 'est@@', 'im@@', 'ate', 'of', 'the', 'est@@', '.', '</s>']
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Hypothesis: You go and sandp and you get the extent of the estimate of the est.
2024-05-27 22:14:43,654 - INFO - joeynmt.training - Example #4
2024-05-27 22:14:43,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:14:43,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:14:43,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'es', 'is', 'going', 'to', 'be', 'a', 'very', 'quick@@', 'ly', 'as', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:14:43,654 - INFO - joeynmt.training - 	Hypothesis: The next next devices is going to be a very quickly as the last 25 years.
2024-05-27 22:15:01,274 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     2.326418, Batch Acc: 0.439314, Tokens per Sec:     3843, Lr: 0.000300
2024-05-27 22:15:18,697 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     2.019309, Batch Acc: 0.432024, Tokens per Sec:     3888, Lr: 0.000300
2024-05-27 22:15:36,564 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.141855, Batch Acc: 0.435205, Tokens per Sec:     3876, Lr: 0.000300
2024-05-27 22:15:54,441 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     2.197417, Batch Acc: 0.436094, Tokens per Sec:     3832, Lr: 0.000300
2024-05-27 22:15:58,166 - INFO - joeynmt.training - Epoch   2: total training loss 9272.04
2024-05-27 22:15:58,166 - INFO - joeynmt.training - EPOCH 3
2024-05-27 22:16:12,323 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.197438, Batch Acc: 0.463028, Tokens per Sec:     3837, Lr: 0.000300
2024-05-27 22:16:12,325 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:16:12,325 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:17:20,093 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.15, ppl:   8.57, acc:   0.43, generation: 67.7604[sec], evaluation: 0.0000[sec]
2024-05-27 22:17:20,096 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:17:20,265 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/5500.ckpt
2024-05-27 22:17:20,266 - INFO - joeynmt.training - Example #0
2024-05-27 22:17:20,266 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:17:20,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:17:20,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'es', 'these', 'de@@', 'vic@@', 'es', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'te', 'gl@@', 'aci@@', 'al', 'of', 'gl@@', 'aci@@', 'al', 'and', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'is', 're@@', 'ver@@', 'ver@@', 'se', 'by', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Hypothesis: I showed these devices these devices to show that the calote glacial of glacial and almost three million years of the size of the size of the size of 48 United States is reververse by 40-percent.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - Example #1
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'st@@', 'ance', 'because', 'it', "doesn't", 'shows', 'the', 'problem', 'because', 'it', "doesn't", 'shows', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'it', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'ice', 'because', 'it', 'is', 'not', 'the', 'real', 'problem', 'because', 'it', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it']
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Hypothesis: Hoowever, this substance because it doesn't shows the problem because it doesn't shows the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice because it doesn't show it is the gravity of the ice because it is not the real problem because it is the gravity of the problem because it
2024-05-27 22:17:20,267 - INFO - joeynmt.training - Example #2
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:17:20,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'tic', 'is', 'a', 'certain', 'sen@@', 'se,', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'system', 'system', 'system@@', '.', '</s>']
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - 	Hypothesis: The glacial calotic is a certain sense, in a certain sense, the heart of the system system system.
2024-05-27 22:17:20,267 - INFO - joeynmt.training - Example #3
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'go', 'and', 're@@', 'ver@@', 'se', 'and', 'you', 'get', 'the', 'ext@@', 'inc@@', 't', 'of', 'the', 'ext@@', 'inc@@', 't', 'of', 'the', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Hypothesis: You go and go and reverse and you get the extinct of the extinct of the extinct.
2024-05-27 22:17:20,268 - INFO - joeynmt.training - Example #4
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:17:20,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', '25', 'years', 'will', 'be', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:17:20,268 - INFO - joeynmt.training - 	Hypothesis: The next next 25 years will be a little bit of a little bit of the last 25 years.
2024-05-27 22:17:37,146 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     2.010486, Batch Acc: 0.464379, Tokens per Sec:     4022, Lr: 0.000300
2024-05-27 22:17:54,876 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.034906, Batch Acc: 0.462335, Tokens per Sec:     3904, Lr: 0.000300
2024-05-27 22:18:12,752 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.791296, Batch Acc: 0.459504, Tokens per Sec:     3906, Lr: 0.000300
2024-05-27 22:18:30,842 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.964238, Batch Acc: 0.457351, Tokens per Sec:     3893, Lr: 0.000300
2024-05-27 22:18:48,716 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     2.044531, Batch Acc: 0.461560, Tokens per Sec:     3803, Lr: 0.000300
2024-05-27 22:18:48,717 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:18:48,718 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:20:00,722 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.12, ppl:   8.36, acc:   0.43, generation: 71.9971[sec], evaluation: 0.0000[sec]
2024-05-27 22:20:00,724 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:20:00,893 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/6000.ckpt
2024-05-27 22:20:00,894 - INFO - joeynmt.training - Example #0
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'es', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 'ter', 'is', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 'ter', 'of', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'percent@@', ',', 'you', 'have', 'the', 'stre@@', 't@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these devices to show that the calotter is to show that the calotter of three million years of the size of the size of the size of the 48 percent, you have the stret-percent.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - Example #1
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'y', 'this', 'sub@@', 'st@@', 'it@@', 'ted', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pres@@', 'sure', 'that', 'it', 'is', 'not', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - 	Hypothesis: Howey this substitted of the problem because it doesn't expressure that it is not the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice problem.
2024-05-27 22:20:00,895 - INFO - joeynmt.training - Example #2
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:20:00,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'ar@@', 'tic@@', 'le', 'is', 'a', 'certain', 'sen@@', 'se,', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'system', 'system', 'system', 'system', 'of', 'the', 'global', 'system', 'system', '</s>']
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Hypothesis: The glacial article is a certain sense, in a certain sense, the heart of the system system system system of the global system system
2024-05-27 22:20:00,896 - INFO - joeynmt.training - Example #3
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'and@@', 's', 'and', 're@@', 'ver@@', ',', 'and', 'you', 'get', 'the', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Hypothesis: You go and sands and rever, and you get the extinct.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - Example #4
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:20:00,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'very', 'quick@@', 'ly', 'as', 'a', 'very', 'very', 'quick@@', 'ly', 'that', 'has', 'been', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:20:00,896 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a very quickly as a very very quickly that has been in the last 25 years.
2024-05-27 22:20:17,421 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     2.054420, Batch Acc: 0.466046, Tokens per Sec:     4160, Lr: 0.000300
2024-05-27 22:20:34,715 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     2.047287, Batch Acc: 0.466493, Tokens per Sec:     4003, Lr: 0.000300
2024-05-27 22:20:51,773 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.952212, Batch Acc: 0.465144, Tokens per Sec:     4195, Lr: 0.000300
2024-05-27 22:21:10,066 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     2.042449, Batch Acc: 0.462264, Tokens per Sec:     3801, Lr: 0.000300
2024-05-27 22:21:28,441 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.999178, Batch Acc: 0.467300, Tokens per Sec:     3819, Lr: 0.000300
2024-05-27 22:21:28,443 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:21:28,443 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:22:36,985 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.10, acc:   0.44, generation: 68.5355[sec], evaluation: 0.0000[sec]
2024-05-27 22:22:36,989 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:22:37,153 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/6500.ckpt
2024-05-27 22:22:37,155 - INFO - joeynmt.training - Example #0
2024-05-27 22:22:37,155 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:22:37,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:22:37,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'tim@@', 's', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ator@@', ',', 'which', 'is', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'percent@@', ',', 'the', 'contin@@', 'ent@@', 's,', 'is', 're@@', 'ver@@', 'ver@@', 'ti@@', 'p@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these devictims to show that the calculator, which is for almost three million years has had the size of three million years has had the size of 48 percent, the continents, is reververtip-percent.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - Example #1
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'jec@@', 't', 'to', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'not', 'the', 'ice', 'of', 'the', 'ice', 'of']
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Hypothesis: But this subject to the gravity of the problem because it doesn't show the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the ice problem because it is not the ice of the ice of
2024-05-27 22:22:37,156 - INFO - joeynmt.training - Example #2
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'to', 'ar@@', 't@@', 'tic@@', 'al', 'ar@@', 't@@', 'le,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - 	Hypothesis: The calculation is to arttical artle, the heart of the climate system.
2024-05-27 22:22:37,156 - INFO - joeynmt.training - Example #3
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:22:37,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'av@@', 'es', 'and', 're@@', 'ver@@', 'se', 'and', 'you', 'get', 're@@', 'ver@@', 'se', 'to', 'ext@@', 'inc@@', 'e.', '</s>']
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Hypothesis: You go and saves and reverse and you get reverse to extince.
2024-05-27 22:22:37,157 - INFO - joeynmt.training - Example #4
2024-05-27 22:22:37,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:22:37,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:22:37,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'is', 'going', 'to', 'be', 'a', 'very', 'quick@@', 'ly', 'de@@', 'gre@@', 'es', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:22:37,157 - INFO - joeynmt.training - 	Hypothesis: The next devices is going to be a very quickly degrees of the last 25 years.
2024-05-27 22:22:54,326 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     2.100451, Batch Acc: 0.466582, Tokens per Sec:     4028, Lr: 0.000300
2024-05-27 22:23:12,095 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.984972, Batch Acc: 0.465639, Tokens per Sec:     3876, Lr: 0.000300
2024-05-27 22:23:29,686 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.770920, Batch Acc: 0.471954, Tokens per Sec:     3928, Lr: 0.000300
2024-05-27 22:23:47,195 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.830496, Batch Acc: 0.465550, Tokens per Sec:     3909, Lr: 0.000300
2024-05-27 22:24:05,497 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.865924, Batch Acc: 0.466907, Tokens per Sec:     3718, Lr: 0.000300
2024-05-27 22:24:05,498 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:24:05,498 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:25:12,055 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.95, acc:   0.44, generation: 66.5497[sec], evaluation: 0.0000[sec]
2024-05-27 22:25:12,059 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:25:12,228 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/7000.ckpt
2024-05-27 22:25:12,230 - INFO - joeynmt.training - Example #0
2024-05-27 22:25:12,230 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:25:12,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:25:12,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'spen@@', 't', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'es', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'tic', 'ac@@', 'tic@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'C@@', 'an', 'an', 'an', 'C@@', 'an', 'an', 'an', 'an', 'contin@@', 'ent@@', 's,', '</s>']
2024-05-27 22:25:12,230 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:25:12,230 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Hypothesis: The year I spent these slides to show that the caloes to show that the calotic actice, which for almost three million years of the 48 Can Can Can Can Can Can Can Can Can Can Can Can an an Can an an an continents,
2024-05-27 22:25:12,231 - INFO - joeynmt.training - Example #1
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'jec@@', 't', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'hi@@', 'hi@@', 'p', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Hypothesis: But this subject of the gravity of the problem because it doesn't exhihip of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice problem.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - Example #2
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'cal@@', 'o@@', 'tic', 'c@@', 'alc@@', 'ul@@', 'ator@@', ',', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'cli@@', 'mat@@', 'h', 'of', 'the', 'cli@@', 'mat@@', 'e.', '</s>']
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - 	Hypothesis: The calotic calculator, in a certain sense, the heart of the climath of the climate.
2024-05-27 22:25:12,231 - INFO - joeynmt.training - Example #3
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:25:12,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'to', 'the', 'in@@', 'ver@@', 'se', 'and', 'you', 'get', 'the', 'ext@@', 'rem@@', 'end@@', 'ous', 'and', 'you', 'get', 'the', 'ext@@', 'rem@@', 'end@@', 'ous', 'of', 'the', 'ext@@', 'rem@@', 'end@@', 'ous', 'and', 'you', 'get', 'the', 'ext@@', 'rem@@', 'end@@', 'ous', 'and', 'you', 'get', 'the', 'ext@@', 'rem@@', 'end@@', 'ous', 'and', 'you', 'get', 'the', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Hypothesis: You go to the inverse and you get the extremendous and you get the extremendous of the extremendous and you get the extremendous and you get the extremendous and you get the extinct.
2024-05-27 22:25:12,232 - INFO - joeynmt.training - Example #4
2024-05-27 22:25:12,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:25:12,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:25:12,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'rapi@@', 'd', 'pa@@', 'per', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:25:12,232 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a rapid paper is going to be a rapid of the last 25 years.
2024-05-27 22:25:30,003 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.717681, Batch Acc: 0.466798, Tokens per Sec:     3838, Lr: 0.000300
2024-05-27 22:25:47,967 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.959267, Batch Acc: 0.477917, Tokens per Sec:     3945, Lr: 0.000300
2024-05-27 22:26:05,681 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.851843, Batch Acc: 0.477445, Tokens per Sec:     3999, Lr: 0.000300
2024-05-27 22:26:23,276 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.837024, Batch Acc: 0.472523, Tokens per Sec:     4038, Lr: 0.000300
2024-05-27 22:26:41,267 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     2.002050, Batch Acc: 0.479712, Tokens per Sec:     3999, Lr: 0.000300
2024-05-27 22:26:41,269 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:26:41,269 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:27:49,566 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.05, ppl:   7.73, acc:   0.45, generation: 68.2902[sec], evaluation: 0.0000[sec]
2024-05-27 22:27:49,568 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:27:49,732 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/7500.ckpt
2024-05-27 22:27:49,733 - INFO - joeynmt.training - Example #0
2024-05-27 22:27:49,733 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:27:49,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'tic', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'tic', 'c@@', 'alc@@', 'ul@@', 'ator@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'is', 're@@', 'ally,', "it's", 'been', 'ro@@', 'w@@', 'ed', 'by', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to show that the calotic to show that the calotic calculator, which for almost three million years had the size of 48 U.S. continents, is really, it's been rowed by 40-percent.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - Example #1
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', "that's", 'the', 'grav@@', 'ity', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Hypothesis: But that's the gravity of the gravity of the problem because it doesn't exhibition of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice problem.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - Example #2
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:27:49,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'a', 'very', 'much', 'of', 'the', 'clim@@', 'ate', 'system', 'is', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'clim@@', 'ate', 'system', '</s>']
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:27:49,734 - INFO - joeynmt.training - 	Hypothesis: The calculation is a very much of the climate system is a certain sense, the heart of climate system
2024-05-27 22:27:49,734 - INFO - joeynmt.training - Example #3
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'p@@', 'and@@', 'ing', 'and', 're@@', 're@@', 'si@@', 't', 'in', 'the', 'ext@@', 'rem@@', 'ely', '</s>']
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Hypothesis: You go and panding and reresit in the extremely
2024-05-27 22:27:49,735 - INFO - joeynmt.training - Example #4
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:27:49,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'is', 'going', 'to', 'be', 'a', 'f@@', 'ast', 'f@@', 'ol@@', 'ded', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:27:49,735 - INFO - joeynmt.training - 	Hypothesis: The next devices is going to be a fast folded the last 25 years.
2024-05-27 22:28:06,385 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.746071, Batch Acc: 0.473458, Tokens per Sec:     4213, Lr: 0.000300
2024-05-27 22:28:23,129 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.696388, Batch Acc: 0.475485, Tokens per Sec:     4049, Lr: 0.000300
2024-05-27 22:28:38,850 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     2.046332, Batch Acc: 0.474962, Tokens per Sec:     4473, Lr: 0.000300
2024-05-27 22:28:55,028 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.836329, Batch Acc: 0.476138, Tokens per Sec:     4294, Lr: 0.000300
2024-05-27 22:29:11,484 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.800590, Batch Acc: 0.476724, Tokens per Sec:     4179, Lr: 0.000300
2024-05-27 22:29:11,486 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:29:11,486 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:30:14,670 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.03, ppl:   7.60, acc:   0.45, generation: 63.1770[sec], evaluation: 0.0000[sec]
2024-05-27 22:30:14,671 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:30:14,838 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/8000.ckpt
2024-05-27 22:30:14,840 - INFO - joeynmt.training - Example #0
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'just', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'es', 'I', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'es', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'tic', 'ac@@', 'tic@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '8', 'U@@', 'S', 'S', 'C@@', 'ent@@', 'al', 'in', 'the', '4@@', '0@@', '8', 'U@@', 'S', 'C@@', 'ent@@', 'al', 'of', 'the', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:30:14,840 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:30:14,840 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:30:14,840 - INFO - joeynmt.training - 	Hypothesis: I just showed these devices I showed these devices to show that the calotic actic, which is about three million years had the size of 48 US S Cental in the 408 US Cental of the 40-percent.
2024-05-27 22:30:14,840 - INFO - joeynmt.training - Example #1
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:30:14,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of the gravity of the problem because it doesn't explain the ice problem.
2024-05-27 22:30:14,841 - INFO - joeynmt.training - Example #2
2024-05-27 22:30:14,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:30:14,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:30:14,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'cal@@', 'ot@@', '-@@', 'h@@', 'ot@@', 'ter', 'cal@@', 'o@@', 'tic', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', '</s>']
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:30:14,841 - INFO - joeynmt.training - 	Hypothesis: The calot-hotter calotic is in a sense, the heart heart of the climate system
2024-05-27 22:30:14,841 - INFO - joeynmt.training - Example #3
2024-05-27 22:30:14,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:30:14,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:30:14,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'go', 'and', 'you', 'get', 'the', 'w@@', 'av@@', 'es', 'and', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', '</s>']
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Hypothesis: You go and go and you get the waves and return into the summer of the summer
2024-05-27 22:30:14,842 - INFO - joeynmt.training - Example #4
2024-05-27 22:30:14,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:30:14,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:30:14,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'es', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'for', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:30:14,842 - INFO - joeynmt.training - 	Hypothesis: The next next devices is going to be a rapid for the last 25 years.
2024-05-27 22:30:31,158 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.925796, Batch Acc: 0.478161, Tokens per Sec:     4184, Lr: 0.000300
2024-05-27 22:30:46,533 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.836988, Batch Acc: 0.477654, Tokens per Sec:     4542, Lr: 0.000300
2024-05-27 22:31:01,772 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     2.044631, Batch Acc: 0.481797, Tokens per Sec:     4523, Lr: 0.000300
2024-05-27 22:31:17,763 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.741347, Batch Acc: 0.482960, Tokens per Sec:     4358, Lr: 0.000300
2024-05-27 22:31:33,056 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.754874, Batch Acc: 0.482724, Tokens per Sec:     4565, Lr: 0.000300
2024-05-27 22:31:33,057 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:31:33,057 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:32:40,175 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.44, acc:   0.46, generation: 67.1109[sec], evaluation: 0.0000[sec]
2024-05-27 22:32:40,176 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:32:40,345 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/8500.ckpt
2024-05-27 22:32:40,346 - INFO - joeynmt.training - Example #0
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'cal@@', 'o@@', 'te', 'of', 'the', 'ar@@', 'tic@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'c@@', 'la@@', 'y', 'is', 'a', 'stre@@', 't@@', '-@@', 'A@@', 'g@@', 'g@@', 'g@@', 'o,', 'is', 'a', '4@@', '0@@', '-@@', '8@@', '-@@', 'and@@', '-@@', '8@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slide to show that the calculation that the calote of the artic, which is about three million years had the size of 48 U.S. clay is a stret-Agggo, is a 40-8-and-8-percent.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - Example #1
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'gl@@', 'aci@@', 'er.', '</s>']
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Hypothesis: However, this subvalue of the problem because it doesn't show the glacier.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - Example #2
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:32:40,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'a', 'certain', 'sen@@', 'se,', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:32:40,347 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Hypothesis: The calculation is a certain sense, in a certain sense, the heart customer of the global climate system.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - Example #3
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'put', 'it', 'and', 're@@', 'turn', 'it', 'and', 'you', 'get', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'it', 'in', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'it.', '</s>']
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Hypothesis: You go and put it and return it and you get the summer of the summer of the summer of the summer and you get it in the summer of the summer and return it into it.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - Example #4
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:32:40,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'vic@@', 'e,', 'is', 'a', 'qu@@', 'ick', 'ro@@', 'll@@', 'ed', 'the', 'last', '25', 'years', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:32:40,348 - INFO - joeynmt.training - 	Hypothesis: The next next device, is a quick rolled the last 25 years of the last 25 years.
2024-05-27 22:32:55,686 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.785490, Batch Acc: 0.485275, Tokens per Sec:     4389, Lr: 0.000300
2024-05-27 22:33:11,478 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.775367, Batch Acc: 0.487414, Tokens per Sec:     4345, Lr: 0.000300
2024-05-27 22:33:27,906 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.721406, Batch Acc: 0.487952, Tokens per Sec:     4156, Lr: 0.000300
2024-05-27 22:33:44,846 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     2.050315, Batch Acc: 0.481050, Tokens per Sec:     4101, Lr: 0.000300
2024-05-27 22:34:01,439 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.926346, Batch Acc: 0.487288, Tokens per Sec:     4191, Lr: 0.000300
2024-05-27 22:34:01,439 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:34:01,439 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:35:20,790 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.27, acc:   0.46, generation: 79.3436[sec], evaluation: 0.0000[sec]
2024-05-27 22:35:20,791 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:35:20,953 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/9000.ckpt
2024-05-27 22:35:20,954 - INFO - joeynmt.training - Example #0
2024-05-27 22:35:20,954 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:35:20,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:35:20,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 's@@', 'mo@@', 'ther@@', "'s", 'pa@@', 'per', 'that', 'the', 's@@', 'ca@@', 'p', 'of', 'the', '4@@', '8', 'Americ@@', 'a@@', "'s", 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'has', 'been', 'stre@@', 't@@', 'ch', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'is', 're@@', 'stre@@', 't@@', 'ch', 'of', 'the', '4@@', '0@@', '-@@', '8@@', '-@@', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:35:20,954 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:35:20,954 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the smother's paper that the scap of the 48 America's size of 48 U.S. has been stretch of 48 U.S. continents, is restretch of the 40-8-40-percent.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - Example #1
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', '-@@', 'defin@@', 'ed', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'ex@@', 'ten@@', 'd', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'not', 'sp@@', 'r@@', 'ing.', '</s>']
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Hypothesis: But this under-defined gravity of the problem because it doesn't show the ice of the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't extend of the ice problem because it doesn't show the ice of the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the ice problem because it is not spring.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - Example #2
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'tic', 'cal@@', 'o@@', 'tic', 'cal@@', 'o@@', 'tic', 'c@@', 'alc@@', 'ul@@', 'ous', 'system', 'is', 'a', 'lot', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - 	Hypothesis: The glacial calotic calotic calotic calculous system is a lot of global climate system.
2024-05-27 22:35:20,955 - INFO - joeynmt.training - Example #3
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:35:20,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'you', 'go', 'and', 're@@', 'ver@@', 'se', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'sum@@', 'm@@', 'er', '</s>']
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Hypothesis: You go and you go and reverse and you get into the summer of summer
2024-05-27 22:35:20,956 - INFO - joeynmt.training - Example #4
2024-05-27 22:35:20,956 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:35:20,956 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:35:20,956 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'will', 'be', 'a', 'very', 'quick@@', 'ly', 'f@@', 'ast', 'su@@', 'd@@', 'den@@', 's', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:35:20,956 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick will be a very quickly fast suddens of the last 25 years.
2024-05-27 22:35:37,742 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.777695, Batch Acc: 0.486634, Tokens per Sec:     4164, Lr: 0.000300
2024-05-27 22:35:53,874 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.745109, Batch Acc: 0.488184, Tokens per Sec:     4287, Lr: 0.000300
2024-05-27 22:36:10,063 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.732049, Batch Acc: 0.489785, Tokens per Sec:     4318, Lr: 0.000300
2024-05-27 22:36:22,123 - INFO - joeynmt.training - Epoch   3: total training loss 7653.97
2024-05-27 22:36:22,124 - INFO - joeynmt.training - EPOCH 4
2024-05-27 22:36:26,935 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.751530, Batch Acc: 0.511792, Tokens per Sec:     4195, Lr: 0.000300
2024-05-27 22:36:42,639 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.759529, Batch Acc: 0.507810, Tokens per Sec:     4407, Lr: 0.000300
2024-05-27 22:36:42,639 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:36:42,639 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:37:40,823 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.17, acc:   0.47, generation: 58.1769[sec], evaluation: 0.0000[sec]
2024-05-27 22:37:40,824 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:37:40,987 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/9500.ckpt
2024-05-27 22:37:40,988 - INFO - joeynmt.training - Example #0
2024-05-27 22:37:40,988 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:37:40,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:37:40,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'just', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'cal@@', 'o@@', 'te', 'gl@@', 'aci@@', 'al', 'aci@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '0@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'you', 'have', 'a', '4@@', '0@@', '-@@', 'U.@@', 'S.', '</s>']
2024-05-27 22:37:40,988 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:37:40,988 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:37:40,988 - INFO - joeynmt.training - 	Hypothesis: I just showed these slides to show these slides to show that the calote glacial acial artica, which for almost three million years has had the size of 408 U.S. continents, you have a 40-U.S.
2024-05-27 22:37:40,988 - INFO - joeynmt.training - Example #1
2024-05-27 22:37:40,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', 'shows', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'ice.', '</s>']
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue of the problem because it shows the ice of the ice problem of the ice.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - Example #2
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'cal@@', 'ot@@', 'ot@@', 'ot@@', 'ot@@', 'ot@@', 'ot@@', 'ot@@', 's', 'is,', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', '</s>']
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Hypothesis: The calototototototots is, in a way, the heart of the climate system of the climate system of the climate system
2024-05-27 22:37:40,989 - INFO - joeynmt.training - Example #3
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:37:40,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'put', 'it', 'and', 're@@', 'ver@@', 'se', 'and', 're@@', 're@@', 're@@', 'ach', 'the', 'sum@@', 'm@@', 'er', 'of', 'ex@@', 'ten@@', 'd', '</s>']
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:37:40,989 - INFO - joeynmt.training - 	Hypothesis: You go and put it and reverse and rerereach the summer of extend
2024-05-27 22:37:40,989 - INFO - joeynmt.training - Example #4
2024-05-27 22:37:40,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:37:40,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:37:40,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'rapi@@', 'd', 'very', 'rapi@@', 'd', 'the', 'su@@', 'd@@', 'den@@', 'ly', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:37:40,990 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:37:40,990 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:37:40,990 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a rapid very rapid the suddenly of the last 25 years.
2024-05-27 22:37:57,505 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.653922, Batch Acc: 0.506964, Tokens per Sec:     4206, Lr: 0.000300
2024-05-27 22:38:13,636 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.794799, Batch Acc: 0.509988, Tokens per Sec:     4332, Lr: 0.000300
2024-05-27 22:38:29,696 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.929705, Batch Acc: 0.504604, Tokens per Sec:     4389, Lr: 0.000300
2024-05-27 22:38:45,959 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     1.779173, Batch Acc: 0.510432, Tokens per Sec:     4182, Lr: 0.000300
2024-05-27 22:39:01,965 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.781916, Batch Acc: 0.506436, Tokens per Sec:     4340, Lr: 0.000300
2024-05-27 22:39:01,967 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:39:01,967 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:40:06,235 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.12, acc:   0.47, generation: 64.2618[sec], evaluation: 0.0000[sec]
2024-05-27 22:40:06,237 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:40:06,404 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/10000.ckpt
2024-05-27 22:40:06,405 - INFO - joeynmt.training - Example #0
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'de@@', 'vic@@', 'es', 'to', 'show', 'these', 'de@@', 'vic@@', 'es', 'that', 'the', 'cal@@', 'ot@@', 'ter', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'has', 'been', 'stre@@', 't@@', 'ch', 'of', 'the', '4@@', '0@@', '8', 'U.@@', 'S.', '</s>']
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these devices to show these devices that the calotter of the artica, which for almost three million years had the size of 48 U.S. has been stretch of the 408 U.S.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - Example #1
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'all', 'of', 'this', 'under@@', 'p@@', 'us@@', 'es', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'tr@@', 'act', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'gl@@', 'aci@@', 'er.', '</s>']
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Hypothesis: And all of this underpuses the problem because it doesn't show the extract of the problem because it doesn't show the ice of the ice problem of the glacier.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - Example #2
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:40:06,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'et', 'is', 'a', 'certain', 'sen@@', 'se,', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', '--', '</s>']
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:40:06,406 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Hypothesis: The articket is a certain sense, in a certain sense, the heart heart of the climate system --
2024-05-27 22:40:06,407 - INFO - joeynmt.training - Example #3
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'low@@', 'ing', 'and', 'you', 'get', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Hypothesis: You go and slowing and you get return into the summer of summer.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - Example #4
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:40:06,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'few', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'for', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:40:06,407 - INFO - joeynmt.training - 	Hypothesis: The next next few devices will be a quick for the last 25 years.
2024-05-27 22:40:21,868 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     1.633808, Batch Acc: 0.511853, Tokens per Sec:     4337, Lr: 0.000300
2024-05-27 22:40:37,522 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     1.777301, Batch Acc: 0.507965, Tokens per Sec:     4372, Lr: 0.000300
2024-05-27 22:40:53,675 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     1.814151, Batch Acc: 0.502024, Tokens per Sec:     4451, Lr: 0.000300
2024-05-27 22:41:10,250 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     1.915589, Batch Acc: 0.505341, Tokens per Sec:     4242, Lr: 0.000300
2024-05-27 22:41:26,468 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     1.712587, Batch Acc: 0.504740, Tokens per Sec:     4397, Lr: 0.000300
2024-05-27 22:41:26,468 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:41:26,468 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:42:32,168 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.06, acc:   0.47, generation: 65.6936[sec], evaluation: 0.0000[sec]
2024-05-27 22:42:32,169 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:42:32,334 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/10500.ckpt
2024-05-27 22:42:32,335 - INFO - joeynmt.training - Example #0
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'these', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 'ot@@', 'ta', 'ar@@', 'tic@@', 'al', 'cal@@', 'o@@', 'te', 'gl@@', 'aci@@', 'al', 'dimension@@', 's', 'of', '4@@', '8', 'U.@@', 'S.', 'has', 'been', 'stre@@', 't@@', 's', 'in', 'the', '4@@', '0@@', '8', 'U.@@', 'S.', '</s>']
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Hypothesis: I showed these these slide to show that the slide to show that the calototta artical calote glacial dimensions of 48 U.S. has been strets in the 408 U.S.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - Example #1
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'gro@@', 'und', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 't', 'the', 'gl@@', 'aci@@', 'er.', '</s>']
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - 	Hypothesis: But this underground is the gravity of the problem because it doesn't ext the glacier.
2024-05-27 22:42:32,336 - INFO - joeynmt.training - Example #2
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:42:32,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 't@@', 'ac@@', 't@@', 'ic', 'is', '--', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Hypothesis: The glacial caltactic is -- in a sense, the heart heart of the climate system.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - Example #3
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 's@@', 'kin@@', ',', 'and', 'you', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'ex@@', 'ten@@', 'ded', 'up.', '</s>']
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Hypothesis: You go and skin, and you return into the summer of extended up.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - Example #4
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:42:32,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'for', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:42:32,337 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick for the last 25 years.
2024-05-27 22:42:48,471 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.905723, Batch Acc: 0.505518, Tokens per Sec:     4196, Lr: 0.000300
2024-05-27 22:43:04,506 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.628764, Batch Acc: 0.509546, Tokens per Sec:     4371, Lr: 0.000300
2024-05-27 22:43:20,238 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.783163, Batch Acc: 0.511186, Tokens per Sec:     4407, Lr: 0.000300
2024-05-27 22:43:35,786 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.930139, Batch Acc: 0.509775, Tokens per Sec:     4494, Lr: 0.000300
2024-05-27 22:43:51,466 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.635411, Batch Acc: 0.510085, Tokens per Sec:     4430, Lr: 0.000300
2024-05-27 22:43:51,466 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:43:51,466 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:44:58,269 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.91, acc:   0.48, generation: 66.7962[sec], evaluation: 0.0000[sec]
2024-05-27 22:44:58,272 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:44:58,436 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/11000.ckpt
2024-05-27 22:44:58,437 - INFO - joeynmt.training - Example #0
2024-05-27 22:44:58,437 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:44:58,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:44:58,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ot@@', 'ter', 'gl@@', 'aci@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'you', 'know,', '4@@', '0@@', '-@@', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'you', 'know,', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:44:58,437 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:44:58,437 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to demonstrate that the glacial calototter glacial artica, which for almost three million years had the size of 48 U.S. continents, you know, 40-U.S. continents, you know, 40-percent.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - Example #1
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'hi@@', 'bit', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Hypothesis: However, this subvalue of the problem because it doesn't show the exhibit of the ice of the ice problem.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - Example #2
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ot@@', 'ter', 'is', '--', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - 	Hypothesis: The glacial calototter is -- in a certain sense, the heart heart of global climate system.
2024-05-27 22:44:58,438 - INFO - joeynmt.training - Example #3
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:44:58,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Hypothesis: You expanding and you get into the summer and you get into the summer of extinct.
2024-05-27 22:44:58,439 - INFO - joeynmt.training - Example #4
2024-05-27 22:44:58,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:44:58,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:44:58,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'rapi@@', 'd', 'car@@', 'ry', 'to', 'be', 'a', 'car@@', 'ry', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:44:58,439 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a rapid carry to be a carry of the last 25 years.
2024-05-27 22:45:15,264 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.782942, Batch Acc: 0.506913, Tokens per Sec:     4091, Lr: 0.000300
2024-05-27 22:45:32,303 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.861165, Batch Acc: 0.510423, Tokens per Sec:     4195, Lr: 0.000300
2024-05-27 22:45:48,594 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     2.086214, Batch Acc: 0.505461, Tokens per Sec:     4132, Lr: 0.000300
2024-05-27 22:46:04,784 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.709195, Batch Acc: 0.508685, Tokens per Sec:     4388, Lr: 0.000300
2024-05-27 22:46:21,318 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.717763, Batch Acc: 0.511182, Tokens per Sec:     4303, Lr: 0.000300
2024-05-27 22:46:21,319 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:46:21,319 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:47:27,140 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.80, acc:   0.48, generation: 65.8143[sec], evaluation: 0.0000[sec]
2024-05-27 22:47:27,142 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:47:27,308 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/11500.ckpt
2024-05-27 22:47:27,309 - INFO - joeynmt.training - Example #0
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'th', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ate', 'c@@', 'alc@@', 'ul@@', 'ator@@', 'y,', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'size', 'of', '4@@', '0@@', '8', 'U@@', 'S', 'contin@@', 'ent@@', 'al', 'in', 'the', 'size', 'of', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to demonth to show that the calculate calculatory, which for almost three million years had the size of 408 US continental in the size of 40-percent.
2024-05-27 22:47:27,310 - INFO - joeynmt.training - Example #1
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'h@@', 'ate', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'ex@@', 'h@@', 'old', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'not', 'the', 'sp@@', 'ess@@', 'enti@@', 'al', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'not', 'the', 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'ex@@', 'h@@', 'it', 'the', 'grav@@', 'ity', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'not', 'the', 'ex@@', 'hi@@', 'p@@', 'ess@@', 'enti@@', 'al', 'of', 'the', 'problem', 'because', 'it']
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:47:27,310 - INFO - joeynmt.training - 	Hypothesis: However, this subvalue of the problem because it doesn't exhate the ice of the ice problem because it doesn't exhold the ice of the ice problem because it is not the spessential of the ice problem because it is not the exhibition of the ice problem because it doesn't exhit the gravity of the ice problem because it is not the exhipessential of the problem because it
2024-05-27 22:47:27,310 - INFO - joeynmt.training - Example #2
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:47:27,310 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ator@@', 'y', 'is', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', '.', '</s>']
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Hypothesis: The glacial calculatory is in a certain sense, the heart custom.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - Example #3
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'to', 'in@@', 'ver@@', 'se', 'and', 'you', 'get', 'it', 'and', 'you', 'get', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Hypothesis: You get to inverse and you get it and you get it into the summer.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - Example #4
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:47:27,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'gre@@', 'e', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:47:27,311 - INFO - joeynmt.training - 	Hypothesis: The next degree is going to be a quick in the last 25 years.
2024-05-27 22:47:43,638 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.664414, Batch Acc: 0.506338, Tokens per Sec:     4171, Lr: 0.000300
2024-05-27 22:48:00,532 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.751464, Batch Acc: 0.514492, Tokens per Sec:     4164, Lr: 0.000300
2024-05-27 22:48:17,195 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.878907, Batch Acc: 0.513612, Tokens per Sec:     4266, Lr: 0.000300
2024-05-27 22:48:33,383 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.825329, Batch Acc: 0.515998, Tokens per Sec:     4322, Lr: 0.000300
2024-05-27 22:48:50,264 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.791291, Batch Acc: 0.510830, Tokens per Sec:     4048, Lr: 0.000300
2024-05-27 22:48:50,265 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:48:50,265 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:49:58,950 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.76, acc:   0.48, generation: 68.6789[sec], evaluation: 0.0000[sec]
2024-05-27 22:49:58,952 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:49:59,117 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/12000.ckpt
2024-05-27 22:49:59,118 - INFO - joeynmt.training - Example #0
2024-05-27 22:49:59,118 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:49:59,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:49:59,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'n@@', 'ice', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'ar@@', 'tic@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'U.@@', 'S.', '</s>']
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to show that the nice glacial calculation artic, which for almost three million years of the size of 48 United States of the U.S.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - Example #1
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'gl@@', 'aci@@', 'er.', '</s>']
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of gravity of the problem because it doesn't show the pace of the ice of the ice problem of the glacier.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - Example #2
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:49:59,119 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'is', 'a', 'very', 'n@@', 'ice', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 's', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - 	Hypothesis: The article is a very nice is, in a sense, the heart customs of global climate system.
2024-05-27 22:49:59,119 - INFO - joeynmt.training - Example #3
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'st@@', 'and', 'of', 'in@@', 'ver@@', 'su@@', 's', 'and', 'you', 're@@', 'turn', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Hypothesis: You go and stand of inversus and you return into the summer.
2024-05-27 22:49:59,120 - INFO - joeynmt.training - Example #4
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:49:59,120 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', '25', 'years.', '</s>']
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:49:59,120 - INFO - joeynmt.training - 	Hypothesis: The next 25 years.
2024-05-27 22:50:17,003 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.787887, Batch Acc: 0.512582, Tokens per Sec:     3875, Lr: 0.000300
2024-05-27 22:50:34,232 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.786103, Batch Acc: 0.510956, Tokens per Sec:     4045, Lr: 0.000300
2024-05-27 22:50:50,558 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.826783, Batch Acc: 0.502976, Tokens per Sec:     4179, Lr: 0.000300
2024-05-27 22:51:07,841 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.815167, Batch Acc: 0.512754, Tokens per Sec:     3976, Lr: 0.000300
2024-05-27 22:51:25,594 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.720221, Batch Acc: 0.514605, Tokens per Sec:     3872, Lr: 0.000300
2024-05-27 22:51:25,595 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:51:25,595 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:52:22,450 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.66, acc:   0.49, generation: 56.8480[sec], evaluation: 0.0000[sec]
2024-05-27 22:52:22,452 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:52:22,623 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/12500.ckpt
2024-05-27 22:52:22,624 - INFO - joeynmt.training - Example #0
2024-05-27 22:52:22,624 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:52:22,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:52:22,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'of', 'the', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ator@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'ar@@', 'tic@@', 'al,', 'which', 'is', 're@@', 'ver@@', 'se', 'of', '4@@', '0@@', '8', 'United', 'St@@', 'ates', 'of', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 22:52:22,624 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:52:22,624 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Hypothesis: I showed these slide of the slide to show that the slide to show that the calculator, which for almost three million years of the artical, which is reverse of 408 United States of 40-percent.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - Example #1
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'ice', 'problem', '</s>']
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Hypothesis: But this subvalue is the gravity of the problem because it doesn't show the ice of the ice problem of the ice problem
2024-05-27 22:52:22,625 - INFO - joeynmt.training - Example #2
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'able', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', '--', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Hypothesis: The artable calculation is -- in a way, the heart of the global climate system.
2024-05-27 22:52:22,625 - INFO - joeynmt.training - Example #3
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:52:22,625 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', '.', '</s>']
2024-05-27 22:52:22,625 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:52:22,626 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:52:22,626 - INFO - joeynmt.training - 	Hypothesis: You expanding and you get into the summer and you get into the summer of the summ.
2024-05-27 22:52:22,626 - INFO - joeynmt.training - Example #4
2024-05-27 22:52:22,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:52:22,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:52:22,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'few', 'sli@@', 'des', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'so@@', 'un@@', 'ds', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:52:22,626 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:52:22,626 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:52:22,626 - INFO - joeynmt.training - 	Hypothesis: The next next few slides will be a quick carrelled sounds of the last 25 years.
2024-05-27 22:52:39,904 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.694268, Batch Acc: 0.514932, Tokens per Sec:     3901, Lr: 0.000300
2024-05-27 22:52:56,767 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.667325, Batch Acc: 0.512309, Tokens per Sec:     4052, Lr: 0.000300
2024-05-27 22:53:14,138 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.753237, Batch Acc: 0.516159, Tokens per Sec:     4040, Lr: 0.000300
2024-05-27 22:53:30,712 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     2.025001, Batch Acc: 0.514792, Tokens per Sec:     4181, Lr: 0.000300
2024-05-27 22:53:46,932 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.649339, Batch Acc: 0.515546, Tokens per Sec:     4448, Lr: 0.000300
2024-05-27 22:53:46,932 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:53:46,933 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:54:51,328 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.59, acc:   0.49, generation: 64.3886[sec], evaluation: 0.0000[sec]
2024-05-27 22:54:51,329 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:54:51,499 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/13000.ckpt
2024-05-27 22:54:51,501 - INFO - joeynmt.training - Example #0
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'just', 'just', 'show', 'these', 'sli@@', 'des', 'of', 'the', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', '4@@', '0@@', '8', 'percent@@', '.', '</s>']
2024-05-27 22:54:51,501 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:54:51,501 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:54:51,501 - INFO - joeynmt.training - 	Hypothesis: I just just show these slides of the slide to show that the calculation of artica, which for almost three million years has had the size of 48 United States of the 48 United States of the 408 percent.
2024-05-27 22:54:51,501 - INFO - joeynmt.training - Example #1
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:54:51,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ice.', '</s>']
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of gravity of the problem because it doesn't show the exhibition of the ice.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - Example #2
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'is', 'ar@@', 'tic@@', 'le', '--', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Hypothesis: The article is article -- in a certain sense, the clear heart of the climate system of global climate system.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - Example #3
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'a', 'w@@', 'all@@', 's', 'and', 'you', 're@@', 'ach', 'it', 'into', 'ext@@', 'rem@@', 'ely', 'it.', '</s>']
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - 	Hypothesis: You get a walls and you reach it into extremely it.
2024-05-27 22:54:51,502 - INFO - joeynmt.training - Example #4
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:54:51,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'the', 's@@', 'ou@@', 'ther@@', 'n', '25', 'years.', '</s>']
2024-05-27 22:54:51,503 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:54:51,503 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:54:51,503 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrelled the southern 25 years.
2024-05-27 22:55:07,866 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.927060, Batch Acc: 0.516277, Tokens per Sec:     4118, Lr: 0.000300
2024-05-27 22:55:24,779 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.701993, Batch Acc: 0.511560, Tokens per Sec:     3985, Lr: 0.000300
2024-05-27 22:55:42,991 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.830110, Batch Acc: 0.517758, Tokens per Sec:     3794, Lr: 0.000300
2024-05-27 22:55:46,479 - INFO - joeynmt.training - Epoch   4: total training loss 7018.02
2024-05-27 22:55:46,479 - INFO - joeynmt.training - EPOCH 5
2024-05-27 22:56:02,468 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     1.627530, Batch Acc: 0.540833, Tokens per Sec:     3465, Lr: 0.000300
2024-05-27 22:56:19,222 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     1.594389, Batch Acc: 0.542092, Tokens per Sec:     4167, Lr: 0.000300
2024-05-27 22:56:19,222 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:56:19,222 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 22:57:26,298 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.57, acc:   0.49, generation: 67.0665[sec], evaluation: 0.0000[sec]
2024-05-27 22:57:26,299 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 22:57:26,474 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/13500.ckpt
2024-05-27 22:57:26,476 - INFO - joeynmt.training - Example #0
2024-05-27 22:57:26,476 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 22:57:26,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 22:57:26,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'of', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'ot@@', 'ta', 'ar@@', 'tic@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'is', '4@@', '0@@', '-@@', 'U.@@', 'S.', '</s>']
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Hypothesis: I showed these slide slide to show that the slide of the artical calototta artic, which is about three million years of the size of the 48 United States continents, is 40-U.S.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - Example #1
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'ice', 'problem', 'because', 'it', 'is', 'the', 'ice.', '</s>']
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice problem of the ice problem because it is the ice.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - Example #2
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'cal@@', 't', 'is', 'a', 'very', 'much', 'of', 'the', 'lim@@', 'est@@', ',', 'the', 'cle@@', 'an@@', 'er', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - 	Hypothesis: The artical calt is a very much of the limest, the cleaner of the climate system of global climate system.
2024-05-27 22:57:26,477 - INFO - joeynmt.training - Example #3
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 22:57:26,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'in@@', 'ver@@', 'se', 'and', 'you', 'get', 'into', 'ext@@', 'inc@@', 't', 'and', 'ext@@', 'inc@@', 't', 'it.', '</s>']
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Hypothesis: You get inverse and you get into extinct and extinct it.
2024-05-27 22:57:26,478 - INFO - joeynmt.training - Example #4
2024-05-27 22:57:26,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 22:57:26,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 22:57:26,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'few', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'rapi@@', 'd', 'car@@', 're@@', 'll@@', 'ed', 'the', 's@@', 'av@@', 'ed', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 22:57:26,478 - INFO - joeynmt.training - 	Hypothesis: The next few devices will be a rapid carrelled the saved of the last 25 years.
2024-05-27 22:57:43,561 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     1.705721, Batch Acc: 0.538255, Tokens per Sec:     3964, Lr: 0.000300
2024-05-27 22:58:00,924 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     1.857173, Batch Acc: 0.539700, Tokens per Sec:     4017, Lr: 0.000300
2024-05-27 22:58:17,995 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     1.960350, Batch Acc: 0.534684, Tokens per Sec:     4140, Lr: 0.000300
2024-05-27 22:58:35,640 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     1.627684, Batch Acc: 0.535460, Tokens per Sec:     3939, Lr: 0.000300
2024-05-27 22:58:52,021 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     1.663455, Batch Acc: 0.538408, Tokens per Sec:     4125, Lr: 0.000300
2024-05-27 22:58:52,021 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 22:58:52,021 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:00:04,636 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.56, acc:   0.49, generation: 72.6076[sec], evaluation: 0.0000[sec]
2024-05-27 23:00:04,638 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:00:04,802 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/14000.ckpt
2024-05-27 23:00:04,803 - INFO - joeynmt.training - Example #0
2024-05-27 23:00:04,803 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:00:04,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:00:04,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'of', 'the', 'gl@@', 'aci@@', 'al', 'he@@', 'at', 'at', 'that', 'the', 'gl@@', 'aci@@', 'al', 'sc@@', 'ale', 'of', 'the', '4@@', '8', 'U.@@', 'S.', 'has', 'been', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'you', 'have', 'a', '4@@', '0@@', '8', 'U.@@', 'S.', '</s>']
2024-05-27 23:00:04,803 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:00:04,803 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:00:04,803 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to show that the slide of the glacial heat at that the glacial scale of the 48 U.S. has been the size of 48 United States, you have a 408 U.S.
2024-05-27 23:00:04,803 - INFO - joeynmt.training - Example #1
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'p@@', 'us@@', 'h@@', 'ed', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', '--', '</s>']
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Hypothesis: But this underpushed the gravity of the problem because it doesn't show the ice of the ice problem --
2024-05-27 23:00:04,804 - INFO - joeynmt.training - Example #2
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'tic', 'he@@', 'at', 'is', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Hypothesis: The glacial calotic heat is a certain sense, the heart heart heart of the global climate system.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - Example #3
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:00:04,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'the', 'sum@@', 'p@@', 'tion', 'and', 'the', 'sum@@', 'p@@', 'tion', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'the', 'sum@@', 'p@@', 'tion', 'of', 'the', 'sum@@', 'p@@', 'tion', 'of', 'the', 'sum@@', 'p@@', 'tion', 'and', 'it', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'p@@', 'tion', 'and', 'the', 'sum@@', 'p@@', 'tion', 'of', 'the', 'sum@@', 'p@@', 'tion', 'and', 'the', 'sum@@', 'p@@', 'tion', 'of', 'the', 'sum@@', 'p@@', 'tion', 'and', 'the', 'sum@@', 'm@@', 'it.', '</s>']
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - 	Hypothesis: You expanding and the sumption and the sumption of the summer and the sumption of the sumption of the sumption and it return it into the sumption and the sumption of the sumption and the sumption of the sumption and the summit.
2024-05-27 23:00:04,804 - INFO - joeynmt.training - Example #4
2024-05-27 23:00:04,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:00:04,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:00:04,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'su@@', 'd@@', 'den@@', 's', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:00:04,805 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:00:04,805 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:00:04,805 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick carrelled suddens of the last 25 years.
2024-05-27 23:00:21,686 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     1.815195, Batch Acc: 0.539050, Tokens per Sec:     4070, Lr: 0.000300
2024-05-27 23:00:38,333 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     1.974464, Batch Acc: 0.532866, Tokens per Sec:     4173, Lr: 0.000300
2024-05-27 23:00:54,799 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     1.705238, Batch Acc: 0.531511, Tokens per Sec:     4127, Lr: 0.000300
2024-05-27 23:01:11,682 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     1.541915, Batch Acc: 0.533172, Tokens per Sec:     4158, Lr: 0.000300
2024-05-27 23:01:28,547 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     1.595131, Batch Acc: 0.533510, Tokens per Sec:     4085, Lr: 0.000300
2024-05-27 23:01:28,549 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:01:28,549 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:02:39,845 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.45, acc:   0.49, generation: 71.2899[sec], evaluation: 0.0000[sec]
2024-05-27 23:02:39,847 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:02:40,015 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/14500.ckpt
2024-05-27 23:02:40,016 - INFO - joeynmt.training - Example #0
2024-05-27 23:02:40,016 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:02:40,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:02:40,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'is', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'and', 'a', '4@@', '0@@', '-@@', 'year@@', '-@@', 'old', 'lev@@', 'el@@', '.', '</s>']
2024-05-27 23:02:40,016 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to demonstrate that the glacial calote, which is almost three million years of the size of the 48 United States of the size of 48 United States continental and a 40-year-old level.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - Example #1
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'p@@', 'us@@', 'es', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice.', '</s>']
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Hypothesis: But this underpuses the gravity of the problem because it doesn't show the ice of the ice because it doesn't explain the ice.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - Example #2
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'cal@@', 'ot@@', 't@@', 'ic@@', 's,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 'e', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - 	Hypothesis: The artic calottics, in a sense, the clear heart custome of the global climate system.
2024-05-27 23:02:40,017 - INFO - joeynmt.training - Example #3
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:02:40,017 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'it', 're@@', 'ver@@', 'se', 'and', 'you', 're@@', 'turn', 'it', 'into', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Hypothesis: It expand it reverse and you return it into extinct.
2024-05-27 23:02:40,018 - INFO - joeynmt.training - Example #4
2024-05-27 23:02:40,018 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:02:40,018 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:02:40,018 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'e,', 'it', 'will', 'be', 'a', 'rapi@@', 'd', 'car@@', 'r@@', 'ying', 'the', 's@@', 'av@@', 'im@@', 'ent', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:02:40,018 - INFO - joeynmt.training - 	Hypothesis: The next device, it will be a rapid carrying the saviment of the last 25 years.
2024-05-27 23:02:56,868 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     1.876824, Batch Acc: 0.531780, Tokens per Sec:     4091, Lr: 0.000300
2024-05-27 23:03:13,483 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.687076, Batch Acc: 0.531921, Tokens per Sec:     4171, Lr: 0.000300
2024-05-27 23:03:29,530 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.647290, Batch Acc: 0.533868, Tokens per Sec:     4270, Lr: 0.000300
2024-05-27 23:03:46,372 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.673542, Batch Acc: 0.533582, Tokens per Sec:     4105, Lr: 0.000300
2024-05-27 23:04:03,240 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.684396, Batch Acc: 0.537696, Tokens per Sec:     4230, Lr: 0.000300
2024-05-27 23:04:03,241 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:04:03,242 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:04:57,751 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.49, generation: 54.5026[sec], evaluation: 0.0000[sec]
2024-05-27 23:04:57,920 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/15000.ckpt
2024-05-27 23:04:57,921 - INFO - joeynmt.training - Example #0
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'to', 'show', 'that', 'the', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'to', 'that', 'the', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'c@@', 'our@@', '.', '</s>']
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to demonstrate that the glacial caloto show that the glacial caloto that the 48 million years has had the size of 48 U.S. continental cour.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - Example #1
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of gravity of the problem because it doesn't show the ice of the ice problem of the ice.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - Example #2
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:04:57,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'cal@@', 'o@@', 'tic', 'cal@@', 'o@@', 'es', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:04:57,922 - INFO - joeynmt.training - 	Hypothesis: The glacial calotic caloes is, in a sense, the heart customer of the global climate system.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - Example #3
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'in@@', 'ver@@', 'sion', 'and', 'you', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'it.', '</s>']
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Hypothesis: It expands of inversion and you return it into the summer of the summer of the summer and return it into the summer of the summer and return it into the summer of the summer and return it into the summer of the summit.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - Example #4
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:04:57,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'quick@@', 'ly', 'quick@@', 'ly', 'f@@', 'ast', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:04:57,923 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly quickly fast of the last 25 years.
2024-05-27 23:05:14,903 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.607020, Batch Acc: 0.536117, Tokens per Sec:     4130, Lr: 0.000300
2024-05-27 23:05:31,624 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.577565, Batch Acc: 0.535151, Tokens per Sec:     4076, Lr: 0.000300
2024-05-27 23:05:48,901 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.677911, Batch Acc: 0.535982, Tokens per Sec:     4125, Lr: 0.000300
2024-05-27 23:06:06,199 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.565742, Batch Acc: 0.526234, Tokens per Sec:     4010, Lr: 0.000300
2024-05-27 23:06:23,186 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.701177, Batch Acc: 0.530800, Tokens per Sec:     4039, Lr: 0.000300
2024-05-27 23:06:23,187 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:06:23,187 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:07:30,434 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.50, generation: 67.2393[sec], evaluation: 0.0000[sec]
2024-05-27 23:07:30,436 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:07:30,602 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/15500.ckpt
2024-05-27 23:07:30,603 - INFO - joeynmt.training - Example #0
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'went', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', '4@@', '8', 'million', 'years', 'has', 'been', 'stre@@', 't@@', 'ch', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Hypothesis: And I went last year I showed these slides to show that the glacial calote, which for almost three million years has been the size of the 48 million years has been stretch of the United States of the 40-percent.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - Example #1
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:07:30,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the exhibition of the ice problem because it doesn't show the ice of the ice.
2024-05-27 23:07:30,603 - INFO - joeynmt.training - Example #2
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'cal@@', 'ot@@', 'ter', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Hypothesis: The artic calotter is in a sense, the heart customer of the global climate system.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - Example #3
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'es', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'ext@@', 'inc@@', 't.', '</s>']
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Hypothesis: It expandes and return it into the summer of the summer of the summer of the summer and return it into the extinct.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - Example #4
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:07:30,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'positi@@', 've', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'd,', 'the', 's@@', 'ent', 'f@@', 'ast', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:07:30,604 - INFO - joeynmt.training - 	Hypothesis: The next deapositive will be a quick carred, the sent fast of the last 25 years.
2024-05-27 23:07:47,868 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.699588, Batch Acc: 0.535449, Tokens per Sec:     3977, Lr: 0.000300
2024-05-27 23:08:04,461 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.522884, Batch Acc: 0.535325, Tokens per Sec:     4153, Lr: 0.000300
2024-05-27 23:08:21,240 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.728991, Batch Acc: 0.534756, Tokens per Sec:     4190, Lr: 0.000300
2024-05-27 23:08:38,141 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.770289, Batch Acc: 0.530573, Tokens per Sec:     4060, Lr: 0.000300
2024-05-27 23:08:55,646 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.865720, Batch Acc: 0.523785, Tokens per Sec:     3893, Lr: 0.000300
2024-05-27 23:08:55,647 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:08:55,647 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:10:03,228 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.33, acc:   0.50, generation: 67.5746[sec], evaluation: 0.0000[sec]
2024-05-27 23:10:03,229 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:10:03,397 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/16000.ckpt
2024-05-27 23:10:03,398 - INFO - joeynmt.training - Example #0
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'n@@', 'ice', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', "it's", 'stre@@', 't@@', 's', 'of', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 23:10:03,398 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:10:03,398 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:10:03,398 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to demonstrate that the nice glacial calotter glacial calote, which for almost three million years has had the size of the 48 United States continents, it's strets of 40-percent.
2024-05-27 23:10:03,398 - INFO - joeynmt.training - Example #1
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:10:03,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'pla@@', 'in', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'act', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of']
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Hypothesis: But this undervalue is the gravity of the problem because it doesn't show the explain the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the problem because it doesn't show the exact of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the problem because it doesn't show the ice of the ice of
2024-05-27 23:10:03,399 - INFO - joeynmt.training - Example #2
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'way,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calculation is, in a way, the clear heart heart of the global climate system.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - Example #3
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'in@@', 'ver@@', 'se', 'and', 'you', 'get', 'into', 'the', 'ext@@', 'ent', 'and', 'you', 'get', 'into', 'ext@@', 'inc@@', 't', 'it.', '</s>']
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - 	Hypothesis: You get inverse and you get into the extent and you get into extinct it.
2024-05-27 23:10:03,399 - INFO - joeynmt.training - Example #4
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:10:03,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'e,', "it's", 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:10:03,400 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:10:03,400 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:10:03,400 - INFO - joeynmt.training - 	Hypothesis: The next device, it's going to be a quick carrelled the last 25 years.
2024-05-27 23:10:20,077 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.681624, Batch Acc: 0.530780, Tokens per Sec:     4157, Lr: 0.000300
2024-05-27 23:10:37,468 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.764803, Batch Acc: 0.532244, Tokens per Sec:     4059, Lr: 0.000300
2024-05-27 23:10:54,924 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.635482, Batch Acc: 0.532238, Tokens per Sec:     3981, Lr: 0.000300
2024-05-27 23:11:13,437 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.594413, Batch Acc: 0.534219, Tokens per Sec:     3729, Lr: 0.000300
2024-05-27 23:11:30,381 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.692501, Batch Acc: 0.529731, Tokens per Sec:     4063, Lr: 0.000300
2024-05-27 23:11:30,382 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:11:30,382 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:12:27,810 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.26, acc:   0.50, generation: 57.4216[sec], evaluation: 0.0000[sec]
2024-05-27 23:12:27,812 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:12:27,981 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/16500.ckpt
2024-05-27 23:12:27,982 - INFO - joeynmt.training - Example #0
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'is', 'ne@@', 'ar@@', 'ly', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'you', 'have', 'been', 'ro@@', 't@@', 'ated', 'by', '4@@', '0@@', 's.', '</s>']
2024-05-27 23:12:27,982 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:12:27,982 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:12:27,982 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to demonstrate that the calculation that is nearly three million years has had the size of 48 years has had the size of 48 United States, you have been rotated by 40s.
2024-05-27 23:12:27,982 - INFO - joeynmt.training - Example #1
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:12:27,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Hypothesis: And this is the gravity of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the ice.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - Example #2
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'is', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calotter is in a certain sense, the clear heart of global climate system.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - Example #3
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's,', 'and', 'you', 'get', 're@@', 'pe@@', 't', 'and', 'you', 'get', 're@@', 'pe@@', 'at@@', 'ed.', '</s>']
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - 	Hypothesis: You expands, and you get repet and you get repeated.
2024-05-27 23:12:27,983 - INFO - joeynmt.training - Example #4
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:12:27,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:12:27,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:12:27,984 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:12:27,984 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:12:27,984 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrelled on the last 25 years.
2024-05-27 23:12:44,469 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.604474, Batch Acc: 0.530684, Tokens per Sec:     4253, Lr: 0.000300
2024-05-27 23:13:00,495 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.667097, Batch Acc: 0.532555, Tokens per Sec:     4307, Lr: 0.000300
2024-05-27 23:13:17,504 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.795111, Batch Acc: 0.534124, Tokens per Sec:     4071, Lr: 0.000300
2024-05-27 23:13:34,639 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.565519, Batch Acc: 0.528117, Tokens per Sec:     4057, Lr: 0.000300
2024-05-27 23:13:51,700 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.550589, Batch Acc: 0.533295, Tokens per Sec:     4131, Lr: 0.000300
2024-05-27 23:13:51,701 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:13:51,701 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:14:59,839 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.50, generation: 68.1309[sec], evaluation: 0.0000[sec]
2024-05-27 23:14:59,839 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:15:00,011 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/17500.ckpt
2024-05-27 23:15:00,012 - INFO - joeynmt.training - Example #0
2024-05-27 23:15:00,012 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:15:00,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:15:00,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'in', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', "it's", 'been', 'stre@@', 't@@', 's', 'of', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:15:00,012 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:15:00,012 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:15:00,012 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides to show that the slide to demonstrate that the calculation is about three million years has had the size of the 48 United States in the continental continents, it's been strets of 40 percent.
2024-05-27 23:15:00,012 - INFO - joeynmt.training - Example #1
2024-05-27 23:15:00,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'is', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Hypothesis: However, this is the gravity of the problem because it doesn't show the ice of the ice of the ice.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - Example #2
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'alc@@', 'ul@@', 'ation', 'is', '--', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calcalculation is -- in a sense, the clear of the global climate system.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - Example #3
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', "e's", 'ex@@', 'p@@', 'and@@', 'ing', 'and', 're@@', 'tur@@', 'n.', '</s>']
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - 	Hypothesis: He's expanding and return.
2024-05-27 23:15:00,013 - INFO - joeynmt.training - Example #4
2024-05-27 23:15:00,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:15:00,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:15:00,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'car@@', 'd@@', 'ly', 'in', 'the', 'last', '25', 'years', 'ol@@', 'd.', '</s>']
2024-05-27 23:15:00,014 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:15:00,014 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:15:00,014 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid cardly in the last 25 years old.
2024-05-27 23:15:17,144 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.966696, Batch Acc: 0.534832, Tokens per Sec:     4071, Lr: 0.000300
2024-05-27 23:15:34,247 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.687362, Batch Acc: 0.533061, Tokens per Sec:     4047, Lr: 0.000300
2024-05-27 23:15:47,221 - INFO - joeynmt.training - Epoch   5: total training loss 6635.96
2024-05-27 23:15:47,222 - INFO - joeynmt.training - EPOCH 6
2024-05-27 23:15:52,024 - INFO - joeynmt.training - Epoch   6, Step:    19800, Batch Loss:     1.817435, Batch Acc: 0.557008, Tokens per Sec:     3728, Lr: 0.000300
2024-05-27 23:16:08,876 - INFO - joeynmt.training - Epoch   6, Step:    19900, Batch Loss:     1.566483, Batch Acc: 0.557552, Tokens per Sec:     4121, Lr: 0.000300
2024-05-27 23:16:25,775 - INFO - joeynmt.training - Epoch   6, Step:    20000, Batch Loss:     1.422577, Batch Acc: 0.562435, Tokens per Sec:     4172, Lr: 0.000300
2024-05-27 23:16:25,776 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:16:25,776 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:17:29,923 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.50, generation: 64.1398[sec], evaluation: 0.0000[sec]
2024-05-27 23:17:30,089 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/17000.ckpt
2024-05-27 23:17:30,090 - INFO - joeynmt.training - Example #0
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ator@@', 'y', 'c@@', 'alc@@', 'ul@@', 'ator@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'had', 'the', 'size', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'percent', 'of', 'the', 'contin@@', 'ent@@', 'al', 'size', '--', '</s>']
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to show that the calculatory calculator, which for almost three million years has had had the size of 48 million years has had the size of 48 percent of the continental size --
2024-05-27 23:17:30,091 - INFO - joeynmt.training - Example #1
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'p@@', 'ace', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:17:30,091 - INFO - joeynmt.training - 	Hypothesis: But this underpace the gravity of the problem because it doesn't show the ice of the ice.
2024-05-27 23:17:30,091 - INFO - joeynmt.training - Example #2
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:17:30,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'to@@', 'o,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calottoo, in a sense, the clean heart of the global climate system.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - Example #3
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'in@@', 'ver@@', 'se', 'and', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Hypothesis: You get the inverse and return into the summer of the summer.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - Example #4
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:17:30,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'a@@', 'ther', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd@@', 'ly', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:17:30,092 - INFO - joeynmt.training - 	Hypothesis: The next deather is going to be a quick cardly in the last 25 years.
2024-05-27 23:17:46,866 - INFO - joeynmt.training - Epoch   6, Step:    20100, Batch Loss:     1.564562, Batch Acc: 0.559936, Tokens per Sec:     4196, Lr: 0.000300
2024-05-27 23:18:03,715 - INFO - joeynmt.training - Epoch   6, Step:    20200, Batch Loss:     1.633222, Batch Acc: 0.555827, Tokens per Sec:     4157, Lr: 0.000300
2024-05-27 23:18:20,691 - INFO - joeynmt.training - Epoch   6, Step:    20300, Batch Loss:     1.656462, Batch Acc: 0.561178, Tokens per Sec:     4079, Lr: 0.000300
2024-05-27 23:18:38,277 - INFO - joeynmt.training - Epoch   6, Step:    20400, Batch Loss:     1.540835, Batch Acc: 0.553778, Tokens per Sec:     3840, Lr: 0.000300
2024-05-27 23:18:54,895 - INFO - joeynmt.training - Epoch   6, Step:    20500, Batch Loss:     1.624729, Batch Acc: 0.548806, Tokens per Sec:     4093, Lr: 0.000300
2024-05-27 23:18:54,895 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:18:54,895 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:20:05,688 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.20, acc:   0.50, generation: 70.7861[sec], evaluation: 0.0000[sec]
2024-05-27 23:20:05,864 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/18000.ckpt
2024-05-27 23:20:05,866 - INFO - joeynmt.training - Example #0
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'n@@', 'a@@', 'ked', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 't@@', 'ac@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'Ear@@', 'th@@', "'s", 'contin@@', 'ent@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation@@', 's.', '</s>']
2024-05-27 23:20:05,866 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:20:05,866 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:20:05,866 - INFO - joeynmt.training - 	Hypothesis: I showed these slide demonstrate these slides to show that the naked glacial calottace, which for almost three million years has had the size of 48 U.S. continental Earth's continental calculations.
2024-05-27 23:20:05,866 - INFO - joeynmt.training - Example #1
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:20:05,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'p@@', 'us@@', 'h', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice.', '</s>']
2024-05-27 23:20:05,866 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Hypothesis: However, this underpush of the problem because it doesn't explain the ice of the ice problem because it doesn't explain the ice.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - Example #2
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ot@@', 'ta', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calototta is in a sense, the clear heart of the global climate system.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - Example #3
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'in@@', 'ver@@', 'su@@', 's', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'sum@@', 'm@@', 'er', '</s>']
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:20:05,867 - INFO - joeynmt.training - 	Hypothesis: You get inversus and you get into the summer of summer
2024-05-27 23:20:05,867 - INFO - joeynmt.training - Example #4
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:20:05,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:20:05,868 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:20:05,868 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:20:05,868 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick carrelled the last 25 years.
2024-05-27 23:20:23,388 - INFO - joeynmt.training - Epoch   6, Step:    20600, Batch Loss:     1.568888, Batch Acc: 0.548630, Tokens per Sec:     3933, Lr: 0.000300
2024-05-27 23:20:40,557 - INFO - joeynmt.training - Epoch   6, Step:    20700, Batch Loss:     1.648291, Batch Acc: 0.550007, Tokens per Sec:     3994, Lr: 0.000300
2024-05-27 23:20:57,322 - INFO - joeynmt.training - Epoch   6, Step:    20800, Batch Loss:     1.595981, Batch Acc: 0.549838, Tokens per Sec:     4165, Lr: 0.000300
2024-05-27 23:21:13,999 - INFO - joeynmt.training - Epoch   6, Step:    20900, Batch Loss:     1.576965, Batch Acc: 0.550234, Tokens per Sec:     4096, Lr: 0.000300
2024-05-27 23:21:31,231 - INFO - joeynmt.training - Epoch   6, Step:    21000, Batch Loss:     1.507661, Batch Acc: 0.553542, Tokens per Sec:     4021, Lr: 0.000300
2024-05-27 23:21:31,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:21:31,233 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:22:41,902 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.16, acc:   0.50, generation: 70.6626[sec], evaluation: 0.0000[sec]
2024-05-27 23:22:42,072 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/18500.ckpt
2024-05-27 23:22:42,073 - INFO - joeynmt.training - Example #0
2024-05-27 23:22:42,073 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:22:42,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:22:42,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'of', 'the', 'ar@@', 'tic@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'and', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:22:42,073 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:22:42,073 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:22:42,073 - INFO - joeynmt.training - 	Hypothesis: And last year I showed these slides to show that the slide of the artic, which for almost three million years has had the size of 48 million years has had the size of 48 U.S. continental continental and 40 percent.
2024-05-27 23:22:42,073 - INFO - joeynmt.training - Example #1
2024-05-27 23:22:42,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pres@@', 's', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the problem because it doesn't express the ice of the ice.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - Example #2
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'gl@@', 'aci@@', 'al', 'he@@', 'at', 'is', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 's', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Hypothesis: The article glacial heat is is in a sense, the clear heart customs of global climate system.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - Example #3
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:22:42,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - 	Hypothesis: You expand you get into the summer and return into the summer.
2024-05-27 23:22:42,074 - INFO - joeynmt.training - Example #4
2024-05-27 23:22:42,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:22:42,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:22:42,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'vic@@', 'e,', 'the', 'next', 'de@@', 'vic@@', 'e,', 'the', 'next', 'de@@', 'ar', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:22:42,075 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:22:42,075 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:22:42,075 - INFO - joeynmt.training - 	Hypothesis: The next device, the next device, the next dear fast on the last 25 years.
2024-05-27 23:22:59,382 - INFO - joeynmt.training - Epoch   6, Step:    21100, Batch Loss:     1.541125, Batch Acc: 0.549633, Tokens per Sec:     3940, Lr: 0.000300
2024-05-27 23:23:16,827 - INFO - joeynmt.training - Epoch   6, Step:    21200, Batch Loss:     1.432239, Batch Acc: 0.546955, Tokens per Sec:     3886, Lr: 0.000300
2024-05-27 23:23:33,659 - INFO - joeynmt.training - Epoch   6, Step:    21300, Batch Loss:     1.731708, Batch Acc: 0.545093, Tokens per Sec:     4089, Lr: 0.000300
2024-05-27 23:23:50,431 - INFO - joeynmt.training - Epoch   6, Step:    21400, Batch Loss:     1.468896, Batch Acc: 0.547343, Tokens per Sec:     4161, Lr: 0.000300
2024-05-27 23:24:07,497 - INFO - joeynmt.training - Epoch   6, Step:    21500, Batch Loss:     1.532645, Batch Acc: 0.550638, Tokens per Sec:     4091, Lr: 0.000300
2024-05-27 23:24:07,499 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:24:07,499 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:25:01,000 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.51, generation: 53.4951[sec], evaluation: 0.0000[sec]
2024-05-27 23:25:01,167 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/19000.ckpt
2024-05-27 23:25:01,168 - INFO - joeynmt.training - Example #0
2024-05-27 23:25:01,168 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'n@@', 'ice', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', '--', 'which', 'is', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'you', 'have', 'the', 'size', 'of', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the nice glacial calotter -- which is almost three million years has had the size of 48 U.S. continental continents, you have the size of 40 percent.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - Example #1
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'gl@@', 'aci@@', 'er.', '</s>']
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the glacier.
2024-05-27 23:25:01,169 - INFO - joeynmt.training - Example #2
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:25:01,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'aci@@', 'al', 'he@@', 'at', 'is', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cle@@', 'an', 'hear@@', 't', 'cle@@', 'an', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:25:01,169 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Hypothesis: The glacial heat is is, in a sense, the heart clean heart clean heart of the global climate system.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - Example #3
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'in@@', 'ver@@', 'su@@', 's', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'ari@@', 'a.', '</s>']
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Hypothesis: You get inversus and return it into the summer of the summaria.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - Example #4
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:25:01,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'next', 'de@@', 'a@@', 'positi@@', 've', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:25:01,170 - INFO - joeynmt.training - 	Hypothesis: The next next deapositive is going to be a quick carrelled on the last 25 years.
2024-05-27 23:25:18,260 - INFO - joeynmt.training - Epoch   6, Step:    21600, Batch Loss:     1.485543, Batch Acc: 0.552992, Tokens per Sec:     3858, Lr: 0.000300
2024-05-27 23:25:35,170 - INFO - joeynmt.training - Epoch   6, Step:    21700, Batch Loss:     1.802371, Batch Acc: 0.549809, Tokens per Sec:     4140, Lr: 0.000300
2024-05-27 23:25:52,473 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.534964, Batch Acc: 0.542483, Tokens per Sec:     4015, Lr: 0.000300
2024-05-27 23:26:09,959 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.493474, Batch Acc: 0.551223, Tokens per Sec:     3967, Lr: 0.000300
2024-05-27 23:26:26,587 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.712911, Batch Acc: 0.549597, Tokens per Sec:     4290, Lr: 0.000300
2024-05-27 23:26:26,587 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:26:26,587 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:27:42,084 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.09, acc:   0.51, generation: 75.4902[sec], evaluation: 0.0000[sec]
2024-05-27 23:27:42,085 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:27:42,253 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/20000.ckpt
2024-05-27 23:27:42,254 - INFO - joeynmt.training - Example #0
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'of', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'at', 'that', 'time,', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:27:42,254 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:27:42,254 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:27:42,254 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to show that the calculation of the artical heat at that time, for almost three million years has had the size of the 48 United States of the United States of the 40 percent.
2024-05-27 23:27:42,254 - INFO - joeynmt.training - Example #1
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:27:42,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'grav@@', 'ity', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice.', '</s>']
2024-05-27 23:27:42,254 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of the problem because it doesn't show the gravity of the ice problem because it doesn't explain the ice.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - Example #2
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'art', 'of', 'the', 'ar@@', 't@@', 'ic', 'he@@', 'at', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Hypothesis: The art of the artic heat is, in a sense, the heart of global climate system.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - Example #3
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'out', 'of', 'in@@', 'ver@@', 'sion', 'and', 're@@', 'turn', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Hypothesis: You get out of inversion and return into the summer.
2024-05-27 23:27:42,255 - INFO - joeynmt.training - Example #4
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:27:42,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:27:42,255 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:27:42,256 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:27:42,256 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrelled on the last 25 years.
2024-05-27 23:27:59,225 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.651459, Batch Acc: 0.542341, Tokens per Sec:     3975, Lr: 0.000300
2024-05-27 23:28:15,994 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.618561, Batch Acc: 0.547795, Tokens per Sec:     4067, Lr: 0.000300
2024-05-27 23:28:33,687 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.642054, Batch Acc: 0.550023, Tokens per Sec:     4013, Lr: 0.000300
2024-05-27 23:28:52,047 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.581000, Batch Acc: 0.550641, Tokens per Sec:     3728, Lr: 0.000300
2024-05-27 23:29:09,683 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.508570, Batch Acc: 0.541643, Tokens per Sec:     4034, Lr: 0.000300
2024-05-27 23:29:09,684 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:29:09,684 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:30:14,078 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.06, acc:   0.51, generation: 64.3873[sec], evaluation: 0.0000[sec]
2024-05-27 23:30:14,079 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:30:14,247 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/20500.ckpt
2024-05-27 23:30:14,249 - INFO - joeynmt.training - Example #0
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'n@@', 'ice', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'that', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'percent@@', '.', '</s>']
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the nice glacial calotter glacial calotter that for almost three million years has had the size of the 48 percent.
2024-05-27 23:30:14,249 - INFO - joeynmt.training - Example #1
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:30:14,249 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ence', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'be', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'be', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', 'shows', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because']
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:30:14,249 - INFO - joeynmt.training - 	Hypothesis: But this subvalence of the gravity of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the ice of the problem because it doesn't be the ice of the ice of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the problem because it doesn't be the ice of the ice problem because it shows the ice of the problem because it doesn't show the ice of the problem because
2024-05-27 23:30:14,250 - INFO - joeynmt.training - Example #2
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'is', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an@@', 'er', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Hypothesis: The artic glacial is is in a sense, the cleaner heart of the global climate system.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - Example #3
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'ex@@', 'p@@', 'and@@', 's', 'and', 'f@@', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Hypothesis: And you expands and fit into the summer.
2024-05-27 23:30:14,250 - INFO - joeynmt.training - Example #4
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:30:14,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'fast@@', 'er', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:30:14,250 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:30:14,251 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:30:14,251 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick faster in the last 25 years.
2024-05-27 23:30:31,389 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.398806, Batch Acc: 0.548962, Tokens per Sec:     3943, Lr: 0.000300
2024-05-27 23:30:48,520 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.544635, Batch Acc: 0.549518, Tokens per Sec:     4001, Lr: 0.000300
2024-05-27 23:31:06,414 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.521033, Batch Acc: 0.553492, Tokens per Sec:     4023, Lr: 0.000300
2024-05-27 23:31:23,858 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.727573, Batch Acc: 0.551320, Tokens per Sec:     3999, Lr: 0.000300
2024-05-27 23:31:41,387 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.509386, Batch Acc: 0.553554, Tokens per Sec:     3861, Lr: 0.000300
2024-05-27 23:31:41,388 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:31:41,388 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:32:42,440 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.07, acc:   0.51, generation: 61.0460[sec], evaluation: 0.0000[sec]
2024-05-27 23:32:42,607 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/21500.ckpt
2024-05-27 23:32:42,608 - INFO - joeynmt.training - Example #0
2024-05-27 23:32:42,608 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:32:42,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:32:42,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'ter', '--', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'had', 'the', 'size', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', '--', 'you', 'have', '4@@', '0', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show that slide to show that the calculation that the artical calotter -- which for almost three million years has had had the size of the U.S. continental -- you have 40 percent of the United States of continental --
2024-05-27 23:32:42,609 - INFO - joeynmt.training - Example #1
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'gra@@', 'de', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Hypothesis: But this undergrade the gravity of the problem because it doesn't show the ice of the ice problem of the ice problem.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - Example #2
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'y', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an@@', 'er', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - 	Hypothesis: The arty glacial calotter is in a sense, the cleaner heart of the global climate system.
2024-05-27 23:32:42,609 - INFO - joeynmt.training - Example #3
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:32:42,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'the', 'w@@', 'av@@', 'es', 'of', 'w@@', 'int@@', 'er', 'and', 'you', 'get', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Hypothesis: You expand the waves of winter and you get it into the summer.
2024-05-27 23:32:42,610 - INFO - joeynmt.training - Example #4
2024-05-27 23:32:42,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:32:42,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:32:42,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:32:42,610 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlled on the last 25 years.
2024-05-27 23:32:59,774 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.437255, Batch Acc: 0.552958, Tokens per Sec:     4018, Lr: 0.000300
2024-05-27 23:33:16,861 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.475001, Batch Acc: 0.543816, Tokens per Sec:     3987, Lr: 0.000300
2024-05-27 23:33:33,779 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.507290, Batch Acc: 0.546748, Tokens per Sec:     4185, Lr: 0.000300
2024-05-27 23:33:51,186 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.592857, Batch Acc: 0.553592, Tokens per Sec:     4021, Lr: 0.000300
2024-05-27 23:34:08,306 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.428723, Batch Acc: 0.553838, Tokens per Sec:     4103, Lr: 0.000300
2024-05-27 23:34:08,306 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:34:08,306 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:35:09,459 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.04, acc:   0.51, generation: 61.1465[sec], evaluation: 0.0000[sec]
2024-05-27 23:35:09,462 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:35:09,628 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/21000.ckpt
2024-05-27 23:35:09,630 - INFO - joeynmt.training - Example #0
2024-05-27 23:35:09,630 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:35:09,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:35:09,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'of', 'the', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', '--', 'which', 'is', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'you', 'have', 'the', 'size', 'of', '4@@', '8', 'percent@@', '.', '</s>']
2024-05-27 23:35:09,630 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides to show that the slide of the article calculation -- which is almost three million years has had the size of 48 United States continents, you have the size of 48 percent.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - Example #1
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pla@@', 'in', 'the', 'ice', 'of', 'the', 'ice', 'problem', '--', '</s>']
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue of the problem because it doesn't explain the ice of the ice problem --
2024-05-27 23:35:09,631 - INFO - joeynmt.training - Example #2
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'cal@@', 'o@@', 'tic', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - 	Hypothesis: The artic calotic calculation is, in a sense, the heart of the climate system.
2024-05-27 23:35:09,631 - INFO - joeynmt.training - Example #3
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:35:09,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 's', 'and', 'you', 'get', 'out', 'and', 'you', 'get', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'ex@@', 'p@@', 'and@@', 's.', '</s>']
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Hypothesis: You expands and you get out and you get out of the summer and expands.
2024-05-27 23:35:09,632 - INFO - joeynmt.training - Example #4
2024-05-27 23:35:09,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:35:09,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:35:09,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'fast@@', ',', 'the', 'next', '25', 'years.', '</s>']
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:35:09,632 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick fast, the next 25 years.
2024-05-27 23:35:26,065 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.709455, Batch Acc: 0.550004, Tokens per Sec:     4039, Lr: 0.000300
2024-05-27 23:35:43,114 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.627109, Batch Acc: 0.545221, Tokens per Sec:     3995, Lr: 0.000300
2024-05-27 23:35:49,571 - INFO - joeynmt.training - Epoch   6: total training loss 6382.30
2024-05-27 23:35:49,571 - INFO - joeynmt.training - EPOCH 7
2024-05-27 23:35:59,840 - INFO - joeynmt.training - Epoch   7, Step:    23800, Batch Loss:     1.470333, Batch Acc: 0.572095, Tokens per Sec:     4114, Lr: 0.000300
2024-05-27 23:36:16,928 - INFO - joeynmt.training - Epoch   7, Step:    23900, Batch Loss:     1.611321, Batch Acc: 0.578702, Tokens per Sec:     4143, Lr: 0.000300
2024-05-27 23:36:33,541 - INFO - joeynmt.training - Epoch   7, Step:    24000, Batch Loss:     1.572765, Batch Acc: 0.570897, Tokens per Sec:     4271, Lr: 0.000300
2024-05-27 23:36:33,541 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:36:33,541 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:37:45,106 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.00, acc:   0.51, generation: 71.5582[sec], evaluation: 0.0000[sec]
2024-05-27 23:37:45,108 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:37:45,278 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/19500.ckpt
2024-05-27 23:37:45,279 - INFO - joeynmt.training - Example #0
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'of', 'the', 'ar@@', 'tic@@', 'k@@', 'et', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'k@@', 'et', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'is', 'a', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '--', "it's", 'about', '40', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it', 'is', '4@@', '0@@', '-@@', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'and', "it's", 'about', 'three', 'percent', 'of', 'the', 'size', 'of', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of']
2024-05-27 23:37:45,279 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:37:45,279 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:37:45,279 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show that the slide of the articket to show that the articket for almost three million years has had the size of the 48 United States, is a continental continental -- it's about 40 percent of the United States, it is 40-percent of the United States, and it's about three percent of the size of three million years has had the size of
2024-05-27 23:37:45,279 - INFO - joeynmt.training - Example #1
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:37:45,279 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem@@', '.', '</s>']
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the ice problem.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - Example #2
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cle@@', 'ar', 'system@@', '.', '</s>']
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calculation is, in a sense, the heart clear system.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - Example #3
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 're@@', 'ver@@', 'se', 'it', 'is', 're@@', 'tur@@', 'ning', 'out', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - 	Hypothesis: It expanding and reverse it is returning out of the summer.
2024-05-27 23:37:45,280 - INFO - joeynmt.training - Example #4
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:37:45,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:37:45,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:37:45,281 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:37:45,281 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:37:45,281 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast of the last 25 years.
2024-05-27 23:38:03,198 - INFO - joeynmt.training - Epoch   7, Step:    24100, Batch Loss:     1.474778, Batch Acc: 0.574110, Tokens per Sec:     3899, Lr: 0.000300
2024-05-27 23:38:19,580 - INFO - joeynmt.training - Epoch   7, Step:    24200, Batch Loss:     1.483241, Batch Acc: 0.570636, Tokens per Sec:     4213, Lr: 0.000300
2024-05-27 23:38:35,494 - INFO - joeynmt.training - Epoch   7, Step:    24300, Batch Loss:     1.596520, Batch Acc: 0.566652, Tokens per Sec:     4379, Lr: 0.000300
2024-05-27 23:38:51,705 - INFO - joeynmt.training - Epoch   7, Step:    24400, Batch Loss:     1.493748, Batch Acc: 0.566329, Tokens per Sec:     4145, Lr: 0.000300
2024-05-27 23:39:08,454 - INFO - joeynmt.training - Epoch   7, Step:    24500, Batch Loss:     1.601878, Batch Acc: 0.565703, Tokens per Sec:     4210, Lr: 0.000300
2024-05-27 23:39:08,455 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:39:08,455 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:40:05,226 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.00, acc:   0.51, generation: 56.7638[sec], evaluation: 0.0000[sec]
2024-05-27 23:40:05,394 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/22000.ckpt
2024-05-27 23:40:05,395 - INFO - joeynmt.training - Example #0
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'sli@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'contin@@', 'ent@@', 's,', 'you', 'have', 'had', 'the', 'size', 'of', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slide slide to demonstrate that the calculation artica, which for almost three million years has had the size of 48 continents, you have had the size of 40-percent.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - Example #1
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pres@@', 's', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'ce@@', 'pt', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'ce@@', 'pt', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'ex@@', 'pres@@', 'se@@', 'd.', '</s>']
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of the problem because it doesn't show the ice of the problem because it doesn't express the ice of the ice of the problem because it doesn't except of the ice of the problem because it doesn't except of the problem because it doesn't expressed.
2024-05-27 23:40:05,396 - INFO - joeynmt.training - Example #2
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:40:05,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'le', 'is', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Hypothesis: The artickle is is in a sense, the clear heart of global climate system.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - Example #3
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 're@@', 'ver@@', 'se', 'and', 'you', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'ver@@', 'se', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'ver@@', 'se', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'ver@@', 'se', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'tur@@', 'n@@', 's.', '</s>']
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Hypothesis: You expanding and reverse and you return into the summer of the summer and reverse of the summer and reverse of the summer and reverse of the summer and returns.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - Example #4
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:40:05,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:40:05,397 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast of the last 25 years.
2024-05-27 23:40:22,253 - INFO - joeynmt.training - Epoch   7, Step:    24600, Batch Loss:     1.704085, Batch Acc: 0.562134, Tokens per Sec:     4161, Lr: 0.000300
2024-05-27 23:40:38,386 - INFO - joeynmt.training - Epoch   7, Step:    24700, Batch Loss:     1.522902, Batch Acc: 0.568597, Tokens per Sec:     4231, Lr: 0.000300
2024-05-27 23:40:54,110 - INFO - joeynmt.training - Epoch   7, Step:    24800, Batch Loss:     1.490855, Batch Acc: 0.561900, Tokens per Sec:     4477, Lr: 0.000300
2024-05-27 23:41:09,975 - INFO - joeynmt.training - Epoch   7, Step:    24900, Batch Loss:     1.530912, Batch Acc: 0.568357, Tokens per Sec:     4275, Lr: 0.000300
2024-05-27 23:41:26,904 - INFO - joeynmt.training - Epoch   7, Step:    25000, Batch Loss:     1.315192, Batch Acc: 0.568592, Tokens per Sec:     4103, Lr: 0.000300
2024-05-27 23:41:26,906 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:41:26,906 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:42:27,948 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.01, acc:   0.51, generation: 61.0350[sec], evaluation: 0.0000[sec]
2024-05-27 23:42:28,109 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/23000.ckpt
2024-05-27 23:42:28,110 - INFO - joeynmt.training - Example #0
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'te', 'gl@@', 'aci@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'you', 'get', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the calculation that the calculte glacial artica, which for almost three million years has had the size of the U.S. continents, you get 40 percent.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - Example #1
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'l@@', 'ying', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - 	Hypothesis: But this underlying the gravity of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice.
2024-05-27 23:42:28,111 - INFO - joeynmt.training - Example #2
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:42:28,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'cal@@', 'ot@@', 'ice', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Hypothesis: The artic calotice is in a sense, the heart heart of the climate system.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - Example #3
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'comes', 'out', 'of', 'w@@', 'int@@', 'er', 'and', 're@@', 'tur@@', 'n@@', 's.', '</s>']
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Hypothesis: It comes out of winter and returns.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - Example #4
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:42:28,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'qu@@', 'ick', 'for', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:42:28,112 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick quick for the last 25 years.
2024-05-27 23:42:44,107 - INFO - joeynmt.training - Epoch   7, Step:    25100, Batch Loss:     1.636668, Batch Acc: 0.569996, Tokens per Sec:     4283, Lr: 0.000300
2024-05-27 23:43:00,769 - INFO - joeynmt.training - Epoch   7, Step:    25200, Batch Loss:     1.260038, Batch Acc: 0.570998, Tokens per Sec:     4137, Lr: 0.000300
2024-05-27 23:43:17,251 - INFO - joeynmt.training - Epoch   7, Step:    25300, Batch Loss:     1.548196, Batch Acc: 0.558473, Tokens per Sec:     4109, Lr: 0.000300
2024-05-27 23:43:33,962 - INFO - joeynmt.training - Epoch   7, Step:    25400, Batch Loss:     1.521958, Batch Acc: 0.562913, Tokens per Sec:     4201, Lr: 0.000300
2024-05-27 23:43:50,692 - INFO - joeynmt.training - Epoch   7, Step:    25500, Batch Loss:     1.484911, Batch Acc: 0.562948, Tokens per Sec:     4100, Lr: 0.000300
2024-05-27 23:43:50,692 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:43:50,692 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:44:46,642 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.96, acc:   0.51, generation: 55.9439[sec], evaluation: 0.0000[sec]
2024-05-27 23:44:46,643 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:44:46,805 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/22500.ckpt
2024-05-27 23:44:46,806 - INFO - joeynmt.training - Example #0
2024-05-27 23:44:46,806 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:44:46,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'is', '4@@', '0@@', '-@@', 'American', 'dimension@@', 's', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the article to show that the artical calote, which for almost three million years has had had the size of 48 United States continents, is 40-American dimensions of the U.S. continental --
2024-05-27 23:44:46,807 - INFO - joeynmt.training - Example #1
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'is@@', 'sue', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue of the problem because it doesn't show the ice of the issue because it doesn't show the ice of the ice of the ice.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - Example #2
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:44:46,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'he@@', 'a@@', 'ded', 'b@@', 'y@@', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - 	Hypothesis: The artic headed bycalculation is in a sense, the heart heart of global climate system.
2024-05-27 23:44:46,807 - INFO - joeynmt.training - Example #3
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'comes', 'out', 'and', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'out.', '</s>']
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Hypothesis: It comes out and winter and return it into the summer and return it out.
2024-05-27 23:44:46,808 - INFO - joeynmt.training - Example #4
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:44:46,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'car@@', 'ro@@', 'll', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:44:46,808 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid carroll on the last 25 years.
2024-05-27 23:45:03,020 - INFO - joeynmt.training - Epoch   7, Step:    25600, Batch Loss:     1.665583, Batch Acc: 0.561384, Tokens per Sec:     4286, Lr: 0.000300
2024-05-27 23:45:19,853 - INFO - joeynmt.training - Epoch   7, Step:    25700, Batch Loss:     1.418091, Batch Acc: 0.561462, Tokens per Sec:     4147, Lr: 0.000300
2024-05-27 23:45:36,613 - INFO - joeynmt.training - Epoch   7, Step:    25800, Batch Loss:     1.622843, Batch Acc: 0.559921, Tokens per Sec:     4105, Lr: 0.000300
2024-05-27 23:45:53,153 - INFO - joeynmt.training - Epoch   7, Step:    25900, Batch Loss:     1.564990, Batch Acc: 0.562661, Tokens per Sec:     4124, Lr: 0.000300
2024-05-27 23:46:09,935 - INFO - joeynmt.training - Epoch   7, Step:    26000, Batch Loss:     1.574968, Batch Acc: 0.563674, Tokens per Sec:     4090, Lr: 0.000300
2024-05-27 23:46:09,935 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:46:09,935 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:47:11,490 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   5.96, acc:   0.51, generation: 61.5488[sec], evaluation: 0.0000[sec]
2024-05-27 23:47:11,661 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/23500.ckpt
2024-05-27 23:47:11,663 - INFO - joeynmt.training - Example #0
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', 'e,', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'you', 'have', 'been', 'stre@@', 't@@', 'ched', 'by', 'the', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-27 23:47:11,663 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:47:11,663 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:47:11,663 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article calote, which is about three million years has had the size of 48 million years has had the size of 48 United States continents, you have been stretched by the continental --
2024-05-27 23:47:11,663 - INFO - joeynmt.training - Example #1
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:47:11,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'l@@', 'ying', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice.', '</s>']
2024-05-27 23:47:11,663 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:47:11,663 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Hypothesis: But this underlying gravity of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - Example #2
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'le', 'is', 'is', 'in', 'a', 'way,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Hypothesis: The artickle is is in a way, the clear heart of the global climate system.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - Example #3
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'out', 'and', 'you', 'get', 'out', 'and', 'you', 're@@', 'turn', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'it', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'it', 'back', 'and', 're@@', 'turn', 'it', 'back', 'and', 'you', 'get', 'it', 'back', 'and', 're@@', 'turn', 'it', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'you', 'get', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'y.', '</s>']
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Hypothesis: You get out and you get out and you return into the summer of the summer of the summer and you get it out of the summer and you get it back and return it back and you get it back and return it out of the summer and you get it into the summer of the summy.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - Example #4
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:47:11,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:47:11,664 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:47:11,665 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-27 23:47:28,064 - INFO - joeynmt.training - Epoch   7, Step:    26100, Batch Loss:     1.423560, Batch Acc: 0.558212, Tokens per Sec:     4394, Lr: 0.000300
2024-05-27 23:47:44,620 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.636194, Batch Acc: 0.563652, Tokens per Sec:     4130, Lr: 0.000300
2024-05-27 23:48:01,564 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.578670, Batch Acc: 0.560612, Tokens per Sec:     4134, Lr: 0.000300
2024-05-27 23:48:18,381 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.388741, Batch Acc: 0.563532, Tokens per Sec:     4125, Lr: 0.000300
2024-05-27 23:48:34,775 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.528482, Batch Acc: 0.560046, Tokens per Sec:     4188, Lr: 0.000300
2024-05-27 23:48:34,776 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:48:34,776 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:49:46,189 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.92, acc:   0.51, generation: 71.4065[sec], evaluation: 0.0000[sec]
2024-05-27 23:49:46,191 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:49:46,356 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/25000.ckpt
2024-05-27 23:49:46,358 - INFO - joeynmt.training - Example #0
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'tim@@', 'ate', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'to@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'you', 'have', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show these slides to show that that the calcultimate glacial calottoa, which for almost three million years has had the size of the United States continents, you have 40 percent.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - Example #1
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of gravity of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice.
2024-05-27 23:49:46,358 - INFO - joeynmt.training - Example #2
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:49:46,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'to@@', 'war@@', 'ds', 'the', 'cle@@', 'an@@', 'ing', 'hear@@', 't@@', '-@@', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calottowards the cleaning heart-of the global climate system.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - Example #3
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'you', 'get', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Hypothesis: It expands and you get it into the summer.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - Example #4
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:49:46,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'fast@@', 'er', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:49:46,359 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick faster on the last 25 years.
2024-05-27 23:50:03,001 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.625433, Batch Acc: 0.556954, Tokens per Sec:     4126, Lr: 0.000300
2024-05-27 23:50:19,488 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.554319, Batch Acc: 0.564661, Tokens per Sec:     4139, Lr: 0.000300
2024-05-27 23:50:35,994 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.496108, Batch Acc: 0.558889, Tokens per Sec:     4170, Lr: 0.000300
2024-05-27 23:50:51,902 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.445022, Batch Acc: 0.560224, Tokens per Sec:     4351, Lr: 0.000300
2024-05-27 23:51:08,781 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.390816, Batch Acc: 0.559225, Tokens per Sec:     4077, Lr: 0.000300
2024-05-27 23:51:08,782 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:51:08,782 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:52:05,787 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.87, acc:   0.52, generation: 56.9988[sec], evaluation: 0.0000[sec]
2024-05-27 23:52:05,789 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-27 23:52:05,952 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/24500.ckpt
2024-05-27 23:52:05,953 - INFO - joeynmt.training - Example #0
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'um', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'it', 're@@', 'ver@@', 'ted', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the calculation that the calculum glacial calot, which for almost three million years has been the size of the 48 United States, it reverted 40 percent.
2024-05-27 23:52:05,953 - INFO - joeynmt.training - Example #1
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:52:05,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:52:05,953 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of the problem because it doesn't show the ice of the ice.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - Example #2
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', '</s>']
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Hypothesis: The artickle calculation is, in a way, the heart of the climate system of the climate system
2024-05-27 23:52:05,954 - INFO - joeynmt.training - Example #3
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'in', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Hypothesis: You expand in the summer and return it into the summer.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - Example #4
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:52:05,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd@@', 'ly', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:52:05,954 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:52:05,955 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapidly fast on the last 25 years.
2024-05-27 23:52:22,328 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.525174, Batch Acc: 0.561069, Tokens per Sec:     4145, Lr: 0.000300
2024-05-27 23:52:38,803 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.699052, Batch Acc: 0.559829, Tokens per Sec:     4337, Lr: 0.000300
2024-05-27 23:52:55,109 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.628595, Batch Acc: 0.558775, Tokens per Sec:     4231, Lr: 0.000300
2024-05-27 23:53:12,092 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.610862, Batch Acc: 0.555660, Tokens per Sec:     4147, Lr: 0.000300
2024-05-27 23:53:28,747 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.573555, Batch Acc: 0.554612, Tokens per Sec:     4166, Lr: 0.000300
2024-05-27 23:53:28,749 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:53:28,749 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:54:25,818 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.89, acc:   0.52, generation: 57.0634[sec], evaluation: 0.0000[sec]
2024-05-27 23:54:25,988 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/24000.ckpt
2024-05-27 23:54:25,989 - INFO - joeynmt.training - Example #0
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'in', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', '</s>']
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article calote, which for almost three million years has had the size of the 48 United States of 48 United States continental continental in the continental continents,
2024-05-27 23:54:25,990 - INFO - joeynmt.training - Example #1
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'jec@@', 't', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Hypothesis: But this subject the gravity of the problem because it doesn't show the pace of the ice because it doesn't show the ice of the ice of the ice.
2024-05-27 23:54:25,990 - INFO - joeynmt.training - Example #2
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:54:25,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'et', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an@@', 'er', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:54:25,990 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Hypothesis: The articket glacial calote, in a sense, the cleaner heart of global climate system.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - Example #3
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'the', 'sum@@', 'm@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Hypothesis: You expand the summer and return it into the summer.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - Example #4
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:54:25,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'pa@@', 'per', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:54:25,991 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick paper in the last 25 years.
2024-05-27 23:54:42,628 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.427971, Batch Acc: 0.565596, Tokens per Sec:     4148, Lr: 0.000300
2024-05-27 23:54:57,731 - INFO - joeynmt.training - Epoch   7: total training loss 6146.90
2024-05-27 23:54:57,731 - INFO - joeynmt.training - EPOCH 8
2024-05-27 23:54:59,080 - INFO - joeynmt.training - Epoch   8, Step:    27700, Batch Loss:     1.301551, Batch Acc: 0.595101, Tokens per Sec:     4511, Lr: 0.000300
2024-05-27 23:55:14,918 - INFO - joeynmt.training - Epoch   8, Step:    27800, Batch Loss:     1.455673, Batch Acc: 0.581417, Tokens per Sec:     4440, Lr: 0.000300
2024-05-27 23:55:31,675 - INFO - joeynmt.training - Epoch   8, Step:    27900, Batch Loss:     1.436422, Batch Acc: 0.581886, Tokens per Sec:     4188, Lr: 0.000300
2024-05-27 23:55:47,969 - INFO - joeynmt.training - Epoch   8, Step:    28000, Batch Loss:     1.309381, Batch Acc: 0.584079, Tokens per Sec:     4317, Lr: 0.000300
2024-05-27 23:55:47,969 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:55:47,969 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:56:43,171 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.89, acc:   0.52, generation: 55.1954[sec], evaluation: 0.0000[sec]
2024-05-27 23:56:43,335 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/26000.ckpt
2024-05-27 23:56:43,336 - INFO - joeynmt.training - Example #0
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'you', 'have', 'been', 'ro@@', 'w@@', 'ed', 'by', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the article calculation that the article calot, which for almost three million years has had the size of the 48 United States continents, you have been rowed by 40 percent.
2024-05-27 23:56:43,336 - INFO - joeynmt.training - Example #1
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:56:43,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the']
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:56:43,336 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the
2024-05-27 23:56:43,337 - INFO - joeynmt.training - Example #2
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'pr@@', 'int', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calotprint is, in a sense, the heart customer of global climate system.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - Example #3
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'mer@@', 's', 'and', 're@@', 'turn', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Hypothesis: You expand you get into the summers and return into the summer.
2024-05-27 23:56:43,337 - INFO - joeynmt.training - Example #4
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:56:43,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:56:43,337 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:56:43,338 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:56:43,338 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid carlled on the last 25 years.
2024-05-27 23:56:59,803 - INFO - joeynmt.training - Epoch   8, Step:    28100, Batch Loss:     1.623198, Batch Acc: 0.585265, Tokens per Sec:     4316, Lr: 0.000300
2024-05-27 23:57:15,923 - INFO - joeynmt.training - Epoch   8, Step:    28200, Batch Loss:     1.690268, Batch Acc: 0.584276, Tokens per Sec:     4246, Lr: 0.000300
2024-05-27 23:57:31,998 - INFO - joeynmt.training - Epoch   8, Step:    28300, Batch Loss:     1.452208, Batch Acc: 0.582522, Tokens per Sec:     4414, Lr: 0.000300
2024-05-27 23:57:48,333 - INFO - joeynmt.training - Epoch   8, Step:    28400, Batch Loss:     1.429134, Batch Acc: 0.581678, Tokens per Sec:     4407, Lr: 0.000300
2024-05-27 23:58:04,392 - INFO - joeynmt.training - Epoch   8, Step:    28500, Batch Loss:     1.358175, Batch Acc: 0.583215, Tokens per Sec:     4281, Lr: 0.000300
2024-05-27 23:58:04,392 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-27 23:58:04,392 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-27 23:59:16,287 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.87, acc:   0.52, generation: 71.8886[sec], evaluation: 0.0000[sec]
2024-05-27 23:59:16,452 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/25500.ckpt
2024-05-27 23:59:16,453 - INFO - joeynmt.training - Example #0
2024-05-27 23:59:16,453 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-27 23:59:16,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-27 23:59:16,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'is', 'contin@@', 'ent@@', 's,', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-27 23:59:16,453 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-27 23:59:16,453 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-27 23:59:16,453 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slide slide to show that the article calot, which for almost three million years has had the size of the 48 United States of the United States is continents, 40 percent.
2024-05-27 23:59:16,453 - INFO - joeynmt.training - Example #1
2024-05-27 23:59:16,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-27 23:59:16,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Hypothesis: But this underestimate the gravity of the problem because it doesn't show the ice of the ice because it doesn't show the ice of the ice.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - Example #2
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'is', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'li@@', 'ver@@', 's', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Hypothesis: The article is in a certain sense, the artical helivers of global climate system.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - Example #3
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'ver@@', 'se', 'is', 're@@', 'ce@@', 'i@@', 've', 'and', 're@@', 'turn', 'it', 'out', 'and', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - 	Hypothesis: You get the winter and reverse is receive and return it out and the summer.
2024-05-27 23:59:16,454 - INFO - joeynmt.training - Example #4
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-27 23:59:16,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'quick@@', 'ly', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-27 23:59:16,455 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-27 23:59:16,455 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-27 23:59:16,455 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly fast on the last 25 years.
2024-05-27 23:59:33,389 - INFO - joeynmt.training - Epoch   8, Step:    28600, Batch Loss:     1.381879, Batch Acc: 0.582321, Tokens per Sec:     4064, Lr: 0.000300
2024-05-27 23:59:49,843 - INFO - joeynmt.training - Epoch   8, Step:    28700, Batch Loss:     1.315239, Batch Acc: 0.576947, Tokens per Sec:     4264, Lr: 0.000300
2024-05-28 00:00:06,179 - INFO - joeynmt.training - Epoch   8, Step:    28800, Batch Loss:     1.498232, Batch Acc: 0.571549, Tokens per Sec:     4367, Lr: 0.000300
2024-05-28 00:00:22,948 - INFO - joeynmt.training - Epoch   8, Step:    28900, Batch Loss:     1.332777, Batch Acc: 0.578320, Tokens per Sec:     4324, Lr: 0.000300
2024-05-28 00:00:39,532 - INFO - joeynmt.training - Epoch   8, Step:    29000, Batch Loss:     1.344975, Batch Acc: 0.578782, Tokens per Sec:     4111, Lr: 0.000300
2024-05-28 00:00:39,532 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:00:39,532 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:01:38,467 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.85, acc:   0.52, generation: 58.9286[sec], evaluation: 0.0000[sec]
2024-05-28 00:01:38,468 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:01:38,632 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/26500.ckpt
2024-05-28 00:01:38,633 - INFO - joeynmt.training - Example #0
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'um', 'for', 'ne@@', 'ar@@', 'ly', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'of', 'the', 'United', 'St@@', 'ates', 'is', 'contin@@', 'ent@@', 'al', 'of', 'the', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the calculation that the calculum for nearly three million years has had the size of the 48 of the United States is continental of the 40 percent.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - Example #1
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of the gravity of the problem because it doesn't show the pace of the ice because it doesn't show the pace of the ice.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - Example #2
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:01:38,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'is', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'li@@', 'gh@@', 'ts', 'the', 'cle@@', 'an@@', 'ing', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:01:38,634 - INFO - joeynmt.training - 	Hypothesis: The article is the artical helights the cleaning heart of the global climate system.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - Example #3
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'w@@', 'int@@', 's', 'and', 'w@@', 'int@@', 'er', 'it', 'f@@', 'low@@', '.', '</s>']
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Hypothesis: You get the wints and winter it flow.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - Example #4
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:01:38,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:01:38,635 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid carlled on the last 25 years.
2024-05-28 00:01:54,582 - INFO - joeynmt.training - Epoch   8, Step:    29100, Batch Loss:     1.309595, Batch Acc: 0.573010, Tokens per Sec:     4204, Lr: 0.000300
2024-05-28 00:02:10,719 - INFO - joeynmt.training - Epoch   8, Step:    29200, Batch Loss:     1.435116, Batch Acc: 0.574617, Tokens per Sec:     4354, Lr: 0.000300
2024-05-28 00:02:27,386 - INFO - joeynmt.training - Epoch   8, Step:    29300, Batch Loss:     1.539651, Batch Acc: 0.574935, Tokens per Sec:     4227, Lr: 0.000300
2024-05-28 00:02:43,515 - INFO - joeynmt.training - Epoch   8, Step:    29400, Batch Loss:     1.613301, Batch Acc: 0.576754, Tokens per Sec:     4215, Lr: 0.000300
2024-05-28 00:02:59,626 - INFO - joeynmt.training - Epoch   8, Step:    29500, Batch Loss:     1.635253, Batch Acc: 0.565096, Tokens per Sec:     4318, Lr: 0.000300
2024-05-28 00:02:59,626 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:02:59,626 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:04:06,325 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.82, acc:   0.52, generation: 66.6918[sec], evaluation: 0.0000[sec]
2024-05-28 00:04:06,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:04:06,491 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/28000.ckpt
2024-05-28 00:04:06,492 - INFO - joeynmt.training - Example #0
2024-05-28 00:04:06,492 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:04:06,492 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:04:06,492 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de@@', 's,', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'have', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'is', 'contin@@', 'ent@@', 'al', 'of', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:04:06,492 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:04:06,492 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the slides, that the calculus that the calculus that the calculus have the size of 48 United States is continental of 40 percent.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - Example #1
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'in@@', 'k', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of the problem because it doesn't show the pink of the ice because it doesn't show the ice of the ice.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - Example #2
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'cli@@', 'mat@@', 'e.', '</s>']
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Hypothesis: The article calculation is, in a sense, the heart of the global climate.
2024-05-28 00:04:06,493 - INFO - joeynmt.training - Example #3
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:04:06,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'you', 'get', 'into', 'the', 'sum@@', 'mer@@', 's', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:04:06,493 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:04:06,494 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:04:06,494 - INFO - joeynmt.training - 	Hypothesis: You expand you get into the summers and return it into the summer.
2024-05-28 00:04:06,494 - INFO - joeynmt.training - Example #4
2024-05-28 00:04:06,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:04:06,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:04:06,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 're@@', 'll@@', 'ed', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:04:06,494 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:04:06,494 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:04:06,494 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick relled in the last 25 years.
2024-05-28 00:04:22,990 - INFO - joeynmt.training - Epoch   8, Step:    29600, Batch Loss:     1.455104, Batch Acc: 0.571070, Tokens per Sec:     4211, Lr: 0.000300
2024-05-28 00:04:39,588 - INFO - joeynmt.training - Epoch   8, Step:    29700, Batch Loss:     1.405744, Batch Acc: 0.578007, Tokens per Sec:     4203, Lr: 0.000300
2024-05-28 00:04:55,532 - INFO - joeynmt.training - Epoch   8, Step:    29800, Batch Loss:     1.517825, Batch Acc: 0.573758, Tokens per Sec:     4242, Lr: 0.000300
2024-05-28 00:05:11,820 - INFO - joeynmt.training - Epoch   8, Step:    29900, Batch Loss:     1.604290, Batch Acc: 0.571713, Tokens per Sec:     4324, Lr: 0.000300
2024-05-28 00:05:28,390 - INFO - joeynmt.training - Epoch   8, Step:    30000, Batch Loss:     1.473567, Batch Acc: 0.575091, Tokens per Sec:     4129, Lr: 0.000300
2024-05-28 00:05:28,390 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:05:28,390 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:06:31,093 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.81, acc:   0.52, generation: 62.6961[sec], evaluation: 0.0000[sec]
2024-05-28 00:06:31,095 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:06:31,258 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/27500.ckpt
2024-05-28 00:06:31,259 - INFO - joeynmt.training - Example #0
2024-05-28 00:06:31,259 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:06:31,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:06:31,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'that', 'the', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'um', 'that', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'has', '4@@', '8', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '--', 'and', 'a', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:06:31,259 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:06:31,259 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:06:31,259 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article that the article calculum that for almost three million years has had the size of the 48 United States has 48 continental continental continental continental -- and a 40 percent.
2024-05-28 00:06:31,259 - INFO - joeynmt.training - Example #1
2024-05-28 00:06:31,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'l@@', 'ying', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Hypothesis: But this underlying gravity of the problem because it doesn't show the ice of the ice.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - Example #2
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'at', 'gl@@', 'aci@@', 'al', 'gl@@', 'aci@@', 'al', 'system@@', ',', 'is', 'a', 'way,', 'the', 'global', 'cu@@', 'st@@', 'om@@', 'er', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Hypothesis: The article at glacial glacial system, is a way, the global customer of the global climate system.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - Example #3
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:06:31,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - 	Hypothesis: It expand the winter and return return it into the summer.
2024-05-28 00:06:31,260 - INFO - joeynmt.training - Example #4
2024-05-28 00:06:31,261 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:06:31,261 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:06:31,261 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'fast@@', 'er', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:06:31,261 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:06:31,261 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:06:31,261 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick faster on the last 25 years.
2024-05-28 00:06:47,379 - INFO - joeynmt.training - Epoch   8, Step:    30100, Batch Loss:     1.482489, Batch Acc: 0.572915, Tokens per Sec:     4191, Lr: 0.000300
2024-05-28 00:07:03,752 - INFO - joeynmt.training - Epoch   8, Step:    30200, Batch Loss:     1.558653, Batch Acc: 0.567005, Tokens per Sec:     4119, Lr: 0.000300
2024-05-28 00:07:19,916 - INFO - joeynmt.training - Epoch   8, Step:    30300, Batch Loss:     1.700549, Batch Acc: 0.575483, Tokens per Sec:     4199, Lr: 0.000300
2024-05-28 00:07:36,242 - INFO - joeynmt.training - Epoch   8, Step:    30400, Batch Loss:     1.571374, Batch Acc: 0.574686, Tokens per Sec:     4172, Lr: 0.000300
2024-05-28 00:07:52,749 - INFO - joeynmt.training - Epoch   8, Step:    30500, Batch Loss:     1.460446, Batch Acc: 0.563771, Tokens per Sec:     4227, Lr: 0.000300
2024-05-28 00:07:52,749 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:07:52,749 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:08:59,894 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.79, acc:   0.52, generation: 67.1388[sec], evaluation: 0.0000[sec]
2024-05-28 00:08:59,896 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:09:00,068 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/28500.ckpt
2024-05-28 00:09:00,069 - INFO - joeynmt.training - Example #0
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'he@@', 'at', 'the', 'he@@', 'at', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'at', 'that', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'is', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-28 00:09:00,069 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:09:00,069 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:09:00,069 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to demonstrate that the heat the heat the artical heat at that almost three million years has had the size of 48 United States is the continental continental continental continental continental --
2024-05-28 00:09:00,069 - INFO - joeynmt.training - Example #1
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:09:00,069 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'you', 'as', 'a', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'you', 'as', 'a', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'you', 'as', 'a', 'ice', 'as', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of']
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the ice of the ice problem because it doesn't show the ice of the ice of the problem because it doesn't show you as a ice of the problem because it doesn't show you as a ice of the problem because it doesn't show you as a ice as it doesn't show the ice of the problem because it doesn't show the ice of the problem because it doesn't show the ice of
2024-05-28 00:09:00,070 - INFO - joeynmt.training - Example #2
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'at', 'is', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Hypothesis: The artical heat at is is, in a sense, the clear heart of the global climate system.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - Example #3
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'in', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - 	Hypothesis: It expand in the winter and return it into the summer.
2024-05-28 00:09:00,070 - INFO - joeynmt.training - Example #4
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:09:00,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'de@@', 'ar', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:09:00,071 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:09:00,071 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:09:00,071 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick dear on the last 25 years.
2024-05-28 00:09:16,647 - INFO - joeynmt.training - Epoch   8, Step:    30600, Batch Loss:     1.526361, Batch Acc: 0.580721, Tokens per Sec:     4117, Lr: 0.000300
2024-05-28 00:09:33,241 - INFO - joeynmt.training - Epoch   8, Step:    30700, Batch Loss:     1.487885, Batch Acc: 0.571153, Tokens per Sec:     4181, Lr: 0.000300
2024-05-28 00:09:49,924 - INFO - joeynmt.training - Epoch   8, Step:    30800, Batch Loss:     1.498443, Batch Acc: 0.572708, Tokens per Sec:     4116, Lr: 0.000300
2024-05-28 00:10:06,513 - INFO - joeynmt.training - Epoch   8, Step:    30900, Batch Loss:     1.614468, Batch Acc: 0.571099, Tokens per Sec:     4210, Lr: 0.000300
2024-05-28 00:10:22,873 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.608246, Batch Acc: 0.575574, Tokens per Sec:     4264, Lr: 0.000300
2024-05-28 00:10:22,874 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:10:22,874 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:11:18,928 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.79, acc:   0.52, generation: 56.0479[sec], evaluation: 0.0000[sec]
2024-05-28 00:11:19,096 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/27000.ckpt
2024-05-28 00:11:19,097 - INFO - joeynmt.training - Example #0
2024-05-28 00:11:19,097 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:11:19,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:11:19,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'ar@@', 'tic@@', 'k@@', 'et', 'gl@@', 'aci@@', 'al', 'cal@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'and', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'percent@@', '.', '</s>']
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the calculation that the articket glacial cal, which for almost three million years has had the size of 48 United States and the continental continental of the continental continental percent.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - Example #1
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice.', '</s>']
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue because it doesn't show the ice of the problem because it doesn't show the ice.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - Example #2
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'et', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cle@@', 'an@@', 'er', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - 	Hypothesis: The articket glacial calculation is, in a sense, the heart cleaner of the global climate system.
2024-05-28 00:11:19,098 - INFO - joeynmt.training - Example #3
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:11:19,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'es', 'of', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Hypothesis: It expandes of winter and return it into the summer.
2024-05-28 00:11:19,099 - INFO - joeynmt.training - Example #4
2024-05-28 00:11:19,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:11:19,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:11:19,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:11:19,099 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:11:35,532 - INFO - joeynmt.training - Epoch   8, Step:    31100, Batch Loss:     1.354599, Batch Acc: 0.567739, Tokens per Sec:     4291, Lr: 0.000300
2024-05-28 00:11:52,338 - INFO - joeynmt.training - Epoch   8, Step:    31200, Batch Loss:     1.457858, Batch Acc: 0.571682, Tokens per Sec:     4158, Lr: 0.000300
2024-05-28 00:12:09,087 - INFO - joeynmt.training - Epoch   8, Step:    31300, Batch Loss:     1.336037, Batch Acc: 0.572029, Tokens per Sec:     4137, Lr: 0.000300
2024-05-28 00:12:25,736 - INFO - joeynmt.training - Epoch   8, Step:    31400, Batch Loss:     1.480962, Batch Acc: 0.570445, Tokens per Sec:     4090, Lr: 0.000300
2024-05-28 00:12:41,579 - INFO - joeynmt.training - Epoch   8, Step:    31500, Batch Loss:     1.494537, Batch Acc: 0.561830, Tokens per Sec:     4225, Lr: 0.000300
2024-05-28 00:12:41,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:12:41,580 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:13:47,793 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.76, acc:   0.52, generation: 66.2071[sec], evaluation: 0.0000[sec]
2024-05-28 00:13:47,794 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:13:47,959 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/29000.ckpt
2024-05-28 00:13:47,960 - INFO - joeynmt.training - Example #0
2024-05-28 00:13:47,960 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:13:47,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:13:47,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 's,', 'which', 'for', 'ne@@', 'ar@@', 'ly', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'from', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:13:47,960 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:13:47,960 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:13:47,960 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the slide glacial calots, which for nearly three million years has had the size of 48 United States of the continental continental continental continental from 40 percent.
2024-05-28 00:13:47,960 - INFO - joeynmt.training - Example #1
2024-05-28 00:13:47,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'p@@', 'ack@@', 'ag@@', 'es', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice.', '</s>']
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Hypothesis: But this underpackages the problem because it doesn't show the ice of the problem because it doesn't show the ice.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - Example #2
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an@@', 'er', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calculation is, in a sense, the cleaner of the climate system.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - Example #3
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:13:47,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'the', 'sum@@', 'mer@@', 's', 'and', 're@@', 'ver@@', 'se', 'and', 're@@', 'turn', 'it', 'out.', '</s>']
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - 	Hypothesis: You get the summers and reverse and return it out.
2024-05-28 00:13:47,961 - INFO - joeynmt.training - Example #4
2024-05-28 00:13:47,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:13:47,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:13:47,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:13:47,962 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:13:47,962 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:13:47,962 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlled on the last 25 years.
2024-05-28 00:14:04,490 - INFO - joeynmt.training - Epoch   8, Step:    31600, Batch Loss:     1.486595, Batch Acc: 0.574442, Tokens per Sec:     4220, Lr: 0.000300
2024-05-28 00:14:11,269 - INFO - joeynmt.training - Epoch   8: total training loss 5965.62
2024-05-28 00:14:11,270 - INFO - joeynmt.training - EPOCH 9
2024-05-28 00:14:20,892 - INFO - joeynmt.training - Epoch   9, Step:    31700, Batch Loss:     1.285210, Batch Acc: 0.597794, Tokens per Sec:     4193, Lr: 0.000300
2024-05-28 00:14:37,390 - INFO - joeynmt.training - Epoch   9, Step:    31800, Batch Loss:     1.469530, Batch Acc: 0.599472, Tokens per Sec:     4204, Lr: 0.000300
2024-05-28 00:14:53,781 - INFO - joeynmt.training - Epoch   9, Step:    31900, Batch Loss:     1.469086, Batch Acc: 0.595941, Tokens per Sec:     4208, Lr: 0.000300
2024-05-28 00:15:10,656 - INFO - joeynmt.training - Epoch   9, Step:    32000, Batch Loss:     1.315122, Batch Acc: 0.597003, Tokens per Sec:     3990, Lr: 0.000300
2024-05-28 00:15:10,656 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:15:10,656 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:16:07,481 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.78, acc:   0.52, generation: 56.8186[sec], evaluation: 0.0000[sec]
2024-05-28 00:16:07,647 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/29500.ckpt
2024-05-28 00:16:07,648 - INFO - joeynmt.training - Example #0
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'that', 'the', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'us', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'us', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the slide that the article calculus to show that the calculus three million years has had the size of the 48 United States of the continental continental continental --
2024-05-28 00:16:07,648 - INFO - joeynmt.training - Example #1
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:16:07,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:16:07,648 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue of the problem because it doesn't show the pace of the problem of the ice.
2024-05-28 00:16:07,648 - INFO - joeynmt.training - Example #2
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'is', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Hypothesis: The artic glacial is is, in a sense, the clear heart of the global climate system.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - Example #3
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'in', 'the', 'sum@@', 'm@@', 'er', 'and', 'it', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'it', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'w@@', 'int@@', '.', '</s>']
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Hypothesis: It expand in the summer and it return it into the summer of the summer of the summer and it return it into the summer of wint.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - Example #4
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:16:07,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'de@@', 'ad', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:16:07,649 - INFO - joeynmt.training - 	Hypothesis: The next dead is going to be a quick carlled on the last 25 years.
2024-05-28 00:16:24,778 - INFO - joeynmt.training - Epoch   9, Step:    32100, Batch Loss:     1.262599, Batch Acc: 0.592101, Tokens per Sec:     4106, Lr: 0.000300
2024-05-28 00:16:41,006 - INFO - joeynmt.training - Epoch   9, Step:    32200, Batch Loss:     1.437246, Batch Acc: 0.592427, Tokens per Sec:     4241, Lr: 0.000300
2024-05-28 00:16:57,295 - INFO - joeynmt.training - Epoch   9, Step:    32300, Batch Loss:     1.543225, Batch Acc: 0.589417, Tokens per Sec:     4325, Lr: 0.000300
2024-05-28 00:17:13,649 - INFO - joeynmt.training - Epoch   9, Step:    32400, Batch Loss:     1.492040, Batch Acc: 0.589891, Tokens per Sec:     4198, Lr: 0.000300
2024-05-28 00:17:30,038 - INFO - joeynmt.training - Epoch   9, Step:    32500, Batch Loss:     1.415910, Batch Acc: 0.583925, Tokens per Sec:     4054, Lr: 0.000300
2024-05-28 00:17:30,038 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:17:30,038 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:18:35,083 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.78, acc:   0.52, generation: 65.0389[sec], evaluation: 0.0000[sec]
2024-05-28 00:18:35,251 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/30000.ckpt
2024-05-28 00:18:35,252 - INFO - joeynmt.training - Example #0
2024-05-28 00:18:35,252 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'last', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'to@@', 'o,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'is', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Hypothesis: And last year, I showed these slides to show that the slide to show that the calculation that artical calottoo, which for almost three million years has had the size of the 48 United States is 40-percent.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - Example #1
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Hypothesis: However, this underestimate the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the ice.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - Example #2
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:18:35,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - 	Hypothesis: The article calculation is, in a sense, the heart of the climate system of the climate system.
2024-05-28 00:18:35,253 - INFO - joeynmt.training - Example #3
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'w@@', 'int@@', 'er', 'and', 'it', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Hypothesis: It expands and winter and it return it into the summer.
2024-05-28 00:18:35,254 - INFO - joeynmt.training - Example #4
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:18:35,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:18:35,254 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:18:51,826 - INFO - joeynmt.training - Epoch   9, Step:    32600, Batch Loss:     1.450602, Batch Acc: 0.580983, Tokens per Sec:     4011, Lr: 0.000300
2024-05-28 00:19:08,176 - INFO - joeynmt.training - Epoch   9, Step:    32700, Batch Loss:     1.437038, Batch Acc: 0.584407, Tokens per Sec:     4287, Lr: 0.000300
2024-05-28 00:19:24,576 - INFO - joeynmt.training - Epoch   9, Step:    32800, Batch Loss:     1.646512, Batch Acc: 0.587262, Tokens per Sec:     4330, Lr: 0.000300
2024-05-28 00:19:41,140 - INFO - joeynmt.training - Epoch   9, Step:    32900, Batch Loss:     1.537079, Batch Acc: 0.587870, Tokens per Sec:     4171, Lr: 0.000300
2024-05-28 00:19:57,494 - INFO - joeynmt.training - Epoch   9, Step:    33000, Batch Loss:     1.556293, Batch Acc: 0.585003, Tokens per Sec:     4263, Lr: 0.000300
2024-05-28 00:19:57,495 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:19:57,495 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:20:51,821 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.77, acc:   0.52, generation: 54.3198[sec], evaluation: 0.0000[sec]
2024-05-28 00:20:51,986 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/31000.ckpt
2024-05-28 00:20:51,987 - INFO - joeynmt.training - Example #0
2024-05-28 00:20:51,987 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:20:51,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:20:51,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'been', 'stre@@', 't@@', 'ched', 'by', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the article that the artical calot, which for almost three million years of the size of 48 United States of the United States has been stretched by 40 percent.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - Example #1
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue the gravity of the problem because it doesn't show the ice of the ice.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - Example #2
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'gl@@', 'aci@@', 'al', 'system@@', ',', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - 	Hypothesis: The artical glacial system, in a sense, the heart of the global climate system.
2024-05-28 00:20:51,988 - INFO - joeynmt.training - Example #3
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:20:51,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'in', 'the', 'sum@@', 'mer@@', ',', 'and', 'it', 'comes', 'out', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Hypothesis: You expand in the summer, and it comes out of the summer.
2024-05-28 00:20:51,989 - INFO - joeynmt.training - Example #4
2024-05-28 00:20:51,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:20:51,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:20:51,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:20:51,989 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrelled on the last 25 years.
2024-05-28 00:21:08,526 - INFO - joeynmt.training - Epoch   9, Step:    33100, Batch Loss:     1.377126, Batch Acc: 0.584520, Tokens per Sec:     4102, Lr: 0.000300
2024-05-28 00:21:24,870 - INFO - joeynmt.training - Epoch   9, Step:    33200, Batch Loss:     1.443290, Batch Acc: 0.584868, Tokens per Sec:     4123, Lr: 0.000300
2024-05-28 00:21:41,474 - INFO - joeynmt.training - Epoch   9, Step:    33300, Batch Loss:     1.469372, Batch Acc: 0.581016, Tokens per Sec:     4049, Lr: 0.000300
2024-05-28 00:21:57,944 - INFO - joeynmt.training - Epoch   9, Step:    33400, Batch Loss:     1.301548, Batch Acc: 0.585035, Tokens per Sec:     4127, Lr: 0.000300
2024-05-28 00:22:14,145 - INFO - joeynmt.training - Epoch   9, Step:    33500, Batch Loss:     1.310414, Batch Acc: 0.583125, Tokens per Sec:     4169, Lr: 0.000300
2024-05-28 00:22:14,145 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:22:14,145 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:23:23,978 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.76, acc:   0.52, generation: 69.8259[sec], evaluation: 0.0000[sec]
2024-05-28 00:23:23,979 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:23:24,140 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/30500.ckpt
2024-05-28 00:23:24,141 - INFO - joeynmt.training - Example #0
2024-05-28 00:23:24,141 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:23:24,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:23:24,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'the', 'ar@@', 'tic@@', 'al', 'gl@@', 'aci@@', 'al', 'ac@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'has', 'been', 'the', 'size', 'of', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:23:24,141 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:23:24,141 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:23:24,141 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the calculation that the artical glacial actica, which for almost three million years of the size of the 48 United States has been the size of 40 percent.
2024-05-28 00:23:24,141 - INFO - joeynmt.training - Example #1
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'tra', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'tra', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'ce@@']
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the problem because it doesn't show the extra ice of the problem because it doesn't show the pace of the ice of the problem because it doesn't show the pace of the problem because it doesn't show the extra ice of the problem because it doesn't show the pace of the problem because it doesn't show the expace of the problem because it doesn't show the expace of the problem because it doesn't show the exce
2024-05-28 00:23:24,142 - INFO - joeynmt.training - Example #2
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ter', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'cle@@', 'ar', 'hear@@', 't', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Hypothesis: The artical glacial calotter is in a sense, the heart clear heart climate system.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - Example #3
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:23:24,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'in', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - 	Hypothesis: You expand in the winter and return it into the summer.
2024-05-28 00:23:24,142 - INFO - joeynmt.training - Example #4
2024-05-28 00:23:24,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:23:24,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:23:24,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'rapi@@', 'd', 'car@@', 'e@@', 'ful', 'car@@', 'ro@@', 't', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:23:24,143 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:23:24,143 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:23:24,143 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid careful carrot on the last 25 years.
2024-05-28 00:23:40,818 - INFO - joeynmt.training - Epoch   9, Step:    33600, Batch Loss:     1.578584, Batch Acc: 0.583006, Tokens per Sec:     4260, Lr: 0.000300
2024-05-28 00:23:57,321 - INFO - joeynmt.training - Epoch   9, Step:    33700, Batch Loss:     1.621147, Batch Acc: 0.584968, Tokens per Sec:     4238, Lr: 0.000300
2024-05-28 00:24:13,468 - INFO - joeynmt.training - Epoch   9, Step:    33800, Batch Loss:     1.526889, Batch Acc: 0.586479, Tokens per Sec:     4368, Lr: 0.000300
2024-05-28 00:24:29,672 - INFO - joeynmt.training - Epoch   9, Step:    33900, Batch Loss:     1.357370, Batch Acc: 0.586941, Tokens per Sec:     4440, Lr: 0.000300
2024-05-28 00:24:46,453 - INFO - joeynmt.training - Epoch   9, Step:    34000, Batch Loss:     1.330661, Batch Acc: 0.580945, Tokens per Sec:     4132, Lr: 0.000300
2024-05-28 00:24:46,453 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:24:46,453 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:25:46,371 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.74, acc:   0.52, generation: 59.9114[sec], evaluation: 0.0000[sec]
2024-05-28 00:25:46,372 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:25:46,534 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/32000.ckpt
2024-05-28 00:25:46,535 - INFO - joeynmt.training - Example #0
2024-05-28 00:25:46,535 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:25:46,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 't,', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', '4@@', '8', 'United', 'St@@', 'ates', 'is', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show that the slide demonstrate that the calcult, which is about three million years has had the size of 48 of the United States of the continental 48 United States is 40 percent.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - Example #1
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue the problem because it doesn't show the ice of the ice because it doesn't show the ice of the ice.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - Example #2
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:25:46,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'is,', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - 	Hypothesis: The article calculation is is, in a way, the heart of the global climate system.
2024-05-28 00:25:46,536 - INFO - joeynmt.training - Example #3
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'the', 'sum@@', 'mer@@', 's', 're@@', 'turn', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Hypothesis: It expands and the summers return into the summer.
2024-05-28 00:25:46,537 - INFO - joeynmt.training - Example #4
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:25:46,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:25:46,537 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick career on the last 25 years.
2024-05-28 00:26:03,222 - INFO - joeynmt.training - Epoch   9, Step:    34100, Batch Loss:     1.507241, Batch Acc: 0.573937, Tokens per Sec:     3951, Lr: 0.000300
2024-05-28 00:26:19,613 - INFO - joeynmt.training - Epoch   9, Step:    34200, Batch Loss:     1.486258, Batch Acc: 0.581595, Tokens per Sec:     4203, Lr: 0.000300
2024-05-28 00:26:35,962 - INFO - joeynmt.training - Epoch   9, Step:    34300, Batch Loss:     1.527960, Batch Acc: 0.580219, Tokens per Sec:     4311, Lr: 0.000300
2024-05-28 00:26:52,126 - INFO - joeynmt.training - Epoch   9, Step:    34400, Batch Loss:     1.652441, Batch Acc: 0.574087, Tokens per Sec:     4296, Lr: 0.000300
2024-05-28 00:27:08,308 - INFO - joeynmt.training - Epoch   9, Step:    34500, Batch Loss:     1.552608, Batch Acc: 0.577310, Tokens per Sec:     4218, Lr: 0.000300
2024-05-28 00:27:08,308 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:27:08,308 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:28:03,792 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.53, generation: 55.4776[sec], evaluation: 0.0000[sec]
2024-05-28 00:28:03,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:28:03,957 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/32500.ckpt
2024-05-28 00:28:03,957 - INFO - joeynmt.training - Example #0
2024-05-28 00:28:03,957 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:28:03,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:28:03,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'ation', 'c@@', 'alc@@', 'ul@@', 'ation', 'that', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', '--', '</s>']
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to demonstrate that the calculation calculation that for almost three million years has had the size of 48 of the United States of the continental continental continental --
2024-05-28 00:28:03,958 - INFO - joeynmt.training - Example #1
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue the problem because it doesn't show the ice of the ice of the problem because it doesn't show the ice of the ice.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - Example #2
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', ',', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calot, in a way, the heart of the global climate system.
2024-05-28 00:28:03,958 - INFO - joeynmt.training - Example #3
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:28:03,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Hypothesis: It expand the winter and return it into the summer.
2024-05-28 00:28:03,959 - INFO - joeynmt.training - Example #4
2024-05-28 00:28:03,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:28:03,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:28:03,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'fast@@', 'er', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:28:03,959 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick faster on the last 25 years.
2024-05-28 00:28:20,188 - INFO - joeynmt.training - Epoch   9, Step:    34600, Batch Loss:     1.533624, Batch Acc: 0.579751, Tokens per Sec:     4140, Lr: 0.000300
2024-05-28 00:28:36,466 - INFO - joeynmt.training - Epoch   9, Step:    34700, Batch Loss:     1.471208, Batch Acc: 0.579698, Tokens per Sec:     4241, Lr: 0.000300
2024-05-28 00:28:52,108 - INFO - joeynmt.training - Epoch   9, Step:    34800, Batch Loss:     1.673421, Batch Acc: 0.578902, Tokens per Sec:     4288, Lr: 0.000300
2024-05-28 00:29:08,046 - INFO - joeynmt.training - Epoch   9, Step:    34900, Batch Loss:     1.393935, Batch Acc: 0.577066, Tokens per Sec:     4377, Lr: 0.000300
2024-05-28 00:29:24,262 - INFO - joeynmt.training - Epoch   9, Step:    35000, Batch Loss:     1.486360, Batch Acc: 0.578444, Tokens per Sec:     4301, Lr: 0.000300
2024-05-28 00:29:24,262 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:29:24,262 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:30:23,082 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.53, generation: 58.8139[sec], evaluation: 0.0000[sec]
2024-05-28 00:30:23,083 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:30:23,245 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/33000.ckpt
2024-05-28 00:30:23,245 - INFO - joeynmt.training - Example #0
2024-05-28 00:30:23,245 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:30:23,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:30:23,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', ',', 'which', 'for', 'ne@@', 'ar@@', 'ly', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'has', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'it', 'was', '4@@', '0@@', '-@@', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es.', '</s>']
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Hypothesis: I showed these slides to show that the article demonstrate that the artical calot, which for nearly three million years of the United States, has the size of the 48 United States, it was 40-percent of the United States.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - Example #1
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice.', '</s>']
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the ice because it doesn't show the ice.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - Example #2
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'he@@', 'at', 'is', 'is', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '</s>']
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:30:23,246 - INFO - joeynmt.training - 	Hypothesis: The artic heat is is in a way, the heart of the global climate system
2024-05-28 00:30:23,246 - INFO - joeynmt.training - Example #3
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:30:23,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'f@@', 'low@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Hypothesis: It expanding and flower and return it into the summer.
2024-05-28 00:30:23,247 - INFO - joeynmt.training - Example #4
2024-05-28 00:30:23,247 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:30:23,247 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:30:23,247 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'f@@', 'ast', 'of', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:30:23,247 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the fast of the last 25 years.
2024-05-28 00:30:39,412 - INFO - joeynmt.training - Epoch   9, Step:    35100, Batch Loss:     1.612619, Batch Acc: 0.580300, Tokens per Sec:     4317, Lr: 0.000300
2024-05-28 00:30:55,773 - INFO - joeynmt.training - Epoch   9, Step:    35200, Batch Loss:     1.372992, Batch Acc: 0.577185, Tokens per Sec:     4069, Lr: 0.000300
2024-05-28 00:31:11,879 - INFO - joeynmt.training - Epoch   9, Step:    35300, Batch Loss:     1.601261, Batch Acc: 0.582379, Tokens per Sec:     4338, Lr: 0.000300
2024-05-28 00:31:27,881 - INFO - joeynmt.training - Epoch   9, Step:    35400, Batch Loss:     1.471401, Batch Acc: 0.577715, Tokens per Sec:     4248, Lr: 0.000300
2024-05-28 00:31:43,732 - INFO - joeynmt.training - Epoch   9, Step:    35500, Batch Loss:     1.502610, Batch Acc: 0.572216, Tokens per Sec:     4381, Lr: 0.000300
2024-05-28 00:31:43,732 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:31:43,732 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:32:40,250 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.66, acc:   0.53, generation: 56.5112[sec], evaluation: 0.0000[sec]
2024-05-28 00:32:40,251 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:32:40,413 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/31500.ckpt
2024-05-28 00:32:40,414 - INFO - joeynmt.training - Example #0
2024-05-28 00:32:40,414 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:32:40,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:32:40,414 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'c@@', 'alc@@', 'ul@@', 'um', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', '4@@', '8', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'from', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-28 00:32:40,414 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:32:40,414 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:32:40,414 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the calculum glacial calote, which for almost three million years of the size of 48 of the United States of the continental continental continental from 40-percent.
2024-05-28 00:32:40,414 - INFO - joeynmt.training - Example #1
2024-05-28 00:32:40,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:32:40,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Hypothesis: But this undervalue the gravity of the problem because it doesn't show the ice of the ice because it doesn't show the ice of the ice.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - Example #2
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'ta', 'is,', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calotta is, in a way, the heart of the global climate system.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - Example #3
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'gets', 's@@', 'end@@', 's', 'and', 'w@@', 'int@@', 'er', 're@@', 'ver@@', 'se', 'and', 're@@', 'turn', 'it', 'down@@', '.', '</s>']
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - 	Hypothesis: It gets sends and winter reverse and return it down.
2024-05-28 00:32:40,415 - INFO - joeynmt.training - Example #4
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:32:40,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:32:40,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:32:40,416 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:32:40,416 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:32:40,416 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:32:56,485 - INFO - joeynmt.training - Epoch   9, Step:    35600, Batch Loss:     1.526560, Batch Acc: 0.586038, Tokens per Sec:     4298, Lr: 0.000300
2024-05-28 00:32:59,543 - INFO - joeynmt.training - Epoch   9: total training loss 5865.65
2024-05-28 00:32:59,543 - INFO - joeynmt.training - EPOCH 10
2024-05-28 00:33:12,635 - INFO - joeynmt.training - Epoch  10, Step:    35700, Batch Loss:     1.476670, Batch Acc: 0.603230, Tokens per Sec:     4215, Lr: 0.000300
2024-05-28 00:33:29,073 - INFO - joeynmt.training - Epoch  10, Step:    35800, Batch Loss:     1.263689, Batch Acc: 0.607058, Tokens per Sec:     4273, Lr: 0.000300
2024-05-28 00:33:45,614 - INFO - joeynmt.training - Epoch  10, Step:    35900, Batch Loss:     1.452793, Batch Acc: 0.606942, Tokens per Sec:     4093, Lr: 0.000300
2024-05-28 00:34:02,121 - INFO - joeynmt.training - Epoch  10, Step:    36000, Batch Loss:     1.388659, Batch Acc: 0.597606, Tokens per Sec:     4144, Lr: 0.000300
2024-05-28 00:34:02,121 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:34:02,121 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:34:55,294 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.53, generation: 53.1664[sec], evaluation: 0.0000[sec]
2024-05-28 00:34:55,456 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/33500.ckpt
2024-05-28 00:34:55,457 - INFO - joeynmt.training - Example #0
2024-05-28 00:34:55,457 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:34:55,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:34:55,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'of', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'size', 'of', 'the', '4@@', '8', 'has', 'had', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'from', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Hypothesis: I showed these slides of these slides to show that the article calote, which for almost three million years of the size of the 48 has had the size of 48 United States continental continental continental from 40-percent.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - Example #1
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Hypothesis: But this undervalue of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the ice.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - Example #2
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'is', 'a', 'kind', 'of', 'an', 'ar@@', 't@@', 'ic', 'is', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - 	Hypothesis: The article is a kind of an artic is in a way, the heart of the global climate system.
2024-05-28 00:34:55,458 - INFO - joeynmt.training - Example #3
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:34:55,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'comes', 'out', 'of', 'w@@', 'int@@', 's', 'and', 'f@@', 'low@@', 'ing', 'up', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Hypothesis: It comes out of wints and flowing up and return it into the summer.
2024-05-28 00:34:55,459 - INFO - joeynmt.training - Example #4
2024-05-28 00:34:55,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:34:55,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:34:55,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'r@@', 'ying', 'car@@', 'll@@', 'ed', 'in', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:34:55,459 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carrying carlled in the last 25 years.
2024-05-28 00:35:11,891 - INFO - joeynmt.training - Epoch  10, Step:    36100, Batch Loss:     1.454140, Batch Acc: 0.601540, Tokens per Sec:     4273, Lr: 0.000300
2024-05-28 00:35:28,388 - INFO - joeynmt.training - Epoch  10, Step:    36200, Batch Loss:     1.208965, Batch Acc: 0.608209, Tokens per Sec:     4200, Lr: 0.000300
2024-05-28 00:35:44,889 - INFO - joeynmt.training - Epoch  10, Step:    36300, Batch Loss:     1.445604, Batch Acc: 0.597289, Tokens per Sec:     4310, Lr: 0.000300
2024-05-28 00:36:01,216 - INFO - joeynmt.training - Epoch  10, Step:    36400, Batch Loss:     1.358600, Batch Acc: 0.595126, Tokens per Sec:     4366, Lr: 0.000300
2024-05-28 00:36:17,729 - INFO - joeynmt.training - Epoch  10, Step:    36500, Batch Loss:     1.488504, Batch Acc: 0.595869, Tokens per Sec:     4123, Lr: 0.000300
2024-05-28 00:36:17,729 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:36:17,729 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:37:19,958 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.76, acc:   0.52, generation: 62.2225[sec], evaluation: 0.0000[sec]
2024-05-28 00:37:19,959 - INFO - joeynmt.training - Example #0
2024-05-28 00:37:19,959 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:37:19,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:37:19,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'sli@@', 'de', 'de@@', 's,', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'percent@@', '.', '</s>']
2024-05-28 00:37:19,959 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the slide des, which is about three million years has had the size of the 48 million years has had the size of the 48 percent.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - Example #1
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ex@@', 'ce@@', 'pt', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'grav@@']
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Hypothesis: However, this underestimate the gravity of the problem because it doesn't show the ice of the ice problem because it doesn't show the ice of the ice of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the problem because it doesn't show the except of the problem because it doesn't show the ice of the problem because it doesn't show the ice of the problem because it doesn't show the grav
2024-05-28 00:37:19,960 - INFO - joeynmt.training - Example #2
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'gl@@', 'aci@@', 'al', 'is', 'is,', 'in', 'a', 'certain', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - 	Hypothesis: The article glacial is is, in a certain sense, the heart of the global climate system.
2024-05-28 00:37:19,960 - INFO - joeynmt.training - Example #3
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:37:19,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'it', 'goes', 'up', 'and', 're@@', 'tre@@', 'at@@', 'ment.', '</s>']
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Hypothesis: It expand it goes up and retreatment.
2024-05-28 00:37:19,961 - INFO - joeynmt.training - Example #4
2024-05-28 00:37:19,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:37:19,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:37:19,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:37:19,961 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlled on the last 25 years.
2024-05-28 00:37:36,349 - INFO - joeynmt.training - Epoch  10, Step:    36600, Batch Loss:     1.313382, Batch Acc: 0.597255, Tokens per Sec:     4157, Lr: 0.000300
2024-05-28 00:37:52,738 - INFO - joeynmt.training - Epoch  10, Step:    36700, Batch Loss:     1.616948, Batch Acc: 0.595427, Tokens per Sec:     4219, Lr: 0.000300
2024-05-28 00:38:09,189 - INFO - joeynmt.training - Epoch  10, Step:    36800, Batch Loss:     1.367772, Batch Acc: 0.591413, Tokens per Sec:     4147, Lr: 0.000300
2024-05-28 00:38:25,603 - INFO - joeynmt.training - Epoch  10, Step:    36900, Batch Loss:     1.344905, Batch Acc: 0.599435, Tokens per Sec:     4294, Lr: 0.000300
2024-05-28 00:38:42,155 - INFO - joeynmt.training - Epoch  10, Step:    37000, Batch Loss:     1.430303, Batch Acc: 0.595051, Tokens per Sec:     4363, Lr: 0.000300
2024-05-28 00:38:42,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:38:42,155 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:39:39,031 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.52, generation: 56.8692[sec], evaluation: 0.0000[sec]
2024-05-28 00:39:39,196 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/34000.ckpt
2024-05-28 00:39:39,197 - INFO - joeynmt.training - Example #0
2024-05-28 00:39:39,197 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:39:39,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:39:39,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'cal@@', 'ot@@', 'e,', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'the', 'size', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'the', 'size', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'is', 'a', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:39:39,197 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:39:39,197 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:39:39,197 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article calote, which is about three million years has the size of the artica, which for almost three million years has the size of the continental continental continental is a 40 percent.
2024-05-28 00:39:39,197 - INFO - joeynmt.training - Example #1
2024-05-28 00:39:39,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:39:39,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'jec@@', 't', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'to', 'the', 'ice', 'as', 'a', 'ice.', '</s>']
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Hypothesis: But this subject the gravity of the problem because it doesn't show the pace of the ice to the ice as a ice.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - Example #2
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'k@@', 'et', 'c@@', 'alc@@', 'ul@@', 'ation', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an@@', 'ing', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Hypothesis: The articket calculation is in a sense, the cleaning heart of the global climate system.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - Example #3
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'it', 'comes', 'up', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', '</s>']
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:39:39,198 - INFO - joeynmt.training - 	Hypothesis: It expand it comes up and return it into the summer of the summer
2024-05-28 00:39:39,198 - INFO - joeynmt.training - Example #4
2024-05-28 00:39:39,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:39:39,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:39:39,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:39:39,199 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:39:39,199 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:39:39,199 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlled on the last 25 years.
2024-05-28 00:39:55,447 - INFO - joeynmt.training - Epoch  10, Step:    37100, Batch Loss:     1.577118, Batch Acc: 0.591639, Tokens per Sec:     4306, Lr: 0.000300
2024-05-28 00:40:12,010 - INFO - joeynmt.training - Epoch  10, Step:    37200, Batch Loss:     1.446146, Batch Acc: 0.599777, Tokens per Sec:     4382, Lr: 0.000300
2024-05-28 00:40:28,225 - INFO - joeynmt.training - Epoch  10, Step:    37300, Batch Loss:     1.467168, Batch Acc: 0.594699, Tokens per Sec:     4286, Lr: 0.000300
2024-05-28 00:40:44,529 - INFO - joeynmt.training - Epoch  10, Step:    37400, Batch Loss:     1.430159, Batch Acc: 0.592179, Tokens per Sec:     4282, Lr: 0.000300
2024-05-28 00:41:00,964 - INFO - joeynmt.training - Epoch  10, Step:    37500, Batch Loss:     1.188446, Batch Acc: 0.592422, Tokens per Sec:     4253, Lr: 0.000300
2024-05-28 00:41:00,964 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:41:00,964 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:42:02,774 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.66, acc:   0.53, generation: 61.8035[sec], evaluation: 0.0000[sec]
2024-05-28 00:42:02,940 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/37000.ckpt
2024-05-28 00:42:02,941 - INFO - joeynmt.helpers - delete /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/37000.ckpt
2024-05-28 00:42:02,941 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/37000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/37000.ckpt')
2024-05-28 00:42:02,941 - INFO - joeynmt.training - Example #0
2024-05-28 00:42:02,941 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:42:02,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:42:02,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'k@@', 'et', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'it', 're@@', 'port@@', 'ed', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:42:02,941 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:42:02,941 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:42:02,941 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the articket to show that the artical calote, which for almost three million years has had the size of the 48 United States continents, it reported 40 percent.
2024-05-28 00:42:02,941 - INFO - joeynmt.training - Example #1
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', '</s>']
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of gravity of the problem because it doesn't show the pace of the problem because it doesn't show the ice of the ice
2024-05-28 00:42:02,942 - INFO - joeynmt.training - Example #2
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'cal@@', 'ot@@', 'e,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Hypothesis: The artical heat calote, in a sense, the clear heart of global climate system.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - Example #3
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:42:02,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'out', 'of', 'w@@', 'int@@', 'er', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - 	Hypothesis: You get out of winter and return it into the summer.
2024-05-28 00:42:02,942 - INFO - joeynmt.training - Example #4
2024-05-28 00:42:02,943 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:42:02,943 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:42:02,943 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'll@@', 'ed', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:42:02,943 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:42:02,943 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:42:02,943 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carlled on the last 25 years.
2024-05-28 00:42:19,423 - INFO - joeynmt.training - Epoch  10, Step:    37600, Batch Loss:     1.413944, Batch Acc: 0.589529, Tokens per Sec:     4095, Lr: 0.000300
2024-05-28 00:42:35,726 - INFO - joeynmt.training - Epoch  10, Step:    37700, Batch Loss:     1.471522, Batch Acc: 0.591591, Tokens per Sec:     4177, Lr: 0.000300
2024-05-28 00:42:52,364 - INFO - joeynmt.training - Epoch  10, Step:    37800, Batch Loss:     1.589015, Batch Acc: 0.589137, Tokens per Sec:     4196, Lr: 0.000300
2024-05-28 00:43:08,631 - INFO - joeynmt.training - Epoch  10, Step:    37900, Batch Loss:     1.360591, Batch Acc: 0.594243, Tokens per Sec:     4299, Lr: 0.000300
2024-05-28 00:43:24,837 - INFO - joeynmt.training - Epoch  10, Step:    38000, Batch Loss:     1.520356, Batch Acc: 0.590672, Tokens per Sec:     4227, Lr: 0.000300
2024-05-28 00:43:24,839 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:43:24,839 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:44:25,899 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.53, generation: 61.0540[sec], evaluation: 0.0000[sec]
2024-05-28 00:44:26,064 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/34500.ckpt
2024-05-28 00:44:26,066 - INFO - joeynmt.training - Example #0
2024-05-28 00:44:26,066 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:44:26,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:44:26,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'c@@', 'alc@@', 'ul@@', 'ation', '--', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '0', 'years', 'has', 'had', 'the', 'size', 'of', '4@@', '0', 'percent@@', '.', 'And', 'it', 'was', 'stre@@', 's@@', 's', 'of', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:44:26,066 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:44:26,066 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:44:26,066 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article calculation -- which is about three million years has had the size of 40 years has had the size of 40 percent. And it was stress of 40 percent.
2024-05-28 00:44:26,066 - INFO - joeynmt.training - Example #1
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ess@@', 'or', 'the', 'th@@', 'ese.', '</s>']
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Hypothesis: But this underestimate gravity of the problem because it doesn't show the pace of the problem because it doesn't show the pessor the these.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - Example #2
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ful', 'gl@@', 'aci@@', 'al', 'is', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'cle@@', 'an', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Hypothesis: The artful glacial is is, in a sense, the clean heart of the global climate system.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - Example #3
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:44:26,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'out', 'of', 'w@@', 'int@@', 'er', 'and', 're@@', 'ver@@', 'se', 'it', 'gets', 'up', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:44:26,067 - INFO - joeynmt.training - 	Hypothesis: You get out of winter and reverse it gets up into the summer.
2024-05-28 00:44:26,068 - INFO - joeynmt.training - Example #4
2024-05-28 00:44:26,068 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:44:26,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:44:26,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:44:26,068 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:44:26,068 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:44:26,068 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:44:42,791 - INFO - joeynmt.training - Epoch  10, Step:    38100, Batch Loss:     1.592350, Batch Acc: 0.583056, Tokens per Sec:     4116, Lr: 0.000300
2024-05-28 00:44:58,675 - INFO - joeynmt.training - Epoch  10, Step:    38200, Batch Loss:     1.420985, Batch Acc: 0.589292, Tokens per Sec:     4362, Lr: 0.000300
2024-05-28 00:45:15,227 - INFO - joeynmt.training - Epoch  10, Step:    38300, Batch Loss:     1.569230, Batch Acc: 0.589034, Tokens per Sec:     4361, Lr: 0.000300
2024-05-28 00:45:31,751 - INFO - joeynmt.training - Epoch  10, Step:    38400, Batch Loss:     1.403569, Batch Acc: 0.587813, Tokens per Sec:     4152, Lr: 0.000300
2024-05-28 00:45:47,791 - INFO - joeynmt.training - Epoch  10, Step:    38500, Batch Loss:     1.501072, Batch Acc: 0.591874, Tokens per Sec:     4280, Lr: 0.000300
2024-05-28 00:45:47,792 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:45:47,792 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:46:51,806 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.68, acc:   0.53, generation: 64.0079[sec], evaluation: 0.0000[sec]
2024-05-28 00:46:51,971 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/38000.ckpt
2024-05-28 00:46:51,971 - INFO - joeynmt.helpers - delete /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/38000.ckpt
2024-05-28 00:46:51,971 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/38000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/38000.ckpt')
2024-05-28 00:46:51,972 - INFO - joeynmt.training - Example #0
2024-05-28 00:46:51,972 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:46:51,972 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:46:51,972 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'sli@@', 'des', 'to', 'show', 'that', 'the', 'art', 'cal@@', 'ot@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'ar@@', 'tic@@', 'al', 'char@@', 'tic@@', 'e,', 'which', 'for', 'ne@@', 'ar@@', 'ly', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'St@@', 'ates', 'in', 'the', 'United', 'St@@', 'at@@', 'es,', 'and', 'it', 'was', '4@@', '0', 'percent@@', '.', '</s>']
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the art calot, which is about three million years has had the artical chartice, which for nearly three million years has had the size of the United States in the United States, and it was 40 percent.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - Example #1
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ese.', '</s>']
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Hypothesis: However, this underestimate the gravity of the problem because it doesn't show the these.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - Example #2
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ic', 'gl@@', 'aci@@', 'al', 'cal@@', 'ot@@', 'e,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - 	Hypothesis: The artic glacial calote, in a sense, the heart of the global climate system.
2024-05-28 00:46:51,973 - INFO - joeynmt.training - Example #3
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:46:51,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'it', 're@@', 'ver@@', 'se', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Hypothesis: It expand it reverse and return it into the summer.
2024-05-28 00:46:51,974 - INFO - joeynmt.training - Example #4
2024-05-28 00:46:51,974 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:46:51,974 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:46:51,974 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:46:51,974 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:47:08,610 - INFO - joeynmt.training - Epoch  10, Step:    38600, Batch Loss:     1.523863, Batch Acc: 0.584802, Tokens per Sec:     4258, Lr: 0.000300
2024-05-28 00:47:24,980 - INFO - joeynmt.training - Epoch  10, Step:    38700, Batch Loss:     1.298163, Batch Acc: 0.585134, Tokens per Sec:     4117, Lr: 0.000300
2024-05-28 00:47:41,673 - INFO - joeynmt.training - Epoch  10, Step:    38800, Batch Loss:     1.609144, Batch Acc: 0.585507, Tokens per Sec:     4151, Lr: 0.000300
2024-05-28 00:47:58,321 - INFO - joeynmt.training - Epoch  10, Step:    38900, Batch Loss:     1.560355, Batch Acc: 0.582654, Tokens per Sec:     4168, Lr: 0.000300
2024-05-28 00:48:14,783 - INFO - joeynmt.training - Epoch  10, Step:    39000, Batch Loss:     1.379500, Batch Acc: 0.589065, Tokens per Sec:     4218, Lr: 0.000300
2024-05-28 00:48:14,784 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:48:14,784 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:49:19,684 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.68, acc:   0.53, generation: 64.8937[sec], evaluation: 0.0000[sec]
2024-05-28 00:49:19,848 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/36000.ckpt
2024-05-28 00:49:19,849 - INFO - joeynmt.training - Example #0
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'art', 'cal@@', 'ot@@', 'e,', 'which', 'is', 'about', 'three', 'million', 'years', 'has', 'had', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'at@@', ',', 'which', 'for', 'almost', 'three', 'million', 'years', 'has', 'had', 'the', 'size', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 's,', '</s>']
2024-05-28 00:49:19,849 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:49:19,849 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:49:19,849 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slide to show that the art calote, which is about three million years has had the artical heat, which for almost three million years has had the size of the United States of the United States of the continents,
2024-05-28 00:49:19,849 - INFO - joeynmt.training - Example #1
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:49:19,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['But', 'this', 'sub@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'grav@@', 'ity', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'problem', 'because', 'it']
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Hypothesis: But this subvalue of the problem because it doesn't show the pace of the problem because it doesn't show the ice of the ice of the problem because it doesn't show the ice of the gravity of the problem because it doesn't show the pace of the problem because it doesn't show the pace of the problem because it doesn't show the pace of the gravity of the problem because it doesn't show the ice of the ice of the problem because it
2024-05-28 00:49:19,850 - INFO - joeynmt.training - Example #2
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'art', 'ac@@', 'tive', 'ac@@', 'id', 'system@@', ',', 'is', 'in', 'a', 'way,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Hypothesis: The art active acid system, is in a way, the clear heart of global climate system.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - Example #3
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 're@@', 'turn', 'it', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - 	Hypothesis: It expanding and return it into the summer.
2024-05-28 00:49:19,850 - INFO - joeynmt.training - Example #4
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:49:19,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:49:19,851 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:49:19,851 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:49:19,851 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:49:36,298 - INFO - joeynmt.training - Epoch  10, Step:    39100, Batch Loss:     1.392331, Batch Acc: 0.587687, Tokens per Sec:     4159, Lr: 0.000300
2024-05-28 00:49:52,477 - INFO - joeynmt.training - Epoch  10, Step:    39200, Batch Loss:     1.443755, Batch Acc: 0.582561, Tokens per Sec:     4150, Lr: 0.000300
2024-05-28 00:50:08,849 - INFO - joeynmt.training - Epoch  10, Step:    39300, Batch Loss:     1.414497, Batch Acc: 0.584263, Tokens per Sec:     4194, Lr: 0.000300
2024-05-28 00:50:25,335 - INFO - joeynmt.training - Epoch  10, Step:    39400, Batch Loss:     1.754105, Batch Acc: 0.582823, Tokens per Sec:     4149, Lr: 0.000300
2024-05-28 00:50:42,164 - INFO - joeynmt.training - Epoch  10, Step:    39500, Batch Loss:     1.400483, Batch Acc: 0.586034, Tokens per Sec:     4130, Lr: 0.000300
2024-05-28 00:50:42,165 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:50:42,165 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:51:42,272 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.53, generation: 60.1012[sec], evaluation: 0.0000[sec]
2024-05-28 00:51:42,273 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-28 00:51:42,436 - INFO - joeynmt.helpers - delete models/bpe_level_model_4000/39000.ckpt
2024-05-28 00:51:42,436 - INFO - joeynmt.training - Example #0
2024-05-28 00:51:42,436 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'scor@@', 'so', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'a@@', 'positi@@', 've', 'per', 'di@@', 'mostr@@', 'are', 'che', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 'tic@@', 'a,', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 'si', 'è', 'ri@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2024-05-28 00:51:42,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'low@@', 'er', '4@@', '8', 'stat@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year,', 'I', 'show@@', 'ed', 'these', 'sli@@', 'de', 'to', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'tic@@', 'al', 'cal@@', 'ot@@', 'ta', 'have', 'been', 'the', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'at', 'at', 'the', 'size', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'is', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slide to demonstrate that the artical calotta have been the artical heat at at the size of 48 United States of the U.S. continental continental continental is 40-percent.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - Example #1
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'ia', 'questo', 'sot@@', 'to@@', 'val@@', 'uta', 'la', 'grav@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o.']
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['But', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice.']
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'under@@', 'val@@', 'ue', 'of', 'the', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'p@@', 'ace', 'of', 'the', 'ice', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice.', '</s>']
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Hypothesis: However, this undervalue of the problem because it doesn't show the pace of the ice problem because it doesn't show the ice of the ice.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - Example #2
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'aci@@', 'ale', 'ar@@', 't@@', 'ica', 'è,', 'in', 'un', 'certo', 'sen@@', 'so,', 'il', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2024-05-28 00:51:42,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'he@@', 'at', 'be@@', 'comes', 'in', 'a', 'way,', 'the', 'cle@@', 'ar', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', '</s>']
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2024-05-28 00:51:42,437 - INFO - joeynmt.training - 	Hypothesis: The artical heat becomes in a way, the clear heart of the global climate system
2024-05-28 00:51:42,437 - INFO - joeynmt.training - Example #3
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['Si', 'es@@', 'p@@', 'and@@', 'e', "d'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'i@@', 'ra', "d'@@", 'est@@', 'ate.']
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'contr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and', 'it', 're@@', 'ver@@', 'se', 'and', 're@@', 'turn', 'it', 'back', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Hypothesis: It expand it reverse and return it back into the summer.
2024-05-28 00:51:42,438 - INFO - joeynmt.training - Example #4
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'prossi@@', 'ma', 'di@@', 'a@@', 'positi@@', 'va', 'sarà', 'una', 'rapi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'anni.']
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-28 00:51:42,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'next', 'sli@@', 'de', 'is', 'going', 'to', 'be', 'a', 'qu@@', 'ick', 'f@@', 'ast', 'on', 'the', 'last', '25', 'years.', '</s>']
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2024-05-28 00:51:42,438 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick fast on the last 25 years.
2024-05-28 00:51:54,734 - INFO - joeynmt.training - Epoch  10: total training loss 5699.22
2024-05-28 00:51:54,734 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-28 00:51:54,734 - INFO - joeynmt.training - Best validation result (greedy) at step    39500:   5.62 ppl.
2024-05-28 00:51:54,743 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-28 00:51:54,784 - INFO - joeynmt.model - Enc-dec model built.
2024-05-28 00:51:54,797 - INFO - joeynmt.helpers - Load model from /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/39500.ckpt.
2024-05-28 00:51:54,800 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3939),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3939),
	loss_function=None)
2024-05-28 00:51:54,800 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-28 00:51:54,801 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:51:54,801 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:52:46,860 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 52.0536[sec], evaluation: 0.0000[sec]
2024-05-28 00:52:46,863 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/00039500.hyps.dev.
2024-05-28 00:52:46,863 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-28 00:52:46,863 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-28 00:52:46,863 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-28 00:54:04,496 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 77.6245[sec], evaluation: 0.0000[sec]
2024-05-28 00:54:04,499 - INFO - joeynmt.prediction - Translations saved to: /Users/siz/Documents/mt-exercise-5/models/bpe_level_model_4000/00039500.hyps.test.
